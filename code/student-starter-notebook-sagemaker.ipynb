{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing large datasets with Apache Spark and Amazon SageMaker\n",
    "\n",
    "***This notebook run on `Data Science 3.0 - Python 3` kernel on a `ml.t3.large` instance***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Processing Jobs are used  to analyze data and evaluate machine learning models on Amazon SageMaker. With Processing, you can use a simplified, managed experience on SageMaker to run your data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation. You can also use the Amazon SageMaker Processing APIs during the experimentation phase and after the code is deployed in production to evaluate performance.\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/Processing-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding diagram shows how Amazon SageMaker spins up a Processing job. Amazon SageMaker takes your script, copies your data from Amazon Simple Storage Service (Amazon S3), and then pulls a processing container. The processing container image can either be an Amazon SageMaker built-in image or a custom image that you provide. The underlying infrastructure for a Processing job is fully managed by Amazon SageMaker. Cluster resources are provisioned for the duration of your job, and cleaned up when a job completes. The output of the Processing job is stored in the Amazon S3 bucket you specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our workflow for processing large amounts of data with SageMaker\n",
    "\n",
    "We can divide our workflow into two steps:\n",
    "    \n",
    "1. Work with a small subset of the data with Spark running in local model in a SageMaker Studio Notebook.\n",
    "\n",
    "1. Once we are able to work with the small subset of data we can provide the same code (as a Python script rather than a series of interactive steps) to SageMaker Processing which launched a Spark cluster, runs out code and terminates the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook...\n",
    "\n",
    "We will analyze the [Pushshift Reddit dataset](https://arxiv.org/pdf/2001.08435.pdf) to be used for the project and then we will run a SageMaker Processing Job to filter out the comments and submissions from subreddits of interest. The filtered data will be stored in your account's s3 bucket and it is this filtered data that you will be using for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We need an available Java installation to run pyspark. The easiest way to do this is to install JDK and set the proper paths using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda-forge/openjdk/11.0.1/download/linux-64::openjdk==11.0.1=hacce0ff_1021\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  openjdk            conda-forge/openjdk/11.0.1/download/linux-64::openjdk-11.0.1-hacce0ff_1021 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyspark==3.4.0\n",
      "  Using cached pyspark-3.4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark==3.4.0) (0.10.9.7)\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda install https://anaconda.org/conda-forge/openjdk/11.0.1/download/linux-64/openjdk-11.0.1-hacce0ff_1021.tar.bz2\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.4.0\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilize S3 Data within local PySpark\n",
    "* By specifying the `hadoop-aws` jar in our Spark config we're able to access S3 datasets using the s3a file prefix. \n",
    "* Since we've already authenticated ourself to SageMaker Studio , we can use our assumed SageMaker ExecutionRole for any S3 reads/writes by setting the credential provider as `ContainerCredentialsProvider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/sagemaker-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/sagemaker-user/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e9a6a166-9c69-428f-b609-6630a604af85;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 149ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e9a6a166-9c69-428f-b609-6630a604af85\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "24/10/06 18:51:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data into a Spark Dataframe\n",
    "Note that we will be using the \"s3a\" adapter (read more [here](https://aws.amazon.com/blogs/opensource/community-collaboration-the-s3a-story)). S3A enables Hadoop to directly read and write Amazon S3 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/06 18:51:29 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+--------------------+------------+\n",
      "|              author|author_flair_css_class|author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded|     id|   link_id| parent_id|retrieved_on|score|stickied|           subreddit|subreddit_id|\n",
      "+--------------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+--------------------+------------+\n",
      "|Accomplished-Act-126|                  null|             null|Yes I messed that...|               0| 1701783300|         null|  null|     0|kc3bftb|t3_18azj17|t1_kc1u8lg|  1701783318|    0|   false|             Mercari|    t5_3fhq0|\n",
      "|       bravo_rambler|                  null|             null|Let's do this! Go...|               0| 1701783300|         null|  null|     0|kc3bftc|t3_18atk99|t3_18atk99|  1701783318|    1|   false|            discgolf|    t5_2qk68|\n",
      "|        DR_Waldo9529|                  null|             null|Man i got over 55...|               0| 1701783300|         null|  null|     0|kc3bftd|t3_18b6d0q|t3_18b6d0q|  1701783318|    0|   false|           MWZombies|   t5_9489r7|\n",
      "|     DirkZelenskyy41|                  null|             null|Johnson and Johns...|               0| 1701783300|         null|  null|     0|kc3bfte|t3_18b69wo|t3_18b69wo|  1701783318|   31|   false|          neoliberal|    t5_2sfn3|\n",
      "|     RealisticFee830|                  null|             null|Hate it, there ar...|               0| 1701783300|         null|  null|     0|kc3bftf|t3_18auqyk|t3_18auqyk|  1701783318|    1|   false|            sypherpk|    t5_kylwj|\n",
      "|   Business-Emu-6923|                  null|             null|Sam and Rosie cer...|               0| 1701783300|         null|  null|     0|kc3bftg|t3_18bbdu8|t3_18bbdu8|  1701783318|    3|   false|      lordoftherings|    t5_2rnel|\n",
      "|     Solid_Blood5601|                  null|             null|Yeap my ass is de...|               0| 1701783300|         null|  null|     0|kc3bfth|t3_18b7dtm|t1_kc2vc0z|  1701783318|    2|   false|            BruneiGW|   t5_41kf32|\n",
      "|         LittleGrowl|                  null|             null|Itâ€™s like saying ...|               0| 1701783300|         null|  null|     0|kc3bfti|t3_18baicd|t1_kc34yud|  1701783318|   22|   false| relationship_advice|    t5_2r0cn|\n",
      "|            mynkgpta|                  null|             null|                True|               0| 1701783300|         null|  null|     0|kc3bftj|t3_18b96oa|t1_kc37mg9|  1701783318|    1|   false|DesiFragranceAddicts|   t5_52t40p|\n",
      "|      Key-Finger8664|                  null|             null|      r/badsaveouija|               0| 1701783300|         null|  null|     0|kc3bftk|t3_18au1dq|t1_kc0ut3u|  1701783318|    1|   false|            AskOuija|    t5_3eg3f|\n",
      "|   Savannathegoddess|                  null|             null|              Yes ðŸ˜›|               0| 1701783300|         null|  null|     0|kc3bftl|t3_18bbtwv|t1_kc3b3mg|  1701783318|    2|   false|   HornyMomsNextDoor|   t5_7navk7|\n",
      "|           Eltorak95|                  null|             null|That's false. Boo...|               0| 1701783300|         null|  null|     0|kc3bftm| t3_yv5joj|t1_iwd304x|  1701783318|    1|   false|  ProgressionFantasy|    t5_x7vh9|\n",
      "|          Moron_Noxa|                  null|             null|Yep, you will get...|               0| 1701783300|         null|  null|     0|kc3bftn|t3_18bbrc4|t3_18bbrc4|  1701783318|   29|   false|   MobileLegendsGame|    t5_hfsb5|\n",
      "|   Outside_Coach_361|                  null|             null|I want to ship Ca...|               0| 1701783300|         null|  null|     0|kc3bfto|t3_18arp2o|t3_18arp2o|  1701783318|    3|   false|      HonkaiStarRail|   t5_4q5zoc|\n",
      "|       AutoModerator|                  null|             null|/u/Jorddype\\nYour...|               0| 1701783300|    moderator|  null|     0|kc3bftp|t3_18bc2xz|t3_18bc2xz|  1701783318|    1|   false|       HungryArtists|    t5_2wx1w|\n",
      "|              bemvee|                  null|             null|Adding that for m...|               0| 1701783300|         null|  null|     0|kc3bftq|t3_18babh6|t1_kc3be1d|  1701783318|    3|   false|    DreamlightValley|   t5_5xpg1t|\n",
      "|          TimmyOTule|                  null|             null|    It was fake man.|               1| 1701783300|         null|  null|     0|kc3bftr|t3_18b2s9m|t3_18b2s9m|  1701783318|    0|   false|          conspiracy|    t5_2qh4r|\n",
      "|           TheGMPS69|                  null|             null|Ill accidentally ...|               0| 1701783300|         null|  null|     0|kc3bfts|t3_18apvwp|t3_18apvwp|  1701783318|    1|   false|      CougarsForCubs|    t5_36djn|\n",
      "|     Bee-El-Zu-Bubba|                  null|     ê™„â†„arÉ˜â†„law ðŸ–‘|No, you cannot co...|               0| 1701783300|         null|  null|     0|kc3bftu|t3_18bbu11|t3_18bbu11|  1701783318|    6|   false|    YuGiOhMasterDuel|   t5_3l4f55|\n",
      "|    MrsLolaLifestyle|                  null|             null|         Donâ€™t babe!|               0| 1701783300|         null|  null|     0|kc3bftv|t3_189k0p7|t1_kc3ba8a|  1701783318|    1|   false|        mombootynsfw|   t5_9a70e4|\n",
      "+--------------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 31 ms, sys: 2.5 ms, total: 33.5 ms\n",
      "Wall time: 21.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "comments = spark.read.parquet(\n",
    "    \"s3a://bigdatateaching/reddit-project/reddit/parquet/comments/yyyy=*/mm=*/*comments*.parquet\",\n",
    "    header=True\n",
    ")\n",
    "comments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+-----------------+-----------+-------------+--------------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+--------------------+------------+--------------------+--------------------+\n",
      "|              author|author_flair_css_class|author_flair_text|created_utc|distinguished|              domain|edited|     id|is_self|locked|num_comments|over_18|quarantine|retrieved_on|score|            selftext|stickied|           subreddit|subreddit_id|               title|                 url|\n",
      "+--------------------+----------------------+-----------------+-----------+-------------+--------------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+--------------------+------------+--------------------+--------------------+\n",
      "|     Newleaftherapy2|                  null|             null| 1689068092|         null|self.Newleaftherapy2|  null|14wmex5|   true| false|           0|  false|     false|  1689068113|    1| \\n\\nFind relief ...|   false|   u_Newleaftherapy2|   t5_8l1kf2|Discover CBT for ...|https://www.reddi...|\n",
      "|         wcooper1298|                  null|             null| 1689068092|         null|           i.redd.it|  null|14wmex6|  false| false|           0|   true|     false|  1689068113|    1|                    |   false|   GaySnapchatImages|    t5_2xdxr|21 straight dude....|https://i.redd.it...|\n",
      "|     Kattie_Meluanie|              Verified|         Verified| 1689068093|         null|           i.redd.it|  null|14wmex7|  false| false|           0|   true|     false|  1689068113|    1|                    |   false|              mombod|    t5_3ottt|I never get tired...|https://i.redd.it...|\n",
      "|     sequence_string|                  null|             null| 1689068093|         null|           i.redd.it|  null|14wmex8|  false| false|           0|  false|     false|  1689068113|    1|                    |   false|           wallpaper|    t5_2qmjl|A Quiet Moment Be...|https://i.redd.it...|\n",
      "|      mmmttisngksnsh|                  null|             null| 1689068093|         null|           i.redd.it|  null|14wmex9|  false| false|           0|  false|     false|  1689068113|    1|                    |   false|             firefly|    t5_2qs24|Does anyone need ...|https://i.redd.it...|\n",
      "|       antoniipotkon|                  null|             null| 1689068093|         null|           i.redd.it|  null|14wmexa|  false| false|           4|   true|     false|  1689068113|    1|                    |   false|     Pickonepornstar|    t5_3kfxz|Create your own H...|https://i.redd.it...|\n",
      "|        KinkyKorra27|                  null|  Unverified User| 1689068093|         null|           i.redd.it|  null|14wmexb|  false| false|           0|   true|     false|  1689068113|    1|                    |   false|     FullBackPanties|    t5_3i6no|My panties match ...|https://i.redd.it...|\n",
      "|PassageIntelligent55|                  null|             null| 1689068093|         null|self.SpringfieldM...|  null|14wmexc|   true| false|           0|   true|     false|  1689068113|    1|                    |   false|SpringfieldMO_Hoo...|   t5_88dztd|(F4M)ðŸ˜ðŸ˜ I'm dow...|https://www.reddi...|\n",
      "|           igorski81|                  null|             null| 1689068093|         null|drosophelia.bandc...|  null|14wmexd|  false| false|           1|  false|     false|  1689068113|    1|                    |   false|            BandCamp|    t5_2swys|4 Track \"Industri...|https://drosophel...|\n",
      "|SuperbAmbassador3086|                  null|             null| 1689068093|         null|self.Sissy_humili...|  null|14wmexe|   true| false|           0|   true|     false|  1689068113|    1|                    |   false|   Sissy_humiliation|    t5_2tnuq|25 (F4M) No roomm...|https://www.reddi...|\n",
      "|  Content-Aside-3378|                  null|             null| 1689068093|         null|self.couplessexpi...|  null|14wmexf|   true| false|           0|   true|     false|  1689068113|    1|                    |   false|   couplessexpicswap|   t5_46jvuk|Trading my 21 yo ...|https://www.reddi...|\n",
      "|   Excellent_Ear_951|                  null|             null| 1689068093|         null|         i.imgur.com|  null|14wmexg|  false| false|           0|   true|     false|  1689068113|    1|                    |   false| u_Excellent_Ear_951|   t5_4gvpsa|   Fully in comments|https://i.imgur.c...|\n",
      "|         WhoAmIEven2|                  null|             null| 1689068093|         null|           i.redd.it|  null|14wmexh|  false| false|           0|  false|     false|  1689068113|    1|                    |   false|              gaming|    t5_2qh03|Left was made in ...|https://i.redd.it...|\n",
      "|  Correct-Prompt6887|                  null|             null| 1689068093|         null|          self.KGBTR|  null|14wmexi|   true| false|           1|  false|     false|  1689068113|    1|Beyler telegram i...|   false|               KGBTR|   t5_12t6sw|Beyler telegram i...|https://www.reddi...|\n",
      "|           3188steve|                  null|             null| 1689068093|         null|self.DickPicReque...|  null|14wmexj|   true|  true|           0|   true|     false|  1689068113|    1|              Age 55|   false|    DickPicRequestv2|   t5_7x4wox|(Age 55) Requesti...|https://www.reddi...|\n",
      "|          dhk_yunlin|                  null|             null| 1689068093|         null|self.BangladeshGo...|  null|14wmexk|   true| false|           0|   true|     false|  1689068113|    1|Have you ever che...|   false|  BangladeshGoneSexy|   t5_6o4kjn|   Cheating Make Out|https://www.reddi...|\n",
      "|       Hot-Snow-2226|                  null|             null| 1689068093|         null|            5link.id|  null|14wmexl|  false| false|           0|  false|     false|  1689068113|    1|                    |   false|     u_Hot-Snow-2226|   t5_8no7qf|    alexis fawx nude|https://5link.id/...|\n",
      "|    ChattyCollegeBoy|                  null|             null| 1689068094|         null|           i.redd.it|  null|14wmexm|  false| false|           0|   true|     false|  1689068113|    1|                    |   false|      jerkbudsssssss|   t5_7nqfwp|22 [M4T] [Snap] h...|https://i.redd.it...|\n",
      "|      BourbonPursuit|                  null|             null| 1689068094|         null|  bourbonpursuit.com|  null|14wmexn|  false| false|           0|  false|     false|  1689068113|    1|                    |   false|      bourbonpursuit|   t5_29jgyt|Whiskey Quickie: ...|https://bourbonpu...|\n",
      "|  sweetrollscedarpie|                  null|             null| 1689068094|         null|    self.DealsIrised|  null|14wmexo|   true| false|           0|  false|     false|  1689068113|    1|Check out the lin...|   false|         DealsIrised|   t5_7sw5lf|Any Promo Codes f...|https://www.reddi...|\n",
      "+--------------------+----------------------+-----------------+-----------+-------------+--------------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+--------------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 7.72 ms, sys: 489 Î¼s, total: 8.21 ms\n",
      "Wall time: 7.25 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submissions = spark.read.parquet(\n",
    "    \"s3a://bigdatateaching/reddit-project/reddit/parquet/submissions/yyyy=*/mm=*/*submissions*.parquet\",\n",
    "    header=True\n",
    ")\n",
    "submissions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================================>(637 + 1) / 638]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the submissions dataframe is 567,890,869x21\n",
      "CPU times: user 57.4 ms, sys: 14 ms, total: 71.4 ms\n",
      "Wall time: 31.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================================>(3674 + 3) / 3677]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 3,675,768,958x17\n",
      "CPU times: user 229 ms, sys: 58.4 ms, total: 287 ms\n",
      "Wall time: 2min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "Let us find the number of submissions and comments per subreddit. We will use SparkSQL for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submissions.createOrReplaceTempView(\"submissions\")\n",
    "comments.createOrReplaceTempView(\"comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:================================================>       (19 + 3) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|           subreddit|  count|\n",
      "+--------------------+-------+\n",
      "|            dirtyr4r|3633005|\n",
      "|         JerkOffChat|3616759|\n",
      "|           AskReddit|2686082|\n",
      "|   GaySnapchatImages|2655639|\n",
      "|    GaySnapchatShare|2641484|\n",
      "|       DirtySnapchat|1986200|\n",
      "|      PokemonGoRaids|1341799|\n",
      "|     slutsofsnapchat|1334782|\n",
      "|PersonalizedGameRecs|1325223|\n",
      "|  HentaiAndRoleplayy|1315212|\n",
      "|   u_GrownBrilliance|1310625|\n",
      "|   MonopolyGoTrading|1234245|\n",
      "| relationship_advice|1149649|\n",
      "|         MassiveCock|1096306|\n",
      "|       DirtyChatPals|1024807|\n",
      "|         Monopoly_GO| 970421|\n",
      "|       AutoNewspaper| 946314|\n",
      "|              GOONED| 917121|\n",
      "|          ratemycock| 898375|\n",
      "|    DickPicRequestv2| 863552|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 198 ms, sys: 40.1 ms, total: 238 ms\n",
      "Wall time: 1min 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sql_str=\"select subreddit, count(id) as count from submissions group by subreddit order by count desc\"\n",
    "submissions_subreddit_count = spark.sql(sql_str)\n",
    "submissions_subreddit_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process S3 data with SageMaker Processing Job `PySparkProcessor`\n",
    "\n",
    "We are going to move the above processing code in a Python file and then submit that file to SageMaker Processing Job's [`PySparkProcessor`](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#pysparkprocessor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code/process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./code/process.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path_commments\", type=str, help=\"Path of dataset in S3 for reddit comments\")\n",
    "    parser.add_argument(\"--s3_dataset_path_submissions\", type=str, help=\"Path of dataset in S3 for reddit submissions\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_prefix\", type=str, help=\"s3 output prefix\")\n",
    "    parser.add_argument(\"--subreddits\", type=str, help=\"comma separate list of subreddits of interest\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "    logger.info(f\"spark version = {spark.version}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "   \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path_commments}\")\n",
    "    comments = spark.read.parquet(args.s3_dataset_path_commments, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    logger.info(f\"going to read {args.s3_dataset_path_submissions}\")\n",
    "    submissions = spark.read.parquet(args.s3_dataset_path_submissions, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # filter the dataframe to only keep the subreddits of interest\n",
    "    subreddits = [s.strip() for s in args.subreddits.split(\",\")]\n",
    "    excluded_cols = [] # ['edited', 'created_utc', 'retrieved_on']\n",
    "    \n",
    "    comments_included_cols = [c for c in comments.columns if c not in excluded_cols]\n",
    "    logger.info(f\"comments included_cols={comments_included_cols}\")\n",
    "\n",
    "    submissions_included_cols = [c for c in submissions.columns if c not in excluded_cols]\n",
    "    logger.info(f\"submissions included_cols={submissions_included_cols}\")\n",
    "\n",
    "    # subset the dataframes because \"edited\" and \"created_utc\" have data type problems\n",
    "    # sometimes they occur as int some time as float and since schema is encoded in the \n",
    "    # parquet files therefore different files have different data tpyes for these fields (float in some cases, int in some cases)\n",
    "    # and spark enforces strict type checking on read so the only option we have is to either\n",
    "    # fix this outside of spark or delete these columns.\n",
    "    comments = comments.select(comments_included_cols)\n",
    "    submissions = submissions.select(submissions_included_cols)\n",
    "    \n",
    "    submissions_filtered = submissions.where(lower(col(\"subreddit\")).isin(subreddits))\n",
    "    comments_filtered = comments.where(lower(col(\"subreddit\")).isin(subreddits))\n",
    "    \n",
    "    # save the filtered dataframes so that these files can now be used for future analysis\n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/comments\"\n",
    "    logger.info(f\"going to write comments for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the comments_filtered dataframe is {comments_filtered.count():,}x{len(comments_filtered.columns)}\")\n",
    "    comments_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    \n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/submissions\"\n",
    "    logger.info(f\"going to write submissions for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the submissions_filtered dataframe is {submissions_filtered.count():,}x{len(submissions_filtered.columns)}\")\n",
    "    submissions_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now submit this code to SageMaker Processing Job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Change the following line `subreddits = \"aws,azure\"` in the cell below to filter for your subreddits of interest. Note that this is a lot of data so this operation could easily take more than 30 minutes or more (increase the max_runtime_in_seconds from 3600 to 7200 if you see a timeout).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "......................................................................................................................................................................................................................................................................................................................................................................!CPU times: user 3.82 s, sys: 406 ms, total: 4.22 s\n",
      "Wall time: 30min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=4,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_dataset_path_commments = \"s3://bigdatateaching/reddit-project/reddit/parquet/comments/yyyy=*/mm=*/*.parquet\"\n",
    "s3_dataset_path_submissions = \"s3://bigdatateaching/reddit-project/reddit/parquet/submissions/yyyy=*/mm=*/*.parquet\"\n",
    "output_prefix_data = \"project\"\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "\n",
    "# modify this comma separated list to choose the subreddits of interest\n",
    "#subreddits = \"technology,chatgpt\"\n",
    "subreddits = \"aws,azure\"\n",
    "    \n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/process.py\",\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path_commments\",\n",
    "        s3_dataset_path_commments,\n",
    "        \"--s3_dataset_path_submissions\",\n",
    "        s3_dataset_path_submissions,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_prefix\",\n",
    "        output_prefix_data,\n",
    "        \"--subreddits\",\n",
    "        subreddits,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the filtered data\n",
    "\n",
    "Now that we have filtered the data to only keep submissions and comments from subreddits of interest. Let us read data from the s3 path where we saved the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading submissions from s3a://sagemaker-us-east-1-015469603702/project/comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=====================================================>(114 + 1) / 115]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 188,965x17\n",
      "CPU times: user 46.7 ms, sys: 2.62 ms, total: 49.3 ms\n",
      "Wall time: 1min 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data}/comments\"\n",
    "print(f\"reading submissions from {s3_path}\")\n",
    "comments = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: double (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------------+----------+----------+-------+-----------+\n",
      "|subreddit|        author|                body| parent_id|   link_id|     id|created_utc|\n",
      "+---------+--------------+--------------------+----------+----------+-------+-----------+\n",
      "|      aws|       Natuuls|         Skill issue|t3_1av0o2g|t3_1av0o2g|kr7ii0t| 1708382916|\n",
      "|    AZURE|     206SEATTL|Thank you, much a...|t1_kr6fkd7|t3_1auv8p8|kr7ij7m| 1708382929|\n",
      "|      aws|  light_odin05|You can make it s...|t1_kr7gqii|t3_1av0o2g|kr7imzh| 1708382965|\n",
      "|      aws|       darvink|You are thinking ...|t3_1av0o2g|t3_1av0o2g|kr7iztp| 1708383091|\n",
      "|      aws|spurius_tadius|Yes, thanks!\\n\\nT...|t1_kr7gggr|t3_1autiak|kr7j43a| 1708383132|\n",
      "|      aws|     nevaNevan|I donâ€™t have your...|t3_1av0o2g|t3_1av0o2g|kr7j4ur| 1708383140|\n",
      "|    AZURE|       Netskyz|Eating crumpets d...|t1_kr6si28|t3_1aut6p4|kr7j7db| 1708383165|\n",
      "|      aws|wafflefriesdev|Azure is very Win...|t3_1av0st2|t3_1av0st2|kr7jdf4| 1708383225|\n",
      "|      aws|  light_odin05|I'd advise you to...|t3_1av0o2g|t3_1av0o2g|kr7jezk| 1708383241|\n",
      "|    AZURE|       rdhdpsy|I think aws has a...|t3_1av0t6v|t3_1av0t6v|kr7jg4b| 1708383252|\n",
      "|    AZURE| CalvinCalhoun|Hahahahaha this g...|t1_kr7jg4b|t3_1av0t6v|kr7jj11| 1708383281|\n",
      "|      aws|      bunny_go|I think you might...|t1_kr7iztp|t3_1av0o2g|kr7jjra| 1708383288|\n",
      "|      aws|      bunny_go|           hahahahah|t1_kr7jezk|t3_1av0o2g|kr7js5l| 1708383372|\n",
      "|      aws|     hanneninn|thanks for your r...|t1_kr7hzif|t3_1av0st2|kr7js7e| 1708383372|\n",
      "|    AZURE|  YakAttack666|                Both|t3_1av0t6v|t3_1av0t6v|kr7jzdy| 1708383443|\n",
      "|      aws|      bunny_go|Thanks for sharin...|t1_kr7j4ur|t3_1av0o2g|kr7k5po| 1708383506|\n",
      "|      aws|chazmichaels15|         Skill issue|t1_kr7ii0t|t3_1av0o2g|kr7k8bf| 1708383532|\n",
      "|    AZURE|      ZABurner|Is it possible to...|t1_kr6g8at|t3_1auq6qj|kr7k9xh| 1708383549|\n",
      "|      aws|crystalpeaks25|when you only res...|t3_1av0o2g|t3_1av0o2g|kr7kem1| 1708383597|\n",
      "|      aws|      bunny_go|With this \"scale\"...|t1_kr7gqii|t3_1av0o2g|kr7kffy| 1708383605|\n",
      "+---------+--------------+--------------------+----------+----------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display a subset of columns\n",
    "comments.select(\"subreddit\", \"author\", \"body\", \"parent_id\", \"link_id\", \"id\", \"created_utc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading submissions from s3a://sagemaker-us-east-1-015469603702/project/submissions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=================================================>      (16 + 2) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the submissions dataframe is 31,302x21\n",
      "CPU times: user 10.4 ms, sys: 0 ns, total: 10.4 ms\n",
      "Wall time: 13.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data}/submissions\"\n",
    "print(f\"reading submissions from {s3_path}\")\n",
    "submissions = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- edited: double (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_self: boolean (nullable = true)\n",
      " |-- locked: boolean (nullable = true)\n",
      " |-- num_comments: long (nullable = true)\n",
      " |-- over_18: boolean (nullable = true)\n",
      " |-- quarantine: boolean (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- selftext: string (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submissions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+------------+-----------+\n",
      "|subreddit|              author|               title|            selftext|num_comments|created_utc|\n",
      "+---------+--------------------+--------------------+--------------------+------------+-----------+\n",
      "|    AZURE|      dxrkinfuser_44|Azure for Student...|Hey there to ever...|           0| 1715050111|\n",
      "|    AZURE|           EndNo4852|Hang on Create Fu...|Trying to create ...|           0| 1715051024|\n",
      "|    AZURE|         ping-friend|Azure Update Mana...|Could you share h...|           3| 1715051204|\n",
      "|    AZURE|           EndNo4852|Hang on Function App|Trying to create ...|           5| 1715051211|\n",
      "|      aws|       LostOnEarth82|  Password keys help|So I hired some s...|          10| 1715051470|\n",
      "|      aws| SalamanderReady6680|Cloud Computing A...|Howdy yall, Iâ€™m 2...|           2| 1715051939|\n",
      "|      aws|      shivvaaaaakeee|Kubernetes with H...|           [removed]|           0| 1715051984|\n",
      "|    AZURE|         aljandeleon|Databricks with A...|We are planning t...|           0| 1715052105|\n",
      "|    AZURE|         ryan_qumulo|HPC storage - doe...|           [removed]|           0| 1715052836|\n",
      "|    AZURE|           ryanwolfh|Ask an Azure Data...|Data people! Micr...|           0| 1715053488|\n",
      "|      aws|           Minimus-7|Recently Unemploy...|           [removed]|           0| 1715053817|\n",
      "|      aws|             issfire|AWS Lambda => API...|Our current archi...|          12| 1715055361|\n",
      "|      aws|     DigitalSplendid|EC2: How to know ...|    ubuntu@ip-172...|           8| 1715055606|\n",
      "|    AZURE|Delicious_Success303|500 Internal Serv...|Hi all!! I have b...|           1| 1715059010|\n",
      "|      aws|        rangedMisfit|How secure is thi...|Noob question her...|           9| 1715065682|\n",
      "|      aws|            Chip1812|Seeking Advice on...| Hi everyone,\\n\\n...|           6| 1715065885|\n",
      "|    AZURE|        OkBother4153|Storing  large do...|Is it okay use AI...|           0| 1715066097|\n",
      "|      aws|     UpstairsEye4793|Automatic pull re...|           [removed]|           0| 1715066397|\n",
      "|      aws|            Harsh_62|Need help in vect...|Hello all,\\n\\n**P...|           1| 1715068295|\n",
      "|      aws|         whole-fish-|Save Cognito refr...|           [removed]|           0| 1715069320|\n",
      "+---------+--------------------+--------------------+--------------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display a subset of columns\n",
    "submissions.select(\"subreddit\", \"author\", \"title\", \"selftext\", \"num_comments\", \"created_utc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
