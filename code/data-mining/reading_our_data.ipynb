{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Back Our Subsetted Data\n",
        "\n",
        "The data was subsetted from the Azure Blob location with the project data: wasbs://reddit-project@dsan6000fall2024.blob.core.windows.net/<DIRECTORY>/."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "40",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T17:11:54.0171347Z",
              "session_start_time": "2024-12-08T17:11:54.0443734Z",
              "execution_start_time": "2024-12-08T17:15:36.3679095Z",
              "execution_finish_time": "2024-12-08T17:15:38.8414394Z",
              "parent_msg_id": "58b09acb-bbc9-4826-87a0-15211b01a4e1"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 40, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x7f9e2fdd4940>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-97d89382:35003\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1.5.2.20240522.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1733678138966
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set up Data Configuration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing to make sure i can read the parquet file from the blob again\n",
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n",
        "\n",
        "# the parquet path again\n",
        "output_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/subset_job_0_1.parquet\"\n",
        "\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 8,
              "statement_ids": [
                8
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 52305,
                    "rowCount": 122,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 8:\n# testing to make sure i can read the parquet file from the blob again\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/subset_job_0_1.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-06T02:47:56.541GMT",
                    "completionTime": "2024-12-06T02:47:58.887GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 8:\n# testing to make sure i can read the parquet file from the blob again\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/subset_job_0_1.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-06T02:47:46.612GMT",
                    "completionTime": "2024-12-06T02:47:52.626GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "21",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T02:47:45.1806513Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T02:47:45.3357316Z",
              "execution_finish_time": "2024-12-06T02:48:00.2838271Z",
              "parent_msg_id": "4ee1487f-ddd4-45e7-82f5-4ca8824b847e"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 21, 8, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+---------------+------------+----+---+\n|              author|author_flair_css_class|author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded|     id|   link_id| parent_id|retrieved_on|score|stickied|      subreddit|subreddit_id|yyyy| mm|\n+--------------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+---------------+------------+----+---+\n|  Confident_Job_5357|                  null|             null|Hope you will fin...|               0| 1718029382|         null|  null|     0|l7yslh8|t3_1dbwjue|t1_l7ys91m|  1718029397|    1|   false|  BladderCancer|    t5_oi34n|2024|  6|\n|     OneEggplant6511|                  null|             null|Ask for a patient...|               0| 1718029388|         null|  null|     0|l7ysm4e|t3_1dc3o76|t3_1dc3o76|  1718029404|    1|   false|        nursing|    t5_2ra72|2024|  6|\n|Foreign_Afternoon_49|                  null|             null|Your post says th...|               0| 1718029402|         null|  null|     0|l7ysng9|t3_1dcj9vo|t1_l7ys1wz|  1718029420|    2|   false|HealthInsurance|    t5_2qnt0|2024|  6|\n|Responsible-Host-921|                  null|             null|   2-3 weeks I think|               0| 1718029402|         null|  null|     0|l7ysnj5|t3_1dafnpj|t3_1dafnpj|  1718029419|    1|   false|      predental|    t5_2ucup|2024|  6|\n|           [deleted]|                  null|             null|           [removed]|               0| 1718029476|         null|  null|     0|l7ysutc| t3_l29qck| t3_l29qck|  1718029495|    1|   false|  CrohnsDisease|    t5_2s453|2024|  6|\n+--------------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+---------------+------------+----+---+\nonly showing top 5 rows\n\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733453280402
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_row_count = df_read_back.count()\n",
        "comment_col_count = len(df_read_back.columns)\n",
        "print(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 9,
              "statement_ids": [
                9
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 472,
                    "rowCount": 8,
                    "usageDescription": "",
                    "jobId": 3,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-06T02:48:18.146GMT",
                    "completionTime": "2024-12-06T02:48:18.544GMT",
                    "stageIds": [
                      3,
                      4
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 472,
                    "dataRead": 1079750,
                    "rowCount": 11909,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-06T02:48:10.767GMT",
                    "completionTime": "2024-12-06T02:48:17.995GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "21",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T02:48:03.3324291Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T02:48:03.4562992Z",
              "execution_finish_time": "2024-12-06T02:48:20.9118712Z",
              "parent_msg_id": "9be2b119-2479-406d-bbc9-9c82cb450af7"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 21, 9, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape of the comments dataframe is 11,901x19\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733453301044
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the parquet path again\n",
        "output_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer/comments\"\n",
        "\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 11,
              "statement_ids": [
                11
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 224733,
                    "rowCount": 1745,
                    "usageDescription": "",
                    "jobId": 7,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 11:\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer/comments\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-06T02:49:10.523GMT",
                    "completionTime": "2024-12-06T02:49:10.792GMT",
                    "stageIds": [
                      8
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 6,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 11:\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer/comments\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-06T02:49:09.878GMT",
                    "completionTime": "2024-12-06T02:49:10.233GMT",
                    "stageIds": [
                      7
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "21",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T02:49:07.8484052Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T02:49:07.9919597Z",
              "execution_finish_time": "2024-12-06T02:49:12.2134653Z",
              "parent_msg_id": "b9a64a77-cb34-44fd-a15d-055aafb0a559"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 21, 11, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-----------------+------------+----+---+\n|              author|author_flair_css_class|   author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded|     id|   link_id| parent_id|retrieved_on|score|stickied|        subreddit|subreddit_id|yyyy| mm|\n+--------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-----------------+------------+----+---+\n|        Own_Owl_6409|                  null|                null|It’s too late it’...|               0| 1714691160|         null|  null|     0|l2bh4qr|t3_1ciqqgk|t1_l2b0d5x|  1714691174|    8|   false|UlcerativeColitis|    t5_2tb9x|2024|  5|\n|          GoldAd9596|                  null|                null|I have a question...|               0| 1714691162|         null|  null|     0|l2bh4zz|t3_1cilpx7|t3_1cilpx7|  1714691177|    1|   false|        predental|    t5_2ucup|2024|  5|\n|              ivy007|                  null|BSN RN - Hematolo...|Not really. I’m j...|               0| 1714691164|         null|  null|     0|l2bh548|t3_1cfkkym|t1_l1sx6q1|  1714691181|    2|   false|          nursing|    t5_2ra72|2024|  5|\n|    Careless_Web2731|                  null|                null|Honestly. This is...|               0| 1714691184|         null|  null|     0|l2bh6vy|t3_1cilfk1|t1_l2a12qe|  1714691198|    3|   false|          nursing|    t5_2ra72|2024|  5|\n|RealisticShopping625|                  null|                null|I have being read...|               0| 1714691187|         null|  null|     0|l2bh73y| t3_j6ssbi| t3_j6ssbi|  1714691201|    1|   false|      coloncancer|    t5_3g1p0|2024|  5|\n+--------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-----------------+------------+----+---+\nonly showing top 5 rows\n\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733453352342
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_row_count = df_read_back.count()\n",
        "comment_col_count = len(df_read_back.columns)\n",
        "print(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 12,
              "statement_ids": [
                12
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 6961,
                    "rowCount": 118,
                    "usageDescription": "",
                    "jobId": 9,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 12:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-06T02:51:04.293GMT",
                    "completionTime": "2024-12-06T02:51:04.399GMT",
                    "stageIds": [
                      10,
                      11
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 119,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 118,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 6961,
                    "dataRead": 32044175,
                    "rowCount": 4070692,
                    "usageDescription": "",
                    "jobId": 8,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 12:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-06T02:50:20.685GMT",
                    "completionTime": "2024-12-06T02:51:04.235GMT",
                    "stageIds": [
                      9
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 118,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 118,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 118,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "21",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T02:50:20.2724987Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T02:50:20.5150115Z",
              "execution_finish_time": "2024-12-06T02:51:04.6842826Z",
              "parent_msg_id": "5f28ec1a-cfa1-4fb9-933b-9aa46651f5d8"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 21, 12, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape of the comments dataframe is 4,070,574x19\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733453464791
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the parquet path again\n",
        "output_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer/submissions\"\n",
        "\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 13,
              "statement_ids": [
                13
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 496129,
                    "rowCount": 1543,
                    "usageDescription": "",
                    "jobId": 11,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 13:\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer/submissions\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-06T02:51:07.880GMT",
                    "completionTime": "2024-12-06T02:51:08.185GMT",
                    "stageIds": [
                      13
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 10,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 13:\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer/submissions\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-06T02:51:07.427GMT",
                    "completionTime": "2024-12-06T02:51:07.574GMT",
                    "stageIds": [
                      12
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "21",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T02:51:06.9182559Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T02:51:07.0686399Z",
              "execution_finish_time": "2024-12-06T02:51:08.6586262Z",
              "parent_msg_id": "066bd3b3-8a83-4b6b-9271-02807e823e86"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 21, 13, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+----------------------+--------------------+-----------+-------------+---------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+---------+------------+--------------------+--------------------+----+---+\n|              author|author_flair_css_class|   author_flair_text|created_utc|distinguished|         domain|edited|     id|is_self|locked|num_comments|over_18|quarantine|retrieved_on|score|            selftext|stickied|subreddit|subreddit_id|               title|                 url|yyyy| mm|\n+--------------------+----------------------+--------------------+-----------+-------------+---------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+---------+------------+--------------------+--------------------+----+---+\n|Kitchen-Security5235|               default|Layperson/not ver...| 1722354598|         null|   self.AskDocs|  null|1efwtcc|   true| false|           1|  false|     false|  1722354612|    1|Hi everyone. I ha...|   false|  AskDocs|    t5_2xtuc|         Concussion?|https://www.reddi...|2024|  7|\n|    One_Program_1613|                  null|                null| 1722354660|         null|digistore24.com|  null|1efwu99|  false| false|           0|  false|     false|  1722354676|    1|                    |   false|   Health|    t5_2qh9z|These \"unusual ne...|https://www.digis...|2024|  7|\n|      heyheyitsamber|               default|Layperson/not ver...| 1722354710|         null|   self.AskDocs|  null|1efwuz6|   true| false|           2|  false|     false|  1722354724|    0|25 year old femal...|   false|  AskDocs|    t5_2xtuc|Should I still ta...|https://www.reddi...|2024|  7|\n|       oldsoul070924|                  null|                null| 1722354827|         null|   self.AskDocs|  null|1efwwrg|   true| false|           1|  false|     false|  1722354843|    1|           [removed]|   false|  AskDocs|    t5_2xtuc|Mole on nose got ...|https://www.reddi...|2024|  7|\n|              jjjaax|               default|Layperson/not ver...| 1722354938|         null|   self.AskDocs|  null|1efwygz|   true| false|           3|  false|     false|  1722354953|    1|27 year old male,...|   false|  AskDocs|    t5_2xtuc|Is this Skin Canc...|https://www.reddi...|2024|  7|\n+--------------------+----------------------+--------------------+-----------+-------------+---------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+---------+------------+--------------------+--------------------+----+---+\nonly showing top 5 rows\n\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733453468753
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_row_count = df_read_back.count()\n",
        "comment_col_count = len(df_read_back.columns)\n",
        "print(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 14,
              "statement_ids": [
                14
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 1180,
                    "rowCount": 20,
                    "usageDescription": "",
                    "jobId": 13,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 14:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-06T02:51:21.359GMT",
                    "completionTime": "2024-12-06T02:51:21.406GMT",
                    "stageIds": [
                      15,
                      16
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 21,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 20,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 1180,
                    "dataRead": 6026250,
                    "rowCount": 649025,
                    "usageDescription": "",
                    "jobId": 12,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 14:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-06T02:51:14.317GMT",
                    "completionTime": "2024-12-06T02:51:21.315GMT",
                    "stageIds": [
                      14
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "21",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T02:51:14.0775989Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T02:51:14.1907438Z",
              "execution_finish_time": "2024-12-06T02:51:22.3761313Z",
              "parent_msg_id": "e16b89ba-ba12-4bed-9c1f-cd17c1247992"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 21, 14, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape of the comments dataframe is 649,005x23\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733453482477
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lower\n",
        "\n",
        "# sample approximately 0.001% of the data and limit to 50 rows\n",
        "submissions_subset_df = df_read_back.sample(withReplacement=False, fraction=0.001, seed=42).limit(100)\n",
        "\n",
        "# Display the first few rows of subset DataFrame\n",
        "submissions_subset_df.show(5)\n",
        "\n",
        "# Display the size (number of rows) in the df\n",
        "print(f\"Number of rows in the sampled and limited DataFrame: {submissions_subset_df.count()}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 17,
              "statement_ids": [
                17
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 4,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 24,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\nfrom pyspark.sql.functions import col, lower\n\n# sample approximately 0.001% of the data and limit to 50 rows\nsubmissions_subset_df = df_read_back.sample(withReplacement=False, fraction=0.001, seed=42).limit(100)\n\n# Display the first few rows of subset DataFrame\nsubmissions_subset_df.show(5)\n\n# Display the size (number of rows) in the df\nprint(f\"Number of rows in the sampled and limited DataFrame: {submissions_subset_df.count()}\")",
                    "submissionTime": "2024-12-06T02:52:19.127GMT",
                    "completionTime": "2024-12-06T02:52:19.191GMT",
                    "stageIds": [
                      31,
                      32
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 21,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 20,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 1007,
                    "dataRead": 3013125,
                    "rowCount": 649630,
                    "usageDescription": "",
                    "jobId": 23,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\nfrom pyspark.sql.functions import col, lower\n\n# sample approximately 0.001% of the data and limit to 50 rows\nsubmissions_subset_df = df_read_back.sample(withReplacement=False, fraction=0.001, seed=42).limit(100)\n\n# Display the first few rows of subset DataFrame\nsubmissions_subset_df.show(5)\n\n# Display the size (number of rows) in the df\nprint(f\"Number of rows in the sampled and limited DataFrame: {submissions_subset_df.count()}\")",
                    "submissionTime": "2024-12-06T02:52:15.288GMT",
                    "completionTime": "2024-12-06T02:52:19.082GMT",
                    "stageIds": [
                      30
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 22,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\nfrom pyspark.sql.functions import col, lower\n\n# sample approximately 0.001% of the data and limit to 50 rows\nsubmissions_subset_df = df_read_back.sample(withReplacement=False, fraction=0.001, seed=42).limit(100)\n\n# Display the first few rows of subset DataFrame\nsubmissions_subset_df.show(5)\n\n# Display the size (number of rows) in the df\nprint(f\"Number of rows in the sampled and limited DataFrame: {submissions_subset_df.count()}\")",
                    "submissionTime": "2024-12-06T02:52:14.988GMT",
                    "completionTime": "2024-12-06T02:52:15.092GMT",
                    "stageIds": [
                      28,
                      29
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 21,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 20,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 392104,
                    "dataRead": 204489709,
                    "rowCount": 649630,
                    "usageDescription": "",
                    "jobId": 21,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\nfrom pyspark.sql.functions import col, lower\n\n# sample approximately 0.001% of the data and limit to 50 rows\nsubmissions_subset_df = df_read_back.sample(withReplacement=False, fraction=0.001, seed=42).limit(100)\n\n# Display the first few rows of subset DataFrame\nsubmissions_subset_df.show(5)\n\n# Display the size (number of rows) in the df\nprint(f\"Number of rows in the sampled and limited DataFrame: {submissions_subset_df.count()}\")",
                    "submissionTime": "2024-12-06T02:52:07.285GMT",
                    "completionTime": "2024-12-06T02:52:14.879GMT",
                    "stageIds": [
                      27
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "21",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T02:52:06.5541479Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T02:52:07.0286919Z",
              "execution_finish_time": "2024-12-06T02:52:20.338556Z",
              "parent_msg_id": "475b1eba-c345-422a-85cf-863f506f0ef4"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 21, 17, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+-----------------+----------------------+--------------------+-----------+-------------+------------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+-------------+------------+--------------------+--------------------+----+---+\n|           author|author_flair_css_class|   author_flair_text|created_utc|distinguished|            domain|edited|     id|is_self|locked|num_comments|over_18|quarantine|retrieved_on|score|            selftext|stickied|    subreddit|subreddit_id|               title|                 url|yyyy| mm|\n+-----------------+----------------------+--------------------+-----------+-------------+------------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+-------------+------------+--------------------+--------------------+----+---+\n|LiveLongAndPerish|                  null|                null| 1715258812|         null|self.CrohnsDisease|  null|1cnwflm|   true| false|           5|  false|     false|  1715258828|    4|Hi, I hope this i...|   false|CrohnsDisease|    t5_2s453|Crohns and pregnancy|https://www.reddi...|2024|  5|\n|       I_Am_Towel|               default|Layperson/not ver...| 1715265448|         null|      self.AskDocs|  null|1cnytul|   true| false|           4|  false|     false|  1715265465|    1|33 male, i have d...|   false|      AskDocs|    t5_2xtuc|CRP 22mg/L with n...|https://www.reddi...|2024|  5|\n|       PotentHeck|               default|Layperson/not ver...| 1715274768|         null|      self.AskDocs|  null|1co2fuo|   true| false|           1|  false|     false|  1715274785|    1|           [removed]|   false|      AskDocs|    t5_2xtuc|Prescribed 50,000...|https://www.reddi...|2024|  5|\n| Warm-Cheetah8039|               default|Layperson/not ver...| 1715300107|         null|      self.AskDocs|  null|1coc91d|   true| false|           1|  false|     false|  1715300122|    1|           [removed]|   false|      AskDocs|    t5_2xtuc|    Blood discharge |https://www.reddi...|2024|  5|\n|    KingofGerudos|               default|Layperson/not ver...| 1721068340|         null|      self.AskDocs|  null|1e42ao3|   true| false|           2|  false|     false|  1721068360|    1|83F, Cancer dx (u...|   false|      AskDocs|    t5_2xtuc|Help me help my f...|https://www.reddi...|2024|  7|\n+-----------------+----------------------+--------------------+-----------+-------------+------------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+-------------+------------+--------------------+--------------------+----+---+\nonly showing top 5 rows\n\nNumber of rows in the sampled and limited DataFrame: 100\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733453540451
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n",
        "\n",
        "\n",
        "# Save the filtered subset to a Parquet file\n",
        "output_path = f\"{workspace_wasbs_base_url}sample_submissions.parquet\"\n",
        "submissions_subset_df.write.parquet(output_path, mode=\"overwrite\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 8,
              "statement_ids": [
                8
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "40",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T17:17:28.1474744Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T17:17:28.3186136Z",
              "execution_finish_time": "2024-12-08T17:17:28.6635747Z",
              "parent_msg_id": "85886e5b-76a9-4ff3-af0d-e09991039d43"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 40, 8, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'submissions_subset_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [15], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Save the filtered subset to a Parquet file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworkspace_wasbs_base_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124msample_submissions.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43msubmissions_subset_df\u001b[49m\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(output_path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'submissions_subset_df' is not defined"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733678248726
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Read the Sample of our Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 24,
              "statement_ids": [
                24
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "21",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T03:00:10.0291708Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T03:00:10.1530265Z",
              "execution_finish_time": "2024-12-06T03:00:10.4653653Z",
              "parent_msg_id": "c7bc5e9d-1591-40a1-bc72-a0363f5ed7a6"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 21, 24, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 47,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x7f08b300dc00>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-3c588200:43741\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1.5.2.20240522.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733454010578
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n",
        "\n",
        "\n",
        "# Save the filtered subset to a Parquet file\n",
        "output_path = f\"{workspace_wasbs_base_url}sample_submissions.parquet\"\n",
        "sample = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "sample.show(5)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 25,
              "statement_ids": [
                25
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 70873,
                    "rowCount": 100,
                    "usageDescription": "",
                    "jobId": 37,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 25:\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n\n# Save the filtered subset to a Parquet file\noutput_path = f\"{workspace_wasbs_base_url}sample_submissions.parquet\"\nsample = spark.read.parquet(output_path)\n\n# Show first 5 rows\nsample.show(5)",
                    "submissionTime": "2024-12-06T03:00:12.886GMT",
                    "completionTime": "2024-12-06T03:00:13.003GMT",
                    "stageIds": [
                      48
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 36,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 25:\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n\n# Save the filtered subset to a Parquet file\noutput_path = f\"{workspace_wasbs_base_url}sample_submissions.parquet\"\nsample = spark.read.parquet(output_path)\n\n# Show first 5 rows\nsample.show(5)",
                    "submissionTime": "2024-12-06T03:00:12.616GMT",
                    "completionTime": "2024-12-06T03:00:12.731GMT",
                    "stageIds": [
                      47
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "21",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T03:00:12.318411Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T03:00:12.452547Z",
              "execution_finish_time": "2024-12-06T03:00:13.2648413Z",
              "parent_msg_id": "28c0913e-afbc-40a8-98de-dced022624a9"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 21, 25, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+-----------------+----------------------+--------------------+-----------+-------------+------------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+-------------+------------+--------------------+--------------------+----+---+\n|           author|author_flair_css_class|   author_flair_text|created_utc|distinguished|            domain|edited|     id|is_self|locked|num_comments|over_18|quarantine|retrieved_on|score|            selftext|stickied|    subreddit|subreddit_id|               title|                 url|yyyy| mm|\n+-----------------+----------------------+--------------------+-----------+-------------+------------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+-------------+------------+--------------------+--------------------+----+---+\n|LiveLongAndPerish|                  null|                null| 1715258812|         null|self.CrohnsDisease|  null|1cnwflm|   true| false|           5|  false|     false|  1715258828|    4|Hi, I hope this i...|   false|CrohnsDisease|    t5_2s453|Crohns and pregnancy|https://www.reddi...|2024|  5|\n|       I_Am_Towel|               default|Layperson/not ver...| 1715265448|         null|      self.AskDocs|  null|1cnytul|   true| false|           4|  false|     false|  1715265465|    1|33 male, i have d...|   false|      AskDocs|    t5_2xtuc|CRP 22mg/L with n...|https://www.reddi...|2024|  5|\n|       PotentHeck|               default|Layperson/not ver...| 1715274768|         null|      self.AskDocs|  null|1co2fuo|   true| false|           1|  false|     false|  1715274785|    1|           [removed]|   false|      AskDocs|    t5_2xtuc|Prescribed 50,000...|https://www.reddi...|2024|  5|\n| Warm-Cheetah8039|               default|Layperson/not ver...| 1715300107|         null|      self.AskDocs|  null|1coc91d|   true| false|           1|  false|     false|  1715300122|    1|           [removed]|   false|      AskDocs|    t5_2xtuc|    Blood discharge |https://www.reddi...|2024|  5|\n|    KingofGerudos|               default|Layperson/not ver...| 1721068340|         null|      self.AskDocs|  null|1e42ao3|   true| false|           2|  false|     false|  1721068360|    1|83F, Cancer dx (u...|   false|      AskDocs|    t5_2xtuc|Help me help my f...|https://www.reddi...|2024|  7|\n+-----------------+----------------------+--------------------+-----------+-------------+------------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+-------------+------------+--------------------+--------------------+----+---+\nonly showing top 5 rows\n\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733454013372
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_row_count = sample.count()\n",
        "comment_col_count = len(sample.columns)\n",
        "print(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 26,
              "statement_ids": [
                26
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 59,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 39,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 26:\ncomments_row_count = sample.count()\ncomment_col_count = len(sample.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-06T03:00:16.913GMT",
                    "completionTime": "2024-12-06T03:00:16.955GMT",
                    "stageIds": [
                      51,
                      50
                    ],
                    "jobGroup": "26",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 1,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 59,
                    "dataRead": 5188,
                    "rowCount": 101,
                    "usageDescription": "",
                    "jobId": 38,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 26:\ncomments_row_count = sample.count()\ncomment_col_count = len(sample.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-06T03:00:16.711GMT",
                    "completionTime": "2024-12-06T03:00:16.863GMT",
                    "stageIds": [
                      49
                    ],
                    "jobGroup": "26",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "21",
              "normalized_state": "finished",
              "queued_time": "2024-12-06T03:00:16.3314408Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-06T03:00:16.4843844Z",
              "execution_finish_time": "2024-12-06T03:00:17.4345911Z",
              "parent_msg_id": "daa3ea2e-ce65-441d-af07-befc3bb250b0"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 21, 26, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape of the comments dataframe is 100x23\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733454017545
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing to make sure i can read the parquet file from the blob again\n",
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n",
        "\n",
        "# the parquet path again\n",
        "output_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer_subreddit.parquet\"\n",
        "\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)\n",
        "comments_row_count = df_read_back.count()\n",
        "comment_col_count = len(df_read_back.columns)\n",
        "print(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 8,
              "statement_ids": [
                8
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 4,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 59,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 5,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 8:\n# testing to make sure i can read the parquet file from the blob again\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer_subreddit.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-08T05:11:14.436GMT",
                    "completionTime": "2024-12-08T05:11:14.911GMT",
                    "stageIds": [
                      5,
                      6
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 1,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 59,
                    "dataRead": 5012,
                    "rowCount": 10001,
                    "usageDescription": "",
                    "jobId": 4,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 8:\n# testing to make sure i can read the parquet file from the blob again\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer_subreddit.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-08T05:11:13.565GMT",
                    "completionTime": "2024-12-08T05:11:14.151GMT",
                    "stageIds": [
                      4
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 2111428,
                    "rowCount": 4096,
                    "usageDescription": "",
                    "jobId": 3,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 8:\n# testing to make sure i can read the parquet file from the blob again\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer_subreddit.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-08T05:11:09.979GMT",
                    "completionTime": "2024-12-08T05:11:13.028GMT",
                    "stageIds": [
                      3
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 8:\n# testing to make sure i can read the parquet file from the blob again\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer_subreddit.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-08T05:11:03.187GMT",
                    "completionTime": "2024-12-08T05:11:09.654GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "38",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T05:11:02.6854989Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T05:11:02.8193367Z",
              "execution_finish_time": "2024-12-08T05:11:17.3730097Z",
              "parent_msg_id": "22fc55a5-904e-40e5-8882-5d5f01fc2ebf"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 38, 8, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-------------+------------+----+---+-----------------+\n|            author|author_flair_css_class|   author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded|     id|   link_id| parent_id|retrieved_on|score|stickied|    subreddit|subreddit_id|yyyy| mm|cleaned_subreddit|\n+------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-------------+------------+----+---+-----------------+\n| redhtbassplyr0311|                  null|         RN - ICU 🍕|Check out Northsi...|               0| 1714691222|         null|  null|     0|l2bha78|t3_1cir563|t3_1cir563|  1714691237|    2|   false|      nursing|    t5_2ra72|2024|  5|          nursing|\n|  vegan-princess01|                  null|                null|I had something s...|               0| 1714691276|         null|  null|     0|l2bhf47|t3_1cisin7|t3_1cisin7|  1714691291|    1|   false| WomensHealth|    t5_2rpzk|2024|  5|     womenshealth|\n|           Smirto9|                  null|                null|That's an insulti...|               0| 1714691284|         null|  null|     0|l2bhfug|t3_1chbacv|t3_1chbacv|  1714691299|    1|   false|      nursing|    t5_2ra72|2024|  5|          nursing|\n|       StaleWoolfe|               default|Layperson/not ver...|Yeah sorry, it wa...|               0| 1714691482|         null|  null|     0|l2bhxr1|t3_1cin7rh|t1_l2bh09i|  1714691499|    2|   false|      AskDocs|    t5_2xtuc|2024|  5|          askdocs|\n|BeautifulDreamerAZ|                  null|                null|I see my colorect...|               0| 1714691514|         null|  null|     0|l2bi0q4|t3_1cimzaw|t1_l2ap5xq|  1714691529|    1|   false|CrohnsDisease|    t5_2s453|2024|  5|    crohnsdisease|\n+------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-------------+------------+----+---+-----------------+\nonly showing top 5 rows\n\nshape of the comments dataframe is 10,000x20\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733634677528
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_read_back.select('body'))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 9,
              "statement_ids": [
                9
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "38",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T05:14:53.9695457Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T05:14:54.1394242Z",
              "execution_finish_time": "2024-12-08T05:14:54.4601309Z",
              "parent_msg_id": "8420deae-85bf-4f3c-9fba-990d75bc18a6"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 38, 9, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "DataFrame[body: string]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733634894589
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit.parquet\"\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)\n",
        "df_read_back.printSchema()\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)\n",
        "comments_row_count = df_read_back.count()\n",
        "comment_col_count = len(df_read_back.columns)\n",
        "print(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 9,
              "statement_ids": [
                9
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 5
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 59,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 4,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\noutput_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)\ndf_read_back.printSchema()\n# Show first 5 rows\ndf_read_back.show(5)\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-08T17:17:56.165GMT",
                    "completionTime": "2024-12-08T17:17:56.597GMT",
                    "stageIds": [
                      5,
                      4
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 1,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 59,
                    "dataRead": 9640,
                    "rowCount": 10001,
                    "usageDescription": "",
                    "jobId": 3,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\noutput_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)\ndf_read_back.printSchema()\n# Show first 5 rows\ndf_read_back.show(5)\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-08T17:17:53.634GMT",
                    "completionTime": "2024-12-08T17:17:56.018GMT",
                    "stageIds": [
                      3
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 2932823,
                    "rowCount": 4096,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\noutput_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)\ndf_read_back.printSchema()\n# Show first 5 rows\ndf_read_back.show(5)\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-08T17:17:52.829GMT",
                    "completionTime": "2024-12-08T17:17:53.146GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 2937643,
                    "rowCount": 4096,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\noutput_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)\ndf_read_back.printSchema()\n# Show first 5 rows\ndf_read_back.show(5)\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-08T17:17:45.978GMT",
                    "completionTime": "2024-12-08T17:17:52.390GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\noutput_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit.parquet\"\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)\ndf_read_back.printSchema()\n# Show first 5 rows\ndf_read_back.show(5)\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-08T17:17:34.254GMT",
                    "completionTime": "2024-12-08T17:17:42.384GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "40",
              "normalized_state": "finished",
              "queued_time": "2024-12-08T17:17:32.9699076Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-08T17:17:33.104687Z",
              "execution_finish_time": "2024-12-08T17:17:58.770397Z",
              "parent_msg_id": "b6077463-f7f1-4089-9582-b7ac0fc3f86e"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 40, 9, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+----------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-------------------+------------+----+---+--------------------+\n|          author|author_flair_css_class|author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded|     id|   link_id| parent_id|retrieved_on|score|stickied|          subreddit|subreddit_id|yyyy| mm|        cleaned_body|\n+----------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-------------------+------------+----+---+--------------------+\n|     LittleGrowl|                  null|             null|It’s like saying ...|               0| 1701783300|         null|  null|     0|kc3bfti|t3_18baicd|t1_kc34yud|  1701783318|   22|   false|relationship_advice|    t5_2r0cn|2023| 12|its like saying n...|\n|      TimmyOTule|                  null|             null|    It was fake man.|               1| 1701783300|         null|  null|     0|kc3bftr|t3_18b2s9m|t3_18b2s9m|  1701783318|    0|   false|         conspiracy|    t5_2qh4r|2023| 12|     it was fake man|\n| Bee-El-Zu-Bubba|                  null|     Ꙅↄarɘↄlaw 🖑|No, you cannot co...|               0| 1701783300|         null|  null|     0|kc3bftu|t3_18bbu11|t3_18bbu11|  1701783318|    6|   false|   YuGiOhMasterDuel|   t5_3l4f55|2023| 12|no you cannot com...|\n|    MothofTerror|                  null|             null|Wiser words have ...|               0| 1701783300|         null|  null|     0|kc3bfut|t3_18bb1dy|t1_kc37aji|  1701783318|    3|   false|         AmazonVine|   t5_28yu6q|2023| 12|wiser words have ...|\n|Odd-Location1287|                  null|             null|Cara vai sem pres...|               0| 1701783300|         null|  null|     0|kc3bfv1|t3_172y51n|t3_172y51n|  1701783318|    1|   false|    monsterhunterbr|   t5_3bcads|2023| 12|cara vai sem pres...|\n+----------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-------------------+------------+----+---+--------------------+\nonly showing top 5 rows\n\nroot\n |-- author: string (nullable = true)\n |-- author_flair_css_class: string (nullable = true)\n |-- author_flair_text: string (nullable = true)\n |-- body: string (nullable = true)\n |-- controversiality: long (nullable = true)\n |-- created_utc: long (nullable = true)\n |-- distinguished: string (nullable = true)\n |-- edited: double (nullable = true)\n |-- gilded: long (nullable = true)\n |-- id: string (nullable = true)\n |-- link_id: string (nullable = true)\n |-- parent_id: string (nullable = true)\n |-- retrieved_on: long (nullable = true)\n |-- score: long (nullable = true)\n |-- stickied: boolean (nullable = true)\n |-- subreddit: string (nullable = true)\n |-- subreddit_id: string (nullable = true)\n |-- yyyy: integer (nullable = true)\n |-- mm: integer (nullable = true)\n |-- cleaned_body: string (nullable = true)\n\n+----------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-------------------+------------+----+---+--------------------+\n|          author|author_flair_css_class|author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded|     id|   link_id| parent_id|retrieved_on|score|stickied|          subreddit|subreddit_id|yyyy| mm|        cleaned_body|\n+----------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-------------------+------------+----+---+--------------------+\n|     LittleGrowl|                  null|             null|It’s like saying ...|               0| 1701783300|         null|  null|     0|kc3bfti|t3_18baicd|t1_kc34yud|  1701783318|   22|   false|relationship_advice|    t5_2r0cn|2023| 12|its like saying n...|\n|      TimmyOTule|                  null|             null|    It was fake man.|               1| 1701783300|         null|  null|     0|kc3bftr|t3_18b2s9m|t3_18b2s9m|  1701783318|    0|   false|         conspiracy|    t5_2qh4r|2023| 12|     it was fake man|\n| Bee-El-Zu-Bubba|                  null|     Ꙅↄarɘↄlaw 🖑|No, you cannot co...|               0| 1701783300|         null|  null|     0|kc3bftu|t3_18bbu11|t3_18bbu11|  1701783318|    6|   false|   YuGiOhMasterDuel|   t5_3l4f55|2023| 12|no you cannot com...|\n|    MothofTerror|                  null|             null|Wiser words have ...|               0| 1701783300|         null|  null|     0|kc3bfut|t3_18bb1dy|t1_kc37aji|  1701783318|    3|   false|         AmazonVine|   t5_28yu6q|2023| 12|wiser words have ...|\n|Odd-Location1287|                  null|             null|Cara vai sem pres...|               0| 1701783300|         null|  null|     0|kc3bfv1|t3_172y51n|t3_172y51n|  1701783318|    1|   false|    monsterhunterbr|   t5_3bcads|2023| 12|cara vai sem pres...|\n+----------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-------------------+------------+----+---+--------------------+\nonly showing top 5 rows\n\nshape of the comments dataframe is 10,000x20\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733678278892
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}