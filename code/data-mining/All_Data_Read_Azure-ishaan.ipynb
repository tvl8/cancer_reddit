{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Queries Running on entire Dataset in Azure \n",
        "\n",
        "Using the Azure Blob location with the project data: wasbs://reddit-project@dsan6000fall2024.blob.core.windows.net/<DIRECTORY>/."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "14",
              "normalized_state": "finished",
              "queued_time": "2024-12-05T20:57:44.1290897Z",
              "session_start_time": "2024-12-05T20:57:44.1637034Z",
              "execution_start_time": "2024-12-05T21:01:52.5252623Z",
              "execution_finish_time": "2024-12-05T21:01:57.6407513Z",
              "parent_msg_id": "f6b7a2ce-9cb5-4653-8706-fc1b40071a91"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 14, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x7f35510c3a60>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-2ba21930:37615\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1.5.2.20240522.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1733432538243
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set up Data Configuration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob_account_name = \"dsan6000fall2024\"\n",
        "blob_container_name = \"reddit-project\"\n",
        "wasbs_base_url = (\n",
        "    f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 7,
              "statement_ids": [
                7
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "14",
              "normalized_state": "finished",
              "queued_time": "2024-12-05T21:02:44.1734144Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-05T21:02:44.3197594Z",
              "execution_finish_time": "2024-12-05T21:02:44.6747732Z",
              "parent_msg_id": "0014d314-7390-4842-8642-a44085603db6"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 14, 7, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733432585505
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Reading in a single parquet file "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_path = \"202306-202407/comments/\"\n",
        "submissions_path = \"202306-202407/submissions/\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 8,
              "statement_ids": [
                8
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "8",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T05:30:34.5260405Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T05:30:36.0507629Z",
              "execution_finish_time": "2024-12-01T05:30:36.3803895Z",
              "parent_msg_id": "4989e856-2ede-4f2c-a4e2-a44be6d5fe9a"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 8, 8, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733031036443
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test on a single file only (random file)\n",
        "\n",
        "# comments_single = \"yyyy=2023/mm=11/comments_RC_2023-11.zst_9.parquet\"\n",
        "# submissions_single = \"yyyy=2023/mm=11/submissions_RS_2023-11.zst_36.parquet\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 10,
              "statement_ids": [
                10
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:44:04.1045073Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:44:04.5351283Z",
              "execution_finish_time": "2024-11-24T21:44:05.6603384Z",
              "parent_msg_id": "6c207f9f-1ee9-4fc8-bb78-7c42dc1b88f5"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 4, 10, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732484645737
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# comments_single_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}{comments_single}\")\n",
        "# submissions_single_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}{submissions_single}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 11,
              "statement_ids": [
                11
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 11:\ncomments_single_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}{comments_single}\")\nsubmissions_single_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}{submissions_single}\")",
                    "submissionTime": "2024-11-24T21:44:15.165GMT",
                    "completionTime": "2024-11-24T21:44:15.367GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 11:\ncomments_single_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}{comments_single}\")\nsubmissions_single_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}{submissions_single}\")",
                    "submissionTime": "2024-11-24T21:44:07.684GMT",
                    "completionTime": "2024-11-24T21:44:13.833GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:44:06.6202349Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:44:06.7456341Z",
              "execution_finish_time": "2024-11-24T21:44:16.8325875Z",
              "parent_msg_id": "0b2d1f97-36cf-444c-90d1-40ea2ad89e2d"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 4, 11, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732484656911
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Initial exploration\n",
        "Schema & Row count\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# comments_single_df.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 12,
              "statement_ids": [
                12
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:44:18.2265333Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:44:18.3567245Z",
              "execution_finish_time": "2024-11-24T21:44:18.6996816Z",
              "parent_msg_id": "69992a1e-99c8-4152-9e12-90d07ca86d39"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 4, 12, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "root\n |-- author: string (nullable = true)\n |-- author_flair_css_class: string (nullable = true)\n |-- author_flair_text: string (nullable = true)\n |-- body: string (nullable = true)\n |-- controversiality: long (nullable = true)\n |-- created_utc: long (nullable = true)\n |-- distinguished: string (nullable = true)\n |-- edited: double (nullable = true)\n |-- gilded: long (nullable = true)\n |-- id: string (nullable = true)\n |-- link_id: string (nullable = true)\n |-- parent_id: string (nullable = true)\n |-- retrieved_on: long (nullable = true)\n |-- score: long (nullable = true)\n |-- stickied: boolean (nullable = true)\n |-- subreddit: string (nullable = true)\n |-- subreddit_id: string (nullable = true)\n\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732484658786
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# comments_row_count = comments_single_df.count()\n",
        "# comment_col_count = len(comments_single_df.columns)\n",
        "# print(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 13,
              "statement_ids": [
                13
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 451,
                    "rowCount": 8,
                    "usageDescription": "",
                    "jobId": 3,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 13:\ncomments_row_count = comments_single_df.count()\ncomment_col_count = len(comments_single_df.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-11-24T21:44:29.567GMT",
                    "completionTime": "2024-11-24T21:44:30.161GMT",
                    "stageIds": [
                      3,
                      4
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 451,
                    "dataRead": 91070,
                    "rowCount": 1000008,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 13:\ncomments_row_count = comments_single_df.count()\ncomment_col_count = len(comments_single_df.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-11-24T21:44:23.010GMT",
                    "completionTime": "2024-11-24T21:44:29.462GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:44:21.0741579Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:44:21.1944334Z",
              "execution_finish_time": "2024-11-24T21:44:31.2499196Z",
              "parent_msg_id": "135569ff-8277-4355-be2c-d0a6ec694b49"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 4, 13, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape of the comments dataframe is 1,000,000x17\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732484671352
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submissions_single_df.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 14,
              "statement_ids": [
                14
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:44:34.4175925Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:44:34.5366607Z",
              "execution_finish_time": "2024-11-24T21:44:34.8512373Z",
              "parent_msg_id": "84037465-4deb-44ac-88f0-f7a02d9e1add"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 4, 14, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "root\n |-- author: string (nullable = true)\n |-- author_flair_css_class: string (nullable = true)\n |-- author_flair_text: string (nullable = true)\n |-- created_utc: long (nullable = true)\n |-- distinguished: string (nullable = true)\n |-- domain: string (nullable = true)\n |-- edited: double (nullable = true)\n |-- id: string (nullable = true)\n |-- is_self: boolean (nullable = true)\n |-- locked: boolean (nullable = true)\n |-- num_comments: long (nullable = true)\n |-- over_18: boolean (nullable = true)\n |-- quarantine: boolean (nullable = true)\n |-- retrieved_on: long (nullable = true)\n |-- score: long (nullable = true)\n |-- selftext: string (nullable = true)\n |-- stickied: boolean (nullable = true)\n |-- subreddit: string (nullable = true)\n |-- subreddit_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- url: string (nullable = true)\n\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732484674941
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submissions_row_count = submissions_single_df.count()\n",
        "# submissions_col_count = len(submissions_single_df.columns)\n",
        "# print(f\"shape of the submissions dataframe is {submissions_row_count:,}x{submissions_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 15,
              "statement_ids": [
                15
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 451,
                    "rowCount": 8,
                    "usageDescription": "",
                    "jobId": 5,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 15:\nsubmissions_row_count = submissions_single_df.count()\nsubmissions_col_count = len(submissions_single_df.columns)\nprint(f\"shape of the submissions dataframe is {submissions_row_count:,}x{submissions_col_count}\")",
                    "submissionTime": "2024-11-24T21:44:37.800GMT",
                    "completionTime": "2024-11-24T21:44:38.061GMT",
                    "stageIds": [
                      6,
                      7
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 8,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 451,
                    "dataRead": 112250,
                    "rowCount": 1000008,
                    "usageDescription": "",
                    "jobId": 4,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 15:\nsubmissions_row_count = submissions_single_df.count()\nsubmissions_col_count = len(submissions_single_df.columns)\nprint(f\"shape of the submissions dataframe is {submissions_row_count:,}x{submissions_col_count}\")",
                    "submissionTime": "2024-11-24T21:44:37.467GMT",
                    "completionTime": "2024-11-24T21:44:37.768GMT",
                    "stageIds": [
                      5
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:44:37.168481Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:44:37.3243729Z",
              "execution_finish_time": "2024-11-24T21:44:38.8670122Z",
              "parent_msg_id": "88081ebf-6e0a-4393-bd64-ed406ef8d6d0"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 4, 15, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape of the submissions dataframe is 1,000,000x21\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732484679015
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reading in an entire month"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test on a single month\n",
        "# comments_month = \"yyyy=2023/mm=11/\"\n",
        "# submissions_month = \"yyyy=2023/mm=11/\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 16,
              "statement_ids": [
                16
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:44:42.2550711Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:44:42.3757868Z",
              "execution_finish_time": "2024-11-24T21:44:42.7001358Z",
              "parent_msg_id": "b01e2a0d-7e2f-4751-a46d-04d3c1b6b748"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 4, 16, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732484682764
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# comments_months_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}{comments_month}\")\n",
        "# submissions_months_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}{submissions_month}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 17,
              "statement_ids": [
                17
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 7,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\ncomments_months_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}{comments_month}\")\nsubmissions_months_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}{submissions_month}\")",
                    "submissionTime": "2024-11-24T21:44:46.618GMT",
                    "completionTime": "2024-11-24T21:44:46.787GMT",
                    "stageIds": [
                      9
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 6,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\ncomments_months_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}{comments_month}\")\nsubmissions_months_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}{submissions_month}\")",
                    "submissionTime": "2024-11-24T21:44:45.861GMT",
                    "completionTime": "2024-11-24T21:44:46.322GMT",
                    "stageIds": [
                      8
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:44:45.1490499Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:44:45.267701Z",
              "execution_finish_time": "2024-11-24T21:44:47.7163339Z",
              "parent_msg_id": "30a92103-b90b-4517-8483-677fe1ad938c"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 4, 17, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732484687787
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Initial exploration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# comments_row_count = comments_months_df.count()\n",
        "# comment_col_count = len(comments_months_df.columns)\n",
        "# print(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 18,
              "statement_ids": [
                18
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 14396,
                    "rowCount": 244,
                    "usageDescription": "",
                    "jobId": 9,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 18:\ncomments_row_count = comments_months_df.count()\ncomment_col_count = len(comments_months_df.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-11-24T21:44:54.571GMT",
                    "completionTime": "2024-11-24T21:44:54.692GMT",
                    "stageIds": [
                      12,
                      11
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 245,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 244,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 14396,
                    "dataRead": 4479843,
                    "rowCount": 243870598,
                    "usageDescription": "",
                    "jobId": 8,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 18:\ncomments_row_count = comments_months_df.count()\ncomment_col_count = len(comments_months_df.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-11-24T21:44:51.105GMT",
                    "completionTime": "2024-11-24T21:44:54.518GMT",
                    "stageIds": [
                      10
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 244,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 244,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 244,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:44:50.8666948Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:44:50.9928145Z",
              "execution_finish_time": "2024-11-24T21:44:56.0919439Z",
              "parent_msg_id": "6a0d6831-6d48-4a06-9f14-bbe331edc226"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 4, 18, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape of the comments dataframe is 243,870,354x17\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732484696218
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submissions_row_count = submissions_months_df.count()\n",
        "# submissions_col_count = len(submissions_months_df.columns)\n",
        "# print(f\"shape of the submissions dataframe is {submissions_row_count:,}x{submissions_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 19,
              "statement_ids": [
                19
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 2525,
                    "rowCount": 43,
                    "usageDescription": "",
                    "jobId": 11,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 19:\nsubmissions_row_count = submissions_months_df.count()\nsubmissions_col_count = len(submissions_months_df.columns)\nprint(f\"shape of the submissions dataframe is {submissions_row_count:,}x{submissions_col_count}\")",
                    "submissionTime": "2024-11-24T21:44:57.406GMT",
                    "completionTime": "2024-11-24T21:44:57.461GMT",
                    "stageIds": [
                      15,
                      14
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 44,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 43,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 2525,
                    "dataRead": 1421880,
                    "rowCount": 38107510,
                    "usageDescription": "",
                    "jobId": 10,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 19:\nsubmissions_row_count = submissions_months_df.count()\nsubmissions_col_count = len(submissions_months_df.columns)\nprint(f\"shape of the submissions dataframe is {submissions_row_count:,}x{submissions_col_count}\")",
                    "submissionTime": "2024-11-24T21:44:56.306GMT",
                    "completionTime": "2024-11-24T21:44:57.380GMT",
                    "stageIds": [
                      13
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 43,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 43,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 43,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:44:54.9276012Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:44:56.2252876Z",
              "execution_finish_time": "2024-11-24T21:44:57.8216058Z",
              "parent_msg_id": "ab670d0c-03a6-4ece-b4e8-0b24db0db237"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 4, 19, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape of the submissions dataframe is 38,107,467x21\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732484697912
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reading in all of the Reddit data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\n",
        "submissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 9,
              "statement_ids": [
                9
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\nsubmissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")",
                    "submissionTime": "2024-12-01T05:30:58.171GMT",
                    "completionTime": "2024-12-01T05:31:02.971GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\nsubmissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")",
                    "submissionTime": "2024-12-01T05:30:49.169GMT",
                    "completionTime": "2024-12-01T05:30:55.799GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "8",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T05:30:44.0449593Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T05:30:44.2007997Z",
              "execution_finish_time": "2024-12-01T05:31:03.5506263Z",
              "parent_msg_id": "8aedcedb-cd5d-4aa5-8e04-38e548c984f0"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 8, 9, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733031063616
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submissions_df.count()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 10,
              "statement_ids": [
                10
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 37447,
                    "rowCount": 638,
                    "usageDescription": "",
                    "jobId": 3,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 10:\nsubmissions_df.count()",
                    "submissionTime": "2024-12-01T05:31:23.653GMT",
                    "completionTime": "2024-12-01T05:31:24.292GMT",
                    "stageIds": [
                      3,
                      4
                    ],
                    "jobGroup": "10",
                    "status": "SUCCEEDED",
                    "numTasks": 639,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 638,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 37447,
                    "dataRead": 20126852,
                    "rowCount": 567891507,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 10:\nsubmissions_df.count()",
                    "submissionTime": "2024-12-01T05:31:09.874GMT",
                    "completionTime": "2024-12-01T05:31:23.462GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "10",
                    "status": "SUCCEEDED",
                    "numTasks": 638,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 638,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 638,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "8",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T05:31:07.1688489Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T05:31:07.3055561Z",
              "execution_finish_time": "2024-12-01T05:31:24.7925989Z",
              "parent_msg_id": "50ffb733-e59f-49b8-9f42-5331bd06ee1f"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 8, 10, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "567890869"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733031084873
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submissions_df.rdd.getNumPartitions()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 21,
              "statement_ids": [
                21
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "3",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:28:26.9198695Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:28:27.0539414Z",
              "execution_finish_time": "2024-11-24T21:28:27.9060498Z",
              "parent_msg_id": "24e12264-8683-4aab-ac25-bb9cd43a4333"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 3, 21, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 41,
          "data": {
            "text/plain": "638"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732483708068
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df.count()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 22,
              "statement_ids": [
                22
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 216943,
                    "rowCount": 3677,
                    "usageDescription": "",
                    "jobId": 17,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 22:\ncomments_df.count()",
                    "submissionTime": "2024-11-24T21:29:16.491GMT",
                    "completionTime": "2024-11-24T21:29:17.242GMT",
                    "stageIds": [
                      22,
                      23
                    ],
                    "jobGroup": "22",
                    "status": "SUCCEEDED",
                    "numTasks": 3678,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 3677,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 216943,
                    "dataRead": 66832605,
                    "rowCount": 3675772635,
                    "usageDescription": "",
                    "jobId": 16,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 22:\ncomments_df.count()",
                    "submissionTime": "2024-11-24T21:28:34.921GMT",
                    "completionTime": "2024-11-24T21:29:16.438GMT",
                    "stageIds": [
                      21
                    ],
                    "jobGroup": "22",
                    "status": "SUCCEEDED",
                    "numTasks": 3677,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 3677,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 3677,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "3",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:28:34.6091083Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:28:34.7285382Z",
              "execution_finish_time": "2024-11-24T21:29:19.3401421Z",
              "parent_msg_id": "c3aba2b0-cb64-48b3-ae42-49e444cedc94"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 3, 22, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 43,
          "data": {
            "text/plain": "3675768958"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732483759466
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df.rdd.getNumPartitions()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 23,
              "statement_ids": [
                23
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "3",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:30:17.5604817Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:30:17.7008462Z",
              "execution_finish_time": "2024-11-24T21:30:19.8177856Z",
              "parent_msg_id": "299320c7-0e54-4772-b88c-29b31db2696a"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 3, 23, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 45,
          "data": {
            "text/plain": "3677"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732483819984
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 22,
              "statement_ids": [
                22
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "finished",
              "queued_time": "2024-11-24T21:45:14.2208138Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:45:16.7908971Z",
              "execution_finish_time": "2024-11-24T21:45:17.1063296Z",
              "parent_msg_id": "7c20be23-89d4-4385-95a5-66e27121abf5"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 4, 22, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "root\n |-- author: string (nullable = true)\n |-- author_flair_css_class: string (nullable = true)\n |-- author_flair_text: string (nullable = true)\n |-- body: string (nullable = true)\n |-- controversiality: long (nullable = true)\n |-- created_utc: long (nullable = true)\n |-- distinguished: string (nullable = true)\n |-- edited: double (nullable = true)\n |-- gilded: long (nullable = true)\n |-- id: string (nullable = true)\n |-- link_id: string (nullable = true)\n |-- parent_id: string (nullable = true)\n |-- retrieved_on: long (nullable = true)\n |-- score: long (nullable = true)\n |-- stickied: boolean (nullable = true)\n |-- subreddit: string (nullable = true)\n |-- subreddit_id: string (nullable = true)\n |-- yyyy: integer (nullable = true)\n |-- mm: integer (nullable = true)\n\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732484717189
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Queries Part 1 \n",
        "\n",
        "#### Sorting Subreddits Relevant to: \n",
        "\n",
        "(A1, B3a, B13), A2b, A3a, A3b\n",
        "\n",
        "* Frustrating or frustrat and cancer (HINTS A2b)\n",
        "* cancer and doctors or trust (i.e. does not need to contain trust because trust is included in the NRC sentiment analysis) (HINTS A3a)\n",
        "* cancer and family or friends or sister or brother or mother or mom or father or mother or cousin or aunt or uncle or trust (HINTS A3b)\n",
        "\n",
        "HINTS Questions: \n",
        "\n",
        "* SeekCancerInfo: A1 | Have you ever looked for information about cancer from any source?\n",
        "* Electronic2_HealthInfo: B3a | In the past 12 months have you used the Internet to look for health or medical information?\n",
        "* MisleadingHealthInfo: B13 | How much of the health information that you see on social media do you think is false or misleading?"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lower\n",
        "\n",
        "# sample approximately 0.001% of the data and limit to 50 rows\n",
        "comments_subset_df = comments_df.sample(withReplacement=False, fraction=0.001, seed=42).limit(50)\n",
        "\n",
        "# Display the first few rows of subset DataFrame\n",
        "comments_subset_df.show(5)\n",
        "\n",
        "# Display the size (number of rows) in the df\n",
        "print(f\"Number of rows in the sampled and limited DataFrame: {comments_subset_df.count()}\")\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 13,
              "statement_ids": [
                13
              ],
              "state": "finished",
              "livy_statement_state": "cancelled",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 1,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 1190196,
                    "dataRead": 10388932692,
                    "rowCount": 4792774,
                    "usageDescription": "",
                    "jobId": 4,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 13:\nfrom pyspark.sql.functions import col, lower\n\n# sample approximately 0.001% of the data and limit to 50 rows\ncomments_subset_df = comments_df.sample(withReplacement=False, fraction=0.001, seed=42).limit(50)\n\n# Display the first few rows of subset DataFrame\ncomments_subset_df.show(5)\n\n# Display the size (number of rows) in the df\nprint(f\"Number of rows in the sampled and limited DataFrame: {comments_subset_df.count()}\")\n",
                    "submissionTime": "2024-12-01T05:32:02.717GMT",
                    "completionTime": "2024-12-01T05:32:47.543GMT",
                    "stageIds": [
                      5
                    ],
                    "jobGroup": "13",
                    "status": "FAILED",
                    "numTasks": 3677,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 91,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 8,
                    "numCompletedIndices": 91,
                    "numActiveStages": 0,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 1,
                    "killedTasksSummary": {
                      "Stage cancelled": 8
                    }
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "8",
              "normalized_state": "cancelled",
              "queued_time": "2024-12-01T05:32:01.8973985Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T05:32:02.0789191Z",
              "execution_finish_time": "2024-12-01T05:32:48.4030622Z",
              "parent_msg_id": "95a60750-a5e3-443f-b784-5dab67a6d3b2"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 8, 13, Finished, Cancelled, Cancelled)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733031167388
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lower\n",
        "\n",
        "# Filter the subset for comments containing \"frustrat\" and \"cancer\"\n",
        "filtered_comments_subset_df = comments_subset_df.filter(\n",
        "    (lower(col(\"body\")).contains(\"frustrat\")) & (lower(col(\"body\")).contains(\"cancer\"))\n",
        ")\n",
        "\n",
        "# Preview the filtered DataFrame\n",
        "filtered_comments_subset_df.show(5)\n",
        "\n",
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n",
        "\n",
        "\n",
        "# Save the filtered subset to a Parquet file\n",
        "output_path = f\"{workspace_wasbs_base_url}subset.parquet\"\n",
        "filtered_comments_subset_df.write.parquet(output_path, mode=\"overwrite\")\n",
        "\n",
        "print(f\"Test results saved to {output_path}\")\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 12,
              "statement_ids": [
                12
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "8",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T05:31:50.7361015Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T05:31:50.8665403Z",
              "execution_finish_time": "2024-12-01T05:31:51.1848375Z",
              "parent_msg_id": "f558c358-e42e-4d41-9614-93c929d0d610"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 8, 12, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'comments_subset_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, lower\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Filter the subset for comments containing \"frustrat\" and \"cancer\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m filtered_comments_subset_df \u001b[38;5;241m=\u001b[39m \u001b[43mcomments_subset_df\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m      5\u001b[0m     (lower(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrustrat\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m&\u001b[39m (lower(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancer\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Preview the filtered DataFrame\u001b[39;00m\n\u001b[1;32m      9\u001b[0m filtered_comments_subset_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'comments_subset_df' is not defined"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733031111223
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing to make sure i can read the parquet file from the blob again\n",
        "\n",
        "# the parquet path again\n",
        "output_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/subset.parquet\"\n",
        "\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "1c77c5be-01d5-4a73-be0b-1c872b18023f",
              "statement_id": 13,
              "statement_ids": [
                13
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 2,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 3840,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 12,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 13:\n# testing to make sure i can read the parquet file from the blob again\n\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/subset.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-11-25T05:40:28.827GMT",
                    "completionTime": "2024-11-25T05:40:29.072GMT",
                    "stageIds": [
                      16
                    ],
                    "jobGroup": "13",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 11,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 13:\n# testing to make sure i can read the parquet file from the blob again\n\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/subset.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-11-25T05:40:28.161GMT",
                    "completionTime": "2024-11-25T05:40:28.532GMT",
                    "stageIds": [
                      15
                    ],
                    "jobGroup": "13",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "2",
              "normalized_state": "finished",
              "queued_time": "2024-11-25T05:40:27.5395725Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-25T05:40:27.9657267Z",
              "execution_finish_time": "2024-11-25T05:40:29.5357482Z",
              "parent_msg_id": "a722286b-f87e-4d4a-be33-b2ca230d4d05"
            },
            "text/plain": "StatementMeta(1c77c5be-01d5-4a73-be0b-1c872b18023f, 2, 13, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+------+----------------------+-----------------+----+----------------+-----------+-------------+------+------+---+-------+---------+------------+-----+--------+---------+------------+----+---+\n|author|author_flair_css_class|author_flair_text|body|controversiality|created_utc|distinguished|edited|gilded| id|link_id|parent_id|retrieved_on|score|stickied|subreddit|subreddit_id|yyyy| mm|\n+------+----------------------+-----------------+----+----------------+-----------+-------------+------+------+---+-------+---------+------------+-----+--------+---------+------------+----+---+\n+------+----------------------+-----------------+----+----------------+-----------+-------------+------+------+---+-------+---------+------------+-----+--------+---------+------------+----+---+\n\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732513229672
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing to make sure i can read the parquet file from the blob again\n",
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n",
        "\n",
        "# the parquet path again\n",
        "output_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/subset_job_0_1.parquet\"\n",
        "\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 14,
              "statement_ids": [
                14
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 61096,
                    "rowCount": 191,
                    "usageDescription": "",
                    "jobId": 6,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 14:\n# testing to make sure i can read the parquet file from the blob again\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/subset_job_0_1.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-01T05:32:54.916GMT",
                    "completionTime": "2024-12-01T05:32:55.282GMT",
                    "stageIds": [
                      7
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 5,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 14:\n# testing to make sure i can read the parquet file from the blob again\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/subset_job_0_1.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-01T05:32:54.341GMT",
                    "completionTime": "2024-12-01T05:32:54.540GMT",
                    "stageIds": [
                      6
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "8",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T05:32:53.3092215Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T05:32:53.4969872Z",
              "execution_finish_time": "2024-12-01T05:32:56.0313562Z",
              "parent_msg_id": "1882a69d-dd06-432b-85e9-fb085bf9bbbd"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 8, 14, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-----------------+------------+----+---+\n|      author|author_flair_css_class|author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded|     id|   link_id| parent_id|retrieved_on|score|stickied|        subreddit|subreddit_id|yyyy| mm|\n+------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-----------------+------------+----+---+\n|Own_Owl_6409|                  null|             null|Thank you me too....|               0| 1714691209|         null|  null|     0|l2bh93a|t3_1ciqqgk|t1_l2b6mbr|  1714691225|    3|   false|UlcerativeColitis|    t5_2tb9x|2024|  5|\n|Own_Owl_6409|                  null|             null|Honestly ive been...|               0| 1714691241|         null|  null|     0|l2bhbya|t3_1ciqqgk|t1_l2b4ekd|  1714691259|    2|   false|UlcerativeColitis|    t5_2tb9x|2024|  5|\n|Own_Owl_6409|                  null|             null|Understandable I...|               0| 1714691313|         null|  null|     0|l2bhif8|t3_1ciqqgk|t1_l2b1p7q|  1714691329|    2|   false|UlcerativeColitis|    t5_2tb9x|2024|  5|\n|  sukhsam296|                  null|             null|i also used bootc...|               0| 1714691344|         null|  null|     0|l2bhl9t|t3_1cirtmr|t1_l2bfjpl|  1714691358|    1|   false|          nursing|    t5_2ra72|2024|  5|\n|Far-Smile870|                  null|             null|OMG , how bad luc...|               0| 1714691393|         null|  null|     0|l2bhppr|t3_1blcyqk|t3_1blcyqk|  1714691410|    1|   false|          nursing|    t5_2ra72|2024|  5|\n+------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-----------------+------------+----+---+\nonly showing top 5 rows\n\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733031176121
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_row_count = df_read_back.count()\n",
        "comment_col_count = len(df_read_back.columns)\n",
        "print(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 15,
              "statement_ids": [
                15
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 6784,
                    "rowCount": 115,
                    "usageDescription": "",
                    "jobId": 8,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 15:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-01T05:34:34.483GMT",
                    "completionTime": "2024-12-01T05:34:34.745GMT",
                    "stageIds": [
                      9,
                      10
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 116,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 115,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 6784,
                    "dataRead": 36600832,
                    "rowCount": 406288,
                    "usageDescription": "",
                    "jobId": 7,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 15:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-01T05:34:00.337GMT",
                    "completionTime": "2024-12-01T05:34:34.431GMT",
                    "stageIds": [
                      8
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 115,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 115,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 115,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "8",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T05:33:59.8233025Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T05:34:00.1135427Z",
              "execution_finish_time": "2024-12-01T05:34:35.4041371Z",
              "parent_msg_id": "30c3f579-ccd0-4157-83ec-a428cca1099a"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 8, 15, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape of the comments dataframe is 406,173x19\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733031275502
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the parquet path again\n",
        "output_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer/comments\"\n",
        "\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 16,
              "statement_ids": [
                16
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 224733,
                    "rowCount": 1745,
                    "usageDescription": "",
                    "jobId": 10,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 16:\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer/comments\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-01T05:41:28.464GMT",
                    "completionTime": "2024-12-01T05:41:28.715GMT",
                    "stageIds": [
                      12
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 9,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 16:\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer/comments\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-01T05:41:28.087GMT",
                    "completionTime": "2024-12-01T05:41:28.235GMT",
                    "stageIds": [
                      11
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "8",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T05:41:26.856994Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T05:41:27.0975897Z",
              "execution_finish_time": "2024-12-01T05:41:29.6600898Z",
              "parent_msg_id": "bee36836-5277-40c3-8c64-9eddd37aac90"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 8, 16, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-----------------+------------+----+---+\n|              author|author_flair_css_class|   author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded|     id|   link_id| parent_id|retrieved_on|score|stickied|        subreddit|subreddit_id|yyyy| mm|\n+--------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-----------------+------------+----+---+\n|        Own_Owl_6409|                  null|                null|Its too late it...|               0| 1714691160|         null|  null|     0|l2bh4qr|t3_1ciqqgk|t1_l2b0d5x|  1714691174|    8|   false|UlcerativeColitis|    t5_2tb9x|2024|  5|\n|          GoldAd9596|                  null|                null|I have a question...|               0| 1714691162|         null|  null|     0|l2bh4zz|t3_1cilpx7|t3_1cilpx7|  1714691177|    1|   false|        predental|    t5_2ucup|2024|  5|\n|              ivy007|                  null|BSN RN - Hematolo...|Not really. Im j...|               0| 1714691164|         null|  null|     0|l2bh548|t3_1cfkkym|t1_l1sx6q1|  1714691181|    2|   false|          nursing|    t5_2ra72|2024|  5|\n|    Careless_Web2731|                  null|                null|Honestly. This is...|               0| 1714691184|         null|  null|     0|l2bh6vy|t3_1cilfk1|t1_l2a12qe|  1714691198|    3|   false|          nursing|    t5_2ra72|2024|  5|\n|RealisticShopping625|                  null|                null|I have being read...|               0| 1714691187|         null|  null|     0|l2bh73y| t3_j6ssbi| t3_j6ssbi|  1714691201|    1|   false|      coloncancer|    t5_3g1p0|2024|  5|\n+--------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-----------------+------------+----+---+\nonly showing top 5 rows\n\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733031689736
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_row_count = df_read_back.count()\n",
        "comment_col_count = len(df_read_back.columns)\n",
        "print(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 17,
              "statement_ids": [
                17
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 6961,
                    "rowCount": 118,
                    "usageDescription": "",
                    "jobId": 12,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-01T05:42:22.123GMT",
                    "completionTime": "2024-12-01T05:42:22.194GMT",
                    "stageIds": [
                      15,
                      14
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 119,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 118,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 6961,
                    "dataRead": 32049406,
                    "rowCount": 4070692,
                    "usageDescription": "",
                    "jobId": 11,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-01T05:41:53.885GMT",
                    "completionTime": "2024-12-01T05:42:22.085GMT",
                    "stageIds": [
                      13
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 118,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 118,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 118,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "8",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T05:41:53.5837338Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T05:41:53.7025225Z",
              "execution_finish_time": "2024-12-01T05:42:23.3799681Z",
              "parent_msg_id": "003466c0-4293-4471-bf2f-89275a1c0625"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 8, 17, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape of the comments dataframe is 4,070,574x19\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733031743489
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the parquet path again\n",
        "output_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer/submissions\"\n",
        "\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 19,
              "statement_ids": [
                19
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 496129,
                    "rowCount": 1543,
                    "usageDescription": "",
                    "jobId": 14,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 19:\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer/submissions\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-01T05:44:01.447GMT",
                    "completionTime": "2024-12-01T05:44:01.756GMT",
                    "stageIds": [
                      17
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 13,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 19:\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/cancer/submissions\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-01T05:44:00.996GMT",
                    "completionTime": "2024-12-01T05:44:01.147GMT",
                    "stageIds": [
                      16
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "8",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T05:44:00.5094749Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T05:44:00.653542Z",
              "execution_finish_time": "2024-12-01T05:44:02.242377Z",
              "parent_msg_id": "213a20f4-bdff-4bcb-8412-7d28947b8e2d"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 8, 19, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+----------------------+--------------------+-----------+-------------+---------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+---------+------------+--------------------+--------------------+----+---+\n|              author|author_flair_css_class|   author_flair_text|created_utc|distinguished|         domain|edited|     id|is_self|locked|num_comments|over_18|quarantine|retrieved_on|score|            selftext|stickied|subreddit|subreddit_id|               title|                 url|yyyy| mm|\n+--------------------+----------------------+--------------------+-----------+-------------+---------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+---------+------------+--------------------+--------------------+----+---+\n|Kitchen-Security5235|               default|Layperson/not ver...| 1722354598|         null|   self.AskDocs|  null|1efwtcc|   true| false|           1|  false|     false|  1722354612|    1|Hi everyone. I ha...|   false|  AskDocs|    t5_2xtuc|         Concussion?|https://www.reddi...|2024|  7|\n|    One_Program_1613|                  null|                null| 1722354660|         null|digistore24.com|  null|1efwu99|  false| false|           0|  false|     false|  1722354676|    1|                    |   false|   Health|    t5_2qh9z|These \"unusual ne...|https://www.digis...|2024|  7|\n|      heyheyitsamber|               default|Layperson/not ver...| 1722354710|         null|   self.AskDocs|  null|1efwuz6|   true| false|           2|  false|     false|  1722354724|    0|25 year old femal...|   false|  AskDocs|    t5_2xtuc|Should I still ta...|https://www.reddi...|2024|  7|\n|       oldsoul070924|                  null|                null| 1722354827|         null|   self.AskDocs|  null|1efwwrg|   true| false|           1|  false|     false|  1722354843|    1|           [removed]|   false|  AskDocs|    t5_2xtuc|Mole on nose got ...|https://www.reddi...|2024|  7|\n|              jjjaax|               default|Layperson/not ver...| 1722354938|         null|   self.AskDocs|  null|1efwygz|   true| false|           3|  false|     false|  1722354953|    1|27 year old male,...|   false|  AskDocs|    t5_2xtuc|Is this Skin Canc...|https://www.reddi...|2024|  7|\n+--------------------+----------------------+--------------------+-----------+-------------+---------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+---------+------------+--------------------+--------------------+----+---+\nonly showing top 5 rows\n\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733031842317
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_row_count = df_read_back.count()\n",
        "comment_col_count = len(df_read_back.columns)\n",
        "print(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 20,
              "statement_ids": [
                20
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 1180,
                    "rowCount": 20,
                    "usageDescription": "",
                    "jobId": 16,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 20:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-01T05:44:23.688GMT",
                    "completionTime": "2024-12-01T05:44:23.736GMT",
                    "stageIds": [
                      19,
                      20
                    ],
                    "jobGroup": "20",
                    "status": "SUCCEEDED",
                    "numTasks": 21,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 20,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 1180,
                    "dataRead": 6026250,
                    "rowCount": 649025,
                    "usageDescription": "",
                    "jobId": 15,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 20:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-01T05:44:15.829GMT",
                    "completionTime": "2024-12-01T05:44:23.650GMT",
                    "stageIds": [
                      18
                    ],
                    "jobGroup": "20",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "8",
              "normalized_state": "finished",
              "queued_time": "2024-12-01T05:44:15.5691202Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-01T05:44:15.691285Z",
              "execution_finish_time": "2024-12-01T05:44:25.7335259Z",
              "parent_msg_id": "d9a16abf-3a24-4ed8-b5ac-61216221ff58"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 8, 20, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape of the comments dataframe is 649,005x23\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733031865807
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Previous Code to save as .csv \n",
        "\n",
        "# Filter the subset for comments containing \"frustrat\" and \"cancer\"\n",
        "#filtered_comments_subset_df = comments_subset_df.filter(\n",
        "#    (lower(col(\"body\")).contains(\"frustrat\")) & (lower(col(\"body\")).contains(\"cancer\"))\n",
        "#)\n",
        "\n",
        "# Preview the filtered DataFrame\n",
        "#filtered_comments_subset_df.show(5)\n",
        "\n",
        "# Save the filtered subset to a test CSV file\n",
        "#output_test_file = \"Test_Query_A2b.csv\"\n",
        "#filtered_comments_subset_df.write.csv(output_test_file, mode=\"overwrite\", header=True)\n",
        "\n",
        "#print(f\"Test results saved to {output_test_file}\")\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "1c77c5be-01d5-4a73-be0b-1c872b18023f",
              "statement_id": 19,
              "statement_ids": [
                19
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 4,
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "csv at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 190,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 20,
                    "name": "csv at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 19:\n# Previous Code to save as .csv \n\n# Filter the subset for comments containing \"frustrat\" and \"cancer\"\nfiltered_comments_subset_df = comments_subset_df.filter(\n    (lower(col(\"body\")).contains(\"frustrat\")) & (lower(col(\"body\")).contains(\"cancer\"))\n)\n\n# Preview the filtered DataFrame\nfiltered_comments_subset_df.show(5)\n\n# Save the filtered subset to a test CSV file\noutput_test_file = \"Test_Query_A2b.csv\"\nfiltered_comments_subset_df.write.csv(output_test_file, mode=\"overwrite\", header=True)\n\nprint(f\"Test results saved to {output_test_file}\")\n",
                    "submissionTime": "2024-11-25T03:18:02.297GMT",
                    "completionTime": "2024-11-25T03:18:03.279GMT",
                    "stageIds": [
                      27,
                      26
                    ],
                    "jobGroup": "19",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 3678,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 3677,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "csv at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 45662716,
                    "dataRead": 396700700646,
                    "rowCount": 191434282,
                    "usageDescription": "",
                    "jobId": 19,
                    "name": "csv at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 19:\n# Previous Code to save as .csv \n\n# Filter the subset for comments containing \"frustrat\" and \"cancer\"\nfiltered_comments_subset_df = comments_subset_df.filter(\n    (lower(col(\"body\")).contains(\"frustrat\")) & (lower(col(\"body\")).contains(\"cancer\"))\n)\n\n# Preview the filtered DataFrame\nfiltered_comments_subset_df.show(5)\n\n# Save the filtered subset to a test CSV file\noutput_test_file = \"Test_Query_A2b.csv\"\nfiltered_comments_subset_df.write.csv(output_test_file, mode=\"overwrite\", header=True)\n\nprint(f\"Test results saved to {output_test_file}\")\n",
                    "submissionTime": "2024-11-25T02:56:44.996GMT",
                    "completionTime": "2024-11-25T03:18:02.059GMT",
                    "stageIds": [
                      25
                    ],
                    "jobGroup": "19",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 3677,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 3677,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 3677,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 18,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 19:\n# Previous Code to save as .csv \n\n# Filter the subset for comments containing \"frustrat\" and \"cancer\"\nfiltered_comments_subset_df = comments_subset_df.filter(\n    (lower(col(\"body\")).contains(\"frustrat\")) & (lower(col(\"body\")).contains(\"cancer\"))\n)\n\n# Preview the filtered DataFrame\nfiltered_comments_subset_df.show(5)\n\n# Save the filtered subset to a test CSV file\noutput_test_file = \"Test_Query_A2b.csv\"\nfiltered_comments_subset_df.write.csv(output_test_file, mode=\"overwrite\", header=True)\n\nprint(f\"Test results saved to {output_test_file}\")\n",
                    "submissionTime": "2024-11-25T02:56:44.305GMT",
                    "completionTime": "2024-11-25T02:56:44.726GMT",
                    "stageIds": [
                      24,
                      23
                    ],
                    "jobGroup": "19",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 3678,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 3677,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 45662716,
                    "dataRead": 396700700646,
                    "rowCount": 191434282,
                    "usageDescription": "",
                    "jobId": 17,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 19:\n# Previous Code to save as .csv \n\n# Filter the subset for comments containing \"frustrat\" and \"cancer\"\nfiltered_comments_subset_df = comments_subset_df.filter(\n    (lower(col(\"body\")).contains(\"frustrat\")) & (lower(col(\"body\")).contains(\"cancer\"))\n)\n\n# Preview the filtered DataFrame\nfiltered_comments_subset_df.show(5)\n\n# Save the filtered subset to a test CSV file\noutput_test_file = \"Test_Query_A2b.csv\"\nfiltered_comments_subset_df.write.csv(output_test_file, mode=\"overwrite\", header=True)\n\nprint(f\"Test results saved to {output_test_file}\")\n",
                    "submissionTime": "2024-11-25T02:35:38.552GMT",
                    "completionTime": "2024-11-25T02:56:44.182GMT",
                    "stageIds": [
                      22
                    ],
                    "jobGroup": "19",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 3677,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 3677,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 3677,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "1",
              "normalized_state": "finished",
              "queued_time": "2024-11-25T02:35:38.0871213Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-25T02:35:38.2902585Z",
              "execution_finish_time": "2024-11-25T03:18:05.471219Z",
              "parent_msg_id": "0ac4d1a0-e46b-408b-b44b-198dfee418e6"
            },
            "text/plain": "StatementMeta(1c77c5be-01d5-4a73-be0b-1c872b18023f, 1, 19, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+------+----------------------+-----------------+----+----------------+-----------+-------------+------+------+---+-------+---------+------------+-----+--------+---------+------------+----+---+\n|author|author_flair_css_class|author_flair_text|body|controversiality|created_utc|distinguished|edited|gilded| id|link_id|parent_id|retrieved_on|score|stickied|subreddit|subreddit_id|yyyy| mm|\n+------+----------------------+-----------------+----+----------------+-----------+-------------+------+------+---+-------+---------+------------+-----+--------+---------+------------+----+---+\n+------+----------------------+-----------------+----+----------------+-----------+-------------+------+------+---+-------+---------+------------+-----+--------+---------+------------+----+---+\n\nTest results saved to Test_Query_A2b.csv\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732504685895
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Full Job \n",
        "# from pyspark.sql.functions import col, lower\n",
        "\n",
        "# # Filter comments containing \"frustrat\" and \"cancer\"\n",
        "# filtered_comments_df = comments_df.filter(\n",
        "#     (lower(col(\"body\")).contains(\"frustrat\")) & (lower(col(\"body\")).contains(\"cancer\"))\n",
        "# )\n",
        "\n",
        "# # Save the filtered comments to a CSV file\n",
        "# output_file = \"Query_A2b.csv\"\n",
        "# filtered_comments_df.write.csv(output_file, mode=\"overwrite\", header=True)\n",
        "\n",
        "# # Preview the filtered DataFrame\n",
        "# filtered_comments_df.show(5)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 23,
              "statement_ids": [
                23
              ],
              "state": "finished",
              "livy_statement_state": "cancelled",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "FAILED": 1
                },
                "jobs": [
                  {
                    "displayName": "csv at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 659445,
                    "dataRead": 10502572209,
                    "rowCount": 92000413,
                    "usageDescription": "",
                    "jobId": 16,
                    "name": "csv at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 23:\nfrom pyspark.sql.functions import col, lower\n\n# Filter comments containing \"frustrat\" and \"cancer\"\nfiltered_comments_df = comments_df.filter(\n    (lower(col(\"body\")).contains(\"frustrat\")) & (lower(col(\"body\")).contains(\"cancer\"))\n)\n\n# Save the filtered comments to a CSV file\noutput_file = \"Query_A2b.csv\"\nfiltered_comments_df.write.csv(output_file, mode=\"overwrite\", header=True)\n\n# Preview the filtered DataFrame\nfiltered_comments_df.show(5)\n",
                    "submissionTime": "2024-11-24T21:46:37.938GMT",
                    "completionTime": "2024-11-24T21:48:20.247GMT",
                    "stageIds": [
                      21
                    ],
                    "jobGroup": "23",
                    "status": "FAILED",
                    "numTasks": 3677,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 92,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 8,
                    "numCompletedIndices": 92,
                    "numActiveStages": 0,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 1,
                    "killedTasksSummary": {
                      "Stage cancelled": 8
                    }
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "4",
              "normalized_state": "cancelled",
              "queued_time": "2024-11-24T21:46:36.8471617Z",
              "session_start_time": null,
              "execution_start_time": "2024-11-24T21:46:37.0619102Z",
              "execution_finish_time": "2024-11-24T21:48:21.8713127Z",
              "parent_msg_id": "5601befc-f3f0-4ae2-97e6-a98aa87c060b"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 4, 23, Finished, Cancelled, Cancelled)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732484900127
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing to make sure i can read the parquet file from the blob again\n",
        "workspace_default_storage_account = \"projectgstoragedfb938a3e\"\n",
        "workspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\n",
        "workspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n",
        "\n",
        "# the parquet path again\n",
        "output_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/subset_job_0_2.parquet\"\n",
        "\n",
        "# Read the Parquet file back into a dataframe\n",
        "df_read_back = spark.read.parquet(output_path)\n",
        "\n",
        "# Show first 5 rows\n",
        "df_read_back.show(5)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 8,
              "statement_ids": [
                8
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 2
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 58034,
                    "rowCount": 9,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 8:\n# testing to make sure i can read the parquet file from the blob again\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/subset_job_0_2.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-05T21:03:09.195GMT",
                    "completionTime": "2024-12-05T21:03:12.702GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 8:\n# testing to make sure i can read the parquet file from the blob again\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\n# the parquet path again\noutput_path = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/subset_job_0_2.parquet\"\n\n# Read the Parquet file back into a dataframe\ndf_read_back = spark.read.parquet(output_path)\n\n# Show first 5 rows\ndf_read_back.show(5)",
                    "submissionTime": "2024-12-05T21:02:53.031GMT",
                    "completionTime": "2024-12-05T21:03:04.082GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "8",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "14",
              "normalized_state": "finished",
              "queued_time": "2024-12-05T21:02:49.1754534Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-05T21:02:49.3147628Z",
              "execution_finish_time": "2024-12-05T21:03:14.8347775Z",
              "parent_msg_id": "2481257e-2a44-4091-bb64-5d023c78a821"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 14, 8, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-------------------+------------+----+---+\n|              author|author_flair_css_class|author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded|     id|   link_id| parent_id|retrieved_on|score|stickied|          subreddit|subreddit_id|yyyy| mm|\n+--------------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-------------------+------------+----+---+\n|Logical-Feature-1136|                  null|             null|Hey. Im sorry ab...|               0| 1715637800|         null|  null|     0|l3wyrxc|t3_1crb5mr|t3_1crb5mr|  1715637814|    3|   false|CancerFamilySupport|    t5_3e65q|2024|  5|\n|       mildhot-sauce|                  null|             null|Professional voic...|               0| 1715640666|         null|  null|     0|l3x68e7|t3_1cqsj4p|t3_1cqsj4p|  1715640683|    1|   false|      thyroidcancer|    t5_3e8dw|2024|  5|\n|            MarsMorn|                  null|             null|Ok, breathe. This...|               0| 1715642425|         null|  null|     0|l3xaoht|t3_1cr90dc|t3_1cr90dc|  1715642440|   15|   false|       breastcancer|    t5_2t6dy|2024|  5|\n|     Internal-Ad8877|                  null|             null|Hi there, I'm als...|               0| 1715644426|         null|  null|     0|l3xfqbo|t3_1cr9114|t3_1cr9114|  1715644442|    1|   false|       breastcancer|    t5_2t6dy|2024|  5|\n|   halfbl00dprinc3ss|                  null|             null|Usually the way i...|               0| 1712181664|         null|  null|     0|kxx7qow|t3_1bv444n|t3_1bv444n|  1712181680|    5|   false|           leukemia|    t5_2rm1p|2024|  4|\n+--------------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+-------------------+------------+----+---+\nonly showing top 5 rows\n\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733432615418
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_row_count = df_read_back.count()\n",
        "comment_col_count = len(df_read_back.columns)\n",
        "print(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ba5e360d-f184-47a0-9859-76b5031b79e3",
              "statement_id": 9,
              "statement_ids": [
                9
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "UNKNOWN": 0,
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 2
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 5841,
                    "rowCount": 99,
                    "usageDescription": "",
                    "jobId": 3,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-05T21:03:58.764GMT",
                    "completionTime": "2024-12-05T21:04:01.110GMT",
                    "stageIds": [
                      3,
                      4
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 100,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 99,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 5841,
                    "dataRead": 34226870,
                    "rowCount": 7730,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_row_count = df_read_back.count()\ncomment_col_count = len(df_read_back.columns)\nprint(f\"shape of the comments dataframe is {comments_row_count:,}x{comment_col_count}\")",
                    "submissionTime": "2024-12-05T21:03:19.534GMT",
                    "completionTime": "2024-12-05T21:03:58.593GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 99,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 99,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 99,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "14",
              "normalized_state": "finished",
              "queued_time": "2024-12-05T21:03:18.601582Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-05T21:03:18.7627489Z",
              "execution_finish_time": "2024-12-05T21:04:02.3780043Z",
              "parent_msg_id": "7a5f771e-96f8-43be-995a-6f097d2a61d4"
            },
            "text/plain": "StatementMeta(ba5e360d-f184-47a0-9859-76b5031b79e3, 14, 9, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape of the comments dataframe is 7,631x19\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733432662972
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}