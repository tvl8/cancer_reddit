[
  {
    "objectID": "reddit_ml.html#methodology",
    "href": "reddit_ml.html#methodology",
    "title": "Machine Learning - Reddit Data",
    "section": "",
    "text": "The dataset preparation and preprocessing were crucial to ensuring the models had the appropriate features for learning. The raw data from the PushShift Reddit Dataset included text data and category labels for cancer-related and non-cancer comments.\nFirst, the data was combined into a single DataFrame, with an additional column named source labeling the comments as “Cancer” or “Non-Cancer.” The comments were then preprocessed to ensure they were in a suitable format for the models.\nText tokenization was applied to split the comments into individual words, enabling feature extraction. Tokenization was followed by the removal of stop words, such as “and,” “the,” and “is,” which are frequent but do not carry significant information for classification. The tokenized words were then converted into numerical vectors using CountVectorizer, which captures word frequencies. To enhance the importance of rare but meaningful words, TF-IDF (Term Frequency-Inverse Document Frequency) scores were computed. This step ensured that commonly occurring but uninformative words received less weight, while rare and distinctive words were emphasized.\nFinally, the labels (“Cancer” and “Non-Cancer”) were encoded numerically as 0 and 1, respectively, using StringIndexer. These preprocessing steps were implemented as a PySpark pipeline to ensure consistency and scalability.\nThe processed dataset was then split into 80% training and 20% testing subsets. Two machine learning models, Naive Bayes and Logistic Regression, were trained on the training data using PySpark’s built-in implementations. The evaluation metrics, such as accuracy, precision, recall, F1-score, and AUC-ROC, were computed on the test set. Additionally, confusion matrices were generated to visualize the distribution of correct and incorrect predictions for both models.\nLogistic Regression and Naive Bayes were chosen for their simplicity, efficiency, and effectiveness in text classification. Naive Bayes handles high-dimensional data well with its independence assumption, while Logistic Regression provides robust, interpretable probability predictions. These models serve as strong benchmarks for binary classification tasks."
  },
  {
    "objectID": "reddit_ml.html#naive-bayes",
    "href": "reddit_ml.html#naive-bayes",
    "title": "Machine Learning - Reddit Data",
    "section": "",
    "text": "Naive Bayes is a probabilistic model based on Bayes’ theorem, with the assumption that features are conditionally independent given the label. This model is particularly suitable for text classification tasks, as it performs well when features (words) contribute independently to the prediction.\nIn this task, Naive Bayes classified comments as cancer-related or non-cancer based on word frequencies and their association with the labels. By assuming independence between words, the model simplifies the computation, making it highly efficient.\n\n\n\nThe model performance, though wasn’t that great, still created a strong baseline for further modeling analysis.\nTable 1 : Performance Metrics for Naive Bayes\n\n\n\nMetric\nValue\n\n\n\n\nTest Precision\n0.777\n\n\nTest Recall\n0.775\n\n\nTest F1-Score\n0.775\n\n\nTest AUC-ROC\n0.776\n\n\n\nThe Naive Bayes model achieved an accuracy of 77.5%, indicating that it correctly classified over three-fourths of the test data. Its precision and recall were similarly high, both at approximately 77.5%, reflecting a good balance between identifying cancer-related comments (true positives) and minimizing false positives (non-cancer comments wrongly classified as cancer). The F1-score, a harmonic mean of precision and recall, was also 77.5%, confirming the model’s consistent performance. The AUC-ROC score of 77.6% highlights the model’s ability to distinguish between the two classes effectively.\n\n\n\nNaive Bayes Classification Confusion Matrix\n\n\nThe confusion matrix provided deeper insights into the model’s strengths and weaknesses. Of the cancer-related comments, 1564 were correctly classified, while 391 were misclassified as non-cancer. Conversely, 1517 non-cancer comments were correctly classified, but 501 were incorrectly predicted as cancer.\nThese false positives suggest that the model occasionally overpredicts the “Cancer” class, possibly due to words or phrases common in both categories. Nevertheless, the relatively low number of false negatives (391) demonstrates that the model is effective at capturing cancer-related content, aligning well with the task’s objective.\nHence, Naive Bayes’s bias toward predicting the “Cancer” class (higher false positives) can be acceptable in scenarios where prioritizing cancer-related content is more important than avoiding false positives."
  },
  {
    "objectID": "reddit_ml.html#logistic-regressio",
    "href": "reddit_ml.html#logistic-regressio",
    "title": "Machine Learning - Reddit Data",
    "section": "Logistic Regressio",
    "text": "Logistic Regressio\n\nRationale\nLogistic Regression is a linear model that predicts the probability of an instance belonging to a particular class by fitting a logistic function (sigmoid curve) to the data. Unlike Naive Bayes, Logistic Regression does not assume independence among features. Instead, it learns the weights of features (words) directly from the data, making it more flexible but also more prone to overfitting in high-dimensional spaces without proper regularization.\n\n\nModel Performance\nThough Logistic Regression underperformed in comparison to Naive Bayes, it was still interesting to look into it’s performance, based on the table below\n\n\n\nMetric\nValue\n\n\n\n\nTest Precision\n0.707\n\n\nTest Recall\n0.707\n\n\nTest F1-Score\n0.707\n\n\nTest AUC-ROC\n0.707\n\n\n\nFor this question, Logistic Regression performed moderately well, achieving an accuracy of 70.7%. Its precision, recall, and F1-score were all consistent at 70.7%, reflecting a balanced performance but slightly lower than that of Naive Bayes. The AUC-ROC score was also 70.7%, indicating that the model struggled more than Naive Bayes in distinguishing cancer-related from non-cancer comments.\n\n\n\nNaive Bayes Classification Confusion Matrix\n\n\nThe confusion matrix highlighted specific challenges faced by Logistic Regression. While it correctly identified 1374 cancer-related comments, it misclassified 581 as non-cancer, a relatively high number of false negatives. Similarly, 1434 non-cancer comments were correctly classified, but 584 were predicted as cancer, resulting in a comparable number of false positives. These results suggest that the model had difficulty capturing the subtle nuances in the text that differentiate the two categories.\nThe lower recall for cancer-related comments (more false negatives) is a significant limitation of Logistic Regression in this context. Missing cancer-related comments is more critical than overclassifying non-cancer content as cancer.\nThis limitation makes Logistic Regression less suitable for the given task, where the focus is on identifying cancer-related content with high confidence. Logistic Regression’s moderate performance can be attributed to its reliance on linear decision boundaries, which may not fully capture the complexities of the text data. While it is a robust and interpretable model, its linear nature and sensitivity to high-dimensional data might have limited its effectiveness in this problem."
  },
  {
    "objectID": "reddit_ml.html#reflection",
    "href": "reddit_ml.html#reflection",
    "title": "Machine Learning - Reddit Data",
    "section": "Reflection",
    "text": "Reflection\nUnderstanding whether a comment is cancer-related is crucial for enhancing online support systems and health communication. Cancer-related subreddits often serve as platforms for individuals seeking emotional support, sharing personal experiences, or discussing treatment options. Automatically identifying such comments can help moderators curate relevant content, ensure timely responses to critical queries, and provide researchers with valuable insights into public sentiment, challenges, and trends related to cancer. This understanding is particularly important for designing interventions, improving healthcare accessibility, and fostering a supportive community for those affected by cancer."
  },
  {
    "objectID": "reddit_ml.html#logistic-regression",
    "href": "reddit_ml.html#logistic-regression",
    "title": "Machine Learning - Reddit Data",
    "section": "",
    "text": "Logistic Regression is a linear model that predicts the probability of an instance belonging to a particular class by fitting a logistic function (sigmoid curve) to the data. Unlike Naive Bayes, Logistic Regression does not assume independence among features. Instead, it learns the weights of features (words) directly from the data, making it more flexible but also more prone to overfitting in high-dimensional spaces without proper regularization.\n\n\n\nThough Logistic Regression underperformed in comparison to Naive Bayes, it was still interesting to look into it’s performance, based on the table below\n\n\n\nMetric\nValue\n\n\n\n\nTest Precision\n0.707\n\n\nTest Recall\n0.707\n\n\nTest F1-Score\n0.707\n\n\nTest AUC-ROC\n0.707\n\n\n\nFor this question, Logistic Regression performed moderately well, achieving an accuracy of 70.7%. Its precision, recall, and F1-score were all consistent at 70.7%, reflecting a balanced performance but slightly lower than that of Naive Bayes. The AUC-ROC score was also 70.7%, indicating that the model struggled more than Naive Bayes in distinguishing cancer-related from non-cancer comments.\n\n\n\nNaive Bayes Classification Confusion Matrix\n\n\nThe confusion matrix highlighted specific challenges faced by Logistic Regression. While it correctly identified 1374 cancer-related comments, it misclassified 581 as non-cancer, a relatively high number of false negatives. Similarly, 1434 non-cancer comments were correctly classified, but 584 were predicted as cancer, resulting in a comparable number of false positives. These results suggest that the model had difficulty capturing the subtle nuances in the text that differentiate the two categories.\nThe lower recall for cancer-related comments (more false negatives) is a significant limitation of Logistic Regression in this context. Missing cancer-related comments is more critical than overclassifying non-cancer content as cancer.\nThis limitation makes Logistic Regression less suitable for the given task, where the focus is on identifying cancer-related content with high confidence. Logistic Regression’s moderate performance can be attributed to its reliance on linear decision boundaries, which may not fully capture the complexities of the text data. While it is a robust and interpretable model, its linear nature and sensitivity to high-dimensional data might have limited its effectiveness in this problem."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "About the Data and Data Cleaning",
    "section": "",
    "text": "To explore the use of online sources in health communication, two publicly available datasets were used, HINTS and Reddit Pushshift. The Health Information National Trends Survey (HINTS) collects information about the Amercian’s use of cancer-related information. In this project, eleven questions from the HINTS survey were collected for review. These questions focus on gathering information about individuals’ behaviors, trust, and perceptions related to cancer information and health communication. Collectively, these questions provide insights into the sources and levels of trust Americans place in health and cancer information, their experiences in searching for such information, and their perceptions of the reliability of online and social media health content. HINTS is used to better understand how the American public looks for health information for themselves or their loved ones.\nThe Reddit Pushshift dataset includes free text data from the Reddit online forum where users can post and look for information. It also includes subreddits, which branch off into various subreddits—dedicated communities centered around specific topics. By examining this dataset, this project complements insights from HINTS, offering a unique perspective on how individuals seek, share, and discuss cancer-related health information online. The analysis of Reddit data allows researchers to explore the dynamic and informal exchanges that occur in digital forums to better our understanding of how cancer information is communicated."
  },
  {
    "objectID": "data_cleaning.html#introduction",
    "href": "data_cleaning.html#introduction",
    "title": "About the Data and Data Cleaning",
    "section": "",
    "text": "To explore the use of online sources in health communication, two publicly available datasets were used, HINTS and Reddit Pushshift. The Health Information National Trends Survey (HINTS) collects information about the Amercian’s use of cancer-related information. In this project, eleven questions from the HINTS survey were collected for review. These questions focus on gathering information about individuals’ behaviors, trust, and perceptions related to cancer information and health communication. Collectively, these questions provide insights into the sources and levels of trust Americans place in health and cancer information, their experiences in searching for such information, and their perceptions of the reliability of online and social media health content. HINTS is used to better understand how the American public looks for health information for themselves or their loved ones.\nThe Reddit Pushshift dataset includes free text data from the Reddit online forum where users can post and look for information. It also includes subreddits, which branch off into various subreddits—dedicated communities centered around specific topics. By examining this dataset, this project complements insights from HINTS, offering a unique perspective on how individuals seek, share, and discuss cancer-related health information online. The analysis of Reddit data allows researchers to explore the dynamic and informal exchanges that occur in digital forums to better our understanding of how cancer information is communicated."
  },
  {
    "objectID": "data_cleaning.html#hints-preparation-of-data",
    "href": "data_cleaning.html#hints-preparation-of-data",
    "title": "About the Data and Data Cleaning",
    "section": "HINTS Preparation of Data",
    "text": "HINTS Preparation of Data\n\n\nCode\nimport pyreadr\nimport pandas as pd\nfrom IPython.display import display\n\n# Load the .rda file\nresult = pyreadr.read_r('/Users/elizabethkovalchuk/Documents/DSAN6000/Project/fall-2024-project-team-35/data/HINTS6_R_20240524/hints6_public.rda')\n\n# Extract the DataFrame from the loaded data\nhints = result['public']  # Assuming 'public' is the name of the R object in the file\n\n# Specify the columns to select\ncolumns = [\n    \"HHID\", \"SeekCancerInfo\", \"CancerFrustrated\", \"CancerTrustDoctor\",\n    \"CancerTrustFamily\", \"CancerTrustGov\", \"CancerTrustCharities\",\n    \"CancerTrustReligiousOrgs\", \"CancerTrustScientists\", \"Electronic2_HealthInfo\",\n    \"MisleadingHealthInfo\", \"TrustHCSystem\"\n]\n\n# Select the relevant columns\nhints_select = hints[columns]\n\n# # Convert the 'updatedate' column if required (commented for now)\n# hints_select['updatedate'] = pd.to_datetime(hints_select['updatedate'] / 1000, unit='s')\n\n# Preview the first few rows\nprint(\"Sample data from the HINTS dataset:\")\ndisplay(hints_select.head())\nprint(f\"Shape of the original dataset: {hints_select.shape}\")\n\n\nSample data from the HINTS dataset:\n\n\n\n\n\n\n\n\n\nHHID\nSeekCancerInfo\nCancerFrustrated\nCancerTrustDoctor\nCancerTrustFamily\nCancerTrustGov\nCancerTrustCharities\nCancerTrustReligiousOrgs\nCancerTrustScientists\nElectronic2_HealthInfo\nMisleadingHealthInfo\nTrustHCSystem\n\n\n\n\n0\n21000006\nNo\nInapplicable, coded 2 in SeekCancerInfo\nA lot\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nQuestion answered in error (Commission Error)\nI do not use social media\nVery\n\n\n1\n21000009\nNo\nInapplicable, coded 2 in SeekCancerInfo\nA lot\nSome\nA lot\nSome\nSome\nA lot\nYes\nI do not use social media\nVery\n\n\n2\n21000020\nYes\nSomewhat disagree\nA lot\nSome\nSome\nA little\nNot at all\nA lot\nYes\nSome\nSomewhat\n\n\n3\n21000022\nNo\nInapplicable, coded 2 in SeekCancerInfo\nA lot\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nMissing data (Not Ascertained)\nInapplicable, coded 2 in UseInternet\nI do not use social media\nSomewhat\n\n\n4\n21000039\nNo\nInapplicable, coded 2 in SeekCancerInfo\nSome\nSome\nSome\nNot at all\nNot at all\nSome\nYes\nA lot\nSomewhat\n\n\n\n\n\n\n\nShape of the original dataset: (6252, 12)\n\n\n\n\nCode\n# Count missing values in each column\nmissing_values = hints_select.isna().sum()\n\n# Display the count of missing values\nprint(\"Missing values per column:\")\ndisplay(missing_values)\n\n\nMissing values per column:\n\n\nHHID                        0\nSeekCancerInfo              0\nCancerFrustrated            0\nCancerTrustDoctor           0\nCancerTrustFamily           0\nCancerTrustGov              0\nCancerTrustCharities        0\nCancerTrustReligiousOrgs    0\nCancerTrustScientists       0\nElectronic2_HealthInfo      0\nMisleadingHealthInfo        0\nTrustHCSystem               0\ndtype: int64\n\n\n\n\nCode\n# List of ordinal columns\nordinal_columns = [\n    \"SeekCancerInfo\", \"CancerFrustrated\", \"CancerTrustDoctor\",\n    \"CancerTrustFamily\", \"CancerTrustGov\", \"CancerTrustCharities\",\n    \"CancerTrustReligiousOrgs\", \"CancerTrustScientists\", \"Electronic2_HealthInfo\",\n    \"MisleadingHealthInfo\", \"TrustHCSystem\"\n]\n\n# Display unique values for each ordinal column\nprint(\"Unique values for ordinal columns:\")\nfor column in ordinal_columns:\n    unique_values = hints_select[column].unique()\n    print(f\"\\nColumn: {column}\")\n    print(f\"Unique Values: {unique_values}\")\n\n\nUnique values for ordinal columns:\n\nColumn: SeekCancerInfo\nUnique Values: ['No', 'Yes', 'Missing data (Not Ascertained)']\nCategories (3, object): ['Missing data (Not Ascertained)', 'No', 'Yes']\n\nColumn: CancerFrustrated\nUnique Values: ['Inapplicable, coded 2 in SeekCancerInfo', 'Somewhat disagree', 'Strongly disagree', 'Somewhat agree', 'Strongly agree', 'Question answered in error (Commission Error)', 'Missing data (Filter Missing)', 'Missing data (Not Ascertained)', 'Multiple responses selected in error']\nCategories (9, object): ['Inapplicable, coded 2 in SeekCancerInfo', 'Missing data (Filter Missing)', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', ..., 'Somewhat agree', 'Somewhat disagree', 'Strongly agree', 'Strongly disagree']\n\nColumn: CancerTrustDoctor\nUnique Values: ['A lot', 'Some', 'Not at all', 'A little', 'Missing data (Not Ascertained)', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustFamily\nUnique Values: ['Missing data (Not Ascertained)', 'Some', 'A little', 'Not at all', 'A lot', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustGov\nUnique Values: ['Missing data (Not Ascertained)', 'A lot', 'Some', 'A little', 'Not at all', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustCharities\nUnique Values: ['Missing data (Not Ascertained)', 'Some', 'A little', 'Not at all', 'A lot', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustReligiousOrgs\nUnique Values: ['Missing data (Not Ascertained)', 'Some', 'Not at all', 'A little', 'A lot', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: CancerTrustScientists\nUnique Values: ['Missing data (Not Ascertained)', 'A lot', 'Some', 'A little', 'Not at all', 'Multiple responses selected in error']\nCategories (6, object): ['A little', 'A lot', 'Missing data (Not Ascertained)', 'Multiple responses selected in error', 'Not at all', 'Some']\n\nColumn: Electronic2_HealthInfo\nUnique Values: ['Question answered in error (Commission Error)', 'Yes', 'Inapplicable, coded 2 in UseInternet', 'No', 'Missing data (Not Ascertained)', 'Missing data (Filter Missing)']\nCategories (6, object): ['Inapplicable, coded 2 in UseInternet', 'Missing data (Filter Missing)', 'Missing data (Not Ascertained)', 'No', 'Question answered in error (Commission Error)', 'Yes']\n\nColumn: MisleadingHealthInfo\nUnique Values: ['I do not use social media', 'Some', 'A lot', 'A little', 'None', 'Missing data (Not Ascertained)', 'Missing data (Web partial - Question Never Se...]\nCategories (7, object): ['A little', 'A lot', 'I do not use social media', 'Missing data (Not Ascertained)', 'Missing data (Web partial - Question Never Se..., 'None', 'Some']\n\nColumn: TrustHCSystem\nUnique Values: ['Very', 'Somewhat', 'A little', 'Not at all', 'Missing data (Web partial - Question Never Se..., 'Missing data (Not Ascertained)', 'Multiple responses selected in error']\nCategories (7, object): ['A little', 'Missing data (Not Ascertained)', 'Missing data (Web partial - Question Never Se..., 'Multiple responses selected in error', 'Not at all', 'Somewhat', 'Very']\n\n\n\n\nCode\n# Define the valid scales for each column\nvalid_scales = {\n    \"CancerFrustrated\": ['Somewhat disagree', 'Strongly disagree', 'Somewhat agree', 'Strongly agree'],\n    \"CancerTrustDoctor\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustFamily\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustGov\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustCharities\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustReligiousOrgs\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"CancerTrustScientists\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"TrustHCSystem\": ['A lot', 'Some', 'Not at all', 'A little'],\n    \"Electronic2_HealthInfo\": ['Yes', 'No'], \n    \"MisleadingHealthInfo\": ['I do not use social media', 'None', 'A little', 'Some', 'A lot']  \n}\n\n# Create a copy of the original DataFrame\nhints_cleaned = hints_select.copy()\n\n# Filter the DataFrame\nfor column, scale in valid_scales.items():\n    hints_cleaned = hints_cleaned[hints_cleaned[column].isin(scale)]\n\n# Display the cleaned dataset and its shape\nprint(\"Data after filtering invalid values:\")\ndisplay(hints_cleaned.head())\nprint(f\"Shape of the cleaned dataset: {hints_cleaned.shape}\")\n\n\nData after filtering invalid values:\n\n\n\n\n\n\n\n\n\nHHID\nSeekCancerInfo\nCancerFrustrated\nCancerTrustDoctor\nCancerTrustFamily\nCancerTrustGov\nCancerTrustCharities\nCancerTrustReligiousOrgs\nCancerTrustScientists\nElectronic2_HealthInfo\nMisleadingHealthInfo\nTrustHCSystem\n\n\n\n\n51\n21000330\nYes\nSomewhat disagree\nSome\nNot at all\nSome\nSome\nNot at all\nA lot\nYes\nA lot\nA little\n\n\n112\n21000976\nYes\nSomewhat agree\nA lot\nSome\nSome\nSome\nSome\nA lot\nYes\nSome\nA little\n\n\n136\n21001112\nYes\nSomewhat disagree\nA little\nA little\nNot at all\nNot at all\nNot at all\nA little\nNo\nA lot\nNot at all\n\n\n157\n21001283\nYes\nSomewhat disagree\nA lot\nSome\nNot at all\nA little\nSome\nNot at all\nNo\nI do not use social media\nNot at all\n\n\n181\n21001548\nYes\nStrongly agree\nA lot\nSome\nNot at all\nSome\nA lot\nA little\nYes\nSome\nA little\n\n\n\n\n\n\n\nShape of the cleaned dataset: (323, 12)\n\n\n\n\nCode\n# Count the number of NA or NaN values in each column\nna_count = hints_cleaned.isna().sum()\n#print(\"NA values count per column:\")\n#print(na_count)\n#print(hints_cleaned.shape)\n# Count unique values in the 'SeekCancerInfo' column\nvalue_counts = hints_cleaned['SeekCancerInfo'].value_counts()\n#print(\"Unique value counts in 'SeekCancerInfo':\")\n#print(value_counts)\n\n# Save the cleaned dataset to an Excel file\noutput_file = \"../data/csv/hints_cleaned_forML_spearman.csv\"\nhints_cleaned.to_csv(output_file, index=False)\n\n#print(f\"Cleaned dataset saved as {output_file}\")\n\n\nUnique value counts in 'SeekCancerInfo':\nSeekCancerInfo\nYes                               323\nMissing data (Not Ascertained)      0\nNo                                  0\nName: count, dtype: int64\nCleaned dataset saved as ../data/csv/hints_cleaned_forML_spearman.csv"
  },
  {
    "objectID": "data_cleaning.html#reddit-preparation-of-the-data",
    "href": "data_cleaning.html#reddit-preparation-of-the-data",
    "title": "About the Data and Data Cleaning",
    "section": "Reddit Preparation of the Data",
    "text": "Reddit Preparation of the Data\nThe data was queried from the Reddit Pushshift dataset. Following the themes captured in the HINTs dataset, we performanced an intial eight queries searching for comments that included keywords in each of the questions in the HINTs dataset. The initial query was performed in AWS on a sample of the data. After reviewing some of the comments, all the unique subreddits were found. Searching through these subreddits, we made a list of subreddits that actually included comments about cancer and filtered out any of the subreddits that were not relevant to health at all.\nList of Cancer Subreddits that discussed cancer in the comments.\nsubreddit_list = ['CrohnsDisease', 'thyroidcancer', 'AskDocs', 'UlcerativeColitis', 'Autoimmune', \n              'BladderCancer', 'breastcancer', 'CancerFamilySupport', 'doihavebreastcancer', \n              'WomensHealth', 'ProstateCancer', 'cll', 'Microbiome', 'predental', 'endometrialcancer', \n              'cancer', 'Hashimotos', 'coloncancer', 'PreCervicalCancer', 'lymphoma', 'Lymphedema', \n              'CancerCaregivers', 'braincancer', 'lynchsyndrome', 'nursing', 'testicularcancer', 'leukemia', \n              'publichealth', 'Health', 'Fuckcancer', 'HealthInsurance', 'BRCA', 'Cancersurvivors', \n              'pancreaticcancer', 'skincancer', 'stomachcancer']\nThese subreddits were compared to a random sample from the full Reddit dataset excluding the list of cancer subreddits above.\nQueries were conducted in Azure ML using Spark, with the data sourced from the instructor’s Azure Blob container. Comments from both cancer-related and non-cancer subreddits were processed using an Azure ML job and saved as Parquet files in an Azure Blob container. The job applied a filter to separate cancer subreddits into one Parquet file and non-cancer subreddits into another. For the non-cancer subreddits, the data was randomized before filtering out the cancer-related subreddits.\n\n\nCode\n# Path to the Azure ML Blob Container\nworkspace_default_storage_account = \"projectgstoragedfb938a3e\"\nworkspace_default_container = \"azureml-blobstore-becc8696-e562-432e-af12-8a5e3e1f9b0f\"\nworkspace_wasbs_base_url = f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\"\n\ncomments_path = \"cancer/comments\"\nsubmissions_path = \"cancer/submissions\"\n\n\nPySpark was used to clean the data by removing leading and trailing whitespaces, removing punctuation (using regex), removing underscores, and converting to lowercase. Both subsets of data were limited to 10,000 rows in order to allow a reasonable compute time for each job. After the data was cleaned it was saved into a two parquet files in an Azure ML blob container to use for the rest of the project. The combined cancer subreddits and non-cancer subreddit totaled in 20,000 rows.\n\n\nCode\n# Cancer subset of Reddit Data saved to an Azure ML Blob Container\noutput_path = f\"{workspace_wasbs_base_url}cancer_subreddit.parquet\"\n\n# Non-cancer subset of Reddit Data saved to an Azure ML Blob Container\noutput_path = f\"{workspace_wasbs_base_url}not_cancer_subreddit.parquet\"\n\n\nThe source code for the cleaning the Reddit data is in GitHub: fall-2024-project-team-35/code/spark-job-sample-data"
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "NLP",
    "section": "",
    "text": "Two different types of sentiment analysis were performed on both the cancer and non-cancer subreddits. In this method, a pretrained SparkNLP pipeline in Azure ML was used to classify the comments as positive, negative, or neutral.\n\n\n\nsentiment_label\ncount_cancer\ncount_non_cancer\n\n\n\n\npositive\n5929\n5715\n\n\nnegative\n4071\n4285\n\n\n\n\nData Processing Comments from cancer-related and non-cancer subreddits were queried using a job in Azure ML and saved as Parquet files in an Azure Blob container. In the job, a filter was used to include cancer subreddits in a Parquet file and exclude cancer subreddits in another Parquet file. In the non-cancer subreddit, the data was randomized before excluding the cancer subreddits.\nSentiment Analysis using SparkNLP Using the Spark NLP analyze_sentiment pretrained pipeline, the comments were analyzed to classify their sentiment as positive, negative, or neutral.\nWeighted Scores A custom function mapped sentiment labels to numerical scores (positive = 1, negative = -1, neutral = 0) to compute a weighted sentiment score for each comment.\nSentiment Labeling Comments were labeled based on their weighted scores (positive, negative, or neutral).\nGrouping and Aggregation The data was grouped by sentiment labels, and counts were computed for cancer-related and non-cancer subreddits.\nStatistical Testing A Chi-square test was conducted to assess whether the differences in sentiment distribution between the two categories showed a difference in positive and negative sentiment in the cancer and non-cancer subreddits.\n\nThe source code for the sentiment analysis using SparkNLP is here: https://github.com/gu-dsan6000/fall-2024-project-team-35/tree/main/code/nlp/spark-job-with-sparknlp-sentiment\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\nResults of Chi-square Test\nChi-square statistic: 9.325853191514204\nP-value: 0.0022594309243996907\nDegrees of freedom: 1\nExpected frequencies: [[5822. 5822.]\n[4178. 4178.]]\nTo assess the difference between positive and negative sentiment in the cancer and non-cancer subreddits the odds ratio (OR) is calculated as:\n\\[Odds Ratio (OR) = \\frac{(\\text{Positive Cancer} / \\text{Positive Non-Cancer})}{(\\text{Negative Cancer} / \\text{Negative Non-Cancer})}\\]\n\\[OR = \\frac{(5929/5715)}{(4071/4285)}\\]\n\\[OR = \\frac{(5929 \\times 4285)}{(5715 \\times 4071)}\\]\n\\[OR = 1.065\\]\n\\[\\beta = ln(OR)\\]\n\\[\\beta = ln(1.065) = 0.063\\]\nThe coefficient indicates the log odds of cancer for the “positive” sentiment label relative to the “negative” sentiment label. A positive value of 0.063 suggests that there is a slight increase in “positive” sentiment in the cancer subreddits compared to the non-cancer subreddits.\nThe p-value of 0.002 confirms a statistical significance. However, with the odds ratio, the positive sentiment is only slightly increased.\nPositive sentiment was higher in the cancer subreddit indicating the these groups may provide support and resiliency in the communities who use these subreddits. In comparison to the rest of Reddit (the non-cancer subreddits), users may look for look and discuss for more information about experience with treatment, logistics, and navigating a cancer diagnosis, fostering a space for encouragement and information.\n\n\n\n\n\n\nTo analyze the emotions in the Reddit Pushshift dataset, the NRC Emotion Lexicon was used. The NRC Emotion Lexicon is a type of sentiment analysis that calculates the negative, positive, and eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust). The annotations to determine emotions were manually done through crowdsourcing according to Saif Mohammad and Peter Turney.\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubreddit\npositive\nnegative\nneutral\nanger\nfear\ndisgust\nsadness\njoy\nsurprise\ntrust\n\n\n\n\nCancer\n0.202806\n0.119079\n0.0\n0.037681\n0.067383\n0.033798\n0.058226\n0.044399\n0.028933\n0.119079\n\n\nNon-Cancer\n0.170739\n0.096125\n0.0\n0.034273\n0.041384\n0.023316\n0.034463\n0.044963\n0.024675\n0.082917\n\n\n\nIn the NRC Lexicon Sentiment Analysis, positive and negative sentiment were both higher in the cancer subreddits compared to the non-cancer subreddits. These findings may indicate that users in the cancer subreddits have complicated sentiment toward cancer discussions. The analysis also showed higher trust, fear, sadness, and disgust. Trust is a particularly noteworthy finding, as it aligns with HINTS’ broader goal of deepening our understanding of trust in cancer-related communication.\nThe source code for the sentiment analysis using NRC-Lex is here: https://github.com/gu-dsan6000/fall-2024-project-team-35/blob/main/code/nlp/spark-job-with-sparknlp-sentiment/nrc_lex_averages_plot.ipynb\n\n\n\n\n\n\nGiven the results of the sentiment analysis, the frequency of popular words in the cancer subreddit were assessed using term frequency-inverse document frequency (TF-IDF). This method was used to identify unique or prominent keywords.\n\nData preparation\n\n\nTwo separate datasets were loaded: one for cancer-related subreddits and another for non-cancer subreddits.\nThe datasets were combined into a single DataFrame with an additional column specifying the source (cancer or non_cancer).\nText content was preprocessed into a column for TF-IDF analysis.\n\n\nTF-IDF VECTORIZATION\n\n\nThe TfidfVectorizer from scikit-learn was applied separately for cancer text data to compute TF-IDF scores for each word in the respective datasets.\nCommon stop words were removed, and the top 20 keywords were extracted based on their aggregated TF-IDF scores across all documents.\n\n\nAGGREGATION & RANKING\n\n\nWords were ranked in descending order of their total TF-IDF scores for the cancer dataset.\nRankings allowed for the identification of keywords in the cancer dataset.\n\n\n\n\nWord\nTF-IDF\n\n\n\n\ntime\n604.333288\n\n\nremoved\n510.072719\n\n\nthank\n508.752837\n\n\ninformation\n491.421062\n\n\ndoctor\n480.772518\n\n\npeople\n467.593201\n\n\nwork\n443.772165\n\n\npatient\n387.074392\n\n\ncancer\n365.678933\n\n\nbest\n340.635342\n\n\n\n\n\n\nTF-IDF\n\n\nIn this word cloud we see words such as time, removed, thank, and information. Some of these words may be used in conversations to talk about cancer care and possibly gratitude from support groups in the Reddit forum.\nThe source code for the TF-IDF is in the same notebook as nrc_lex.ipynb: https://github.com/gu-dsan6000/fall-2024-project-team-35/blob/main/code/nlp/spark-job-with-sparknlp-sentiment/nrc_lex.ipynb\n\n\n\n\n\nCrowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.\nEmotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon, Saif Mohammad and Peter Turney, In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, LA, California."
  },
  {
    "objectID": "nlp.html#sparknlp",
    "href": "nlp.html#sparknlp",
    "title": "NLP",
    "section": "",
    "text": "Two different types of sentiment analysis were performed on both the cancer and non-cancer subreddits. In this method, a pretrained SparkNLP pipeline in Azure ML was used to classify the comments as positive, negative, or neutral.\n\n\n\nsentiment_label\ncount_cancer\ncount_non_cancer\n\n\n\n\npositive\n5929\n5715\n\n\nnegative\n4071\n4285\n\n\n\n\nData Processing Comments from cancer-related and non-cancer subreddits were queried using a job in Azure ML and saved as Parquet files in an Azure Blob container. In the job, a filter was used to include cancer subreddits in a Parquet file and exclude cancer subreddits in another Parquet file. In the non-cancer subreddit, the data was randomized before excluding the cancer subreddits.\nSentiment Analysis using SparkNLP Using the Spark NLP analyze_sentiment pretrained pipeline, the comments were analyzed to classify their sentiment as positive, negative, or neutral.\nWeighted Scores A custom function mapped sentiment labels to numerical scores (positive = 1, negative = -1, neutral = 0) to compute a weighted sentiment score for each comment.\nSentiment Labeling Comments were labeled based on their weighted scores (positive, negative, or neutral).\nGrouping and Aggregation The data was grouped by sentiment labels, and counts were computed for cancer-related and non-cancer subreddits.\nStatistical Testing A Chi-square test was conducted to assess whether the differences in sentiment distribution between the two categories showed a difference in positive and negative sentiment in the cancer and non-cancer subreddits.\n\nThe source code for the sentiment analysis using SparkNLP is here: https://github.com/gu-dsan6000/fall-2024-project-team-35/tree/main/code/nlp/spark-job-with-sparknlp-sentiment\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\nResults of Chi-square Test\nChi-square statistic: 9.325853191514204\nP-value: 0.0022594309243996907\nDegrees of freedom: 1\nExpected frequencies: [[5822. 5822.]\n[4178. 4178.]]\nTo assess the difference between positive and negative sentiment in the cancer and non-cancer subreddits the odds ratio (OR) is calculated as:\n\\[Odds Ratio (OR) = \\frac{(\\text{Positive Cancer} / \\text{Positive Non-Cancer})}{(\\text{Negative Cancer} / \\text{Negative Non-Cancer})}\\]\n\\[OR = \\frac{(5929/5715)}{(4071/4285)}\\]\n\\[OR = \\frac{(5929 \\times 4285)}{(5715 \\times 4071)}\\]\n\\[OR = 1.065\\]\n\\[\\beta = ln(OR)\\]\n\\[\\beta = ln(1.065) = 0.063\\]\nThe coefficient indicates the log odds of cancer for the “positive” sentiment label relative to the “negative” sentiment label. A positive value of 0.063 suggests that there is a slight increase in “positive” sentiment in the cancer subreddits compared to the non-cancer subreddits.\nThe p-value of 0.002 confirms a statistical significance. However, with the odds ratio, the positive sentiment is only slightly increased.\nPositive sentiment was higher in the cancer subreddit indicating the these groups may provide support and resiliency in the communities who use these subreddits. In comparison to the rest of Reddit (the non-cancer subreddits), users may look for look and discuss for more information about experience with treatment, logistics, and navigating a cancer diagnosis, fostering a space for encouragement and information."
  },
  {
    "objectID": "nlp.html#nrc-lex",
    "href": "nlp.html#nrc-lex",
    "title": "NLP",
    "section": "",
    "text": "To analyze the emotions in the Reddit Pushshift dataset, the NRC Emotion Lexicon was used. The NRC Emotion Lexicon is a type of sentiment analysis that calculates the negative, positive, and eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust). The annotations to determine emotions were manually done through crowdsourcing according to Saif Mohammad and Peter Turney.\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\n\n\n\nSentiment in Cancer and Non-Cancer Subreddits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubreddit\npositive\nnegative\nneutral\nanger\nfear\ndisgust\nsadness\njoy\nsurprise\ntrust\n\n\n\n\nCancer\n0.202806\n0.119079\n0.0\n0.037681\n0.067383\n0.033798\n0.058226\n0.044399\n0.028933\n0.119079\n\n\nNon-Cancer\n0.170739\n0.096125\n0.0\n0.034273\n0.041384\n0.023316\n0.034463\n0.044963\n0.024675\n0.082917\n\n\n\nIn the NRC Lexicon Sentiment Analysis, positive and negative sentiment were both higher in the cancer subreddits compared to the non-cancer subreddits. These findings may indicate that users in the cancer subreddits have complicated sentiment toward cancer discussions. The analysis also showed higher trust, fear, sadness, and disgust. Trust is a particularly noteworthy finding, as it aligns with HINTS’ broader goal of deepening our understanding of trust in cancer-related communication.\nThe source code for the sentiment analysis using NRC-Lex is here: https://github.com/gu-dsan6000/fall-2024-project-team-35/blob/main/code/nlp/spark-job-with-sparknlp-sentiment/nrc_lex_averages_plot.ipynb"
  },
  {
    "objectID": "nlp.html#tf-idf",
    "href": "nlp.html#tf-idf",
    "title": "NLP",
    "section": "",
    "text": "Given the results of the sentiment analysis, the frequency of popular words in the cancer subreddit were assessed using term frequency-inverse document frequency (TF-IDF). This method was used to identify unique or prominent keywords.\n\nData preparation\n\n\nTwo separate datasets were loaded: one for cancer-related subreddits and another for non-cancer subreddits.\nThe datasets were combined into a single DataFrame with an additional column specifying the source (cancer or non_cancer).\nText content was preprocessed into a column for TF-IDF analysis.\n\n\nTF-IDF VECTORIZATION\n\n\nThe TfidfVectorizer from scikit-learn was applied separately for cancer text data to compute TF-IDF scores for each word in the respective datasets.\nCommon stop words were removed, and the top 20 keywords were extracted based on their aggregated TF-IDF scores across all documents.\n\n\nAGGREGATION & RANKING\n\n\nWords were ranked in descending order of their total TF-IDF scores for the cancer dataset.\nRankings allowed for the identification of keywords in the cancer dataset.\n\n\n\n\nWord\nTF-IDF\n\n\n\n\ntime\n604.333288\n\n\nremoved\n510.072719\n\n\nthank\n508.752837\n\n\ninformation\n491.421062\n\n\ndoctor\n480.772518\n\n\npeople\n467.593201\n\n\nwork\n443.772165\n\n\npatient\n387.074392\n\n\ncancer\n365.678933\n\n\nbest\n340.635342\n\n\n\n\n\n\nTF-IDF\n\n\nIn this word cloud we see words such as time, removed, thank, and information. Some of these words may be used in conversations to talk about cancer care and possibly gratitude from support groups in the Reddit forum.\nThe source code for the TF-IDF is in the same notebook as nrc_lex.ipynb: https://github.com/gu-dsan6000/fall-2024-project-team-35/blob/main/code/nlp/spark-job-with-sparknlp-sentiment/nrc_lex.ipynb"
  },
  {
    "objectID": "nlp.html#sources",
    "href": "nlp.html#sources",
    "title": "NLP",
    "section": "",
    "text": "Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.\nEmotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon, Saif Mohammad and Peter Turney, In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, LA, California."
  },
  {
    "objectID": "eda-nlp-reddit.html",
    "href": "eda-nlp-reddit.html",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "",
    "text": "Entire comments and subcomments that contain the word “cancer”\n\n(Parallel to A1, B3a, B13 in HINTS?)\nCalculate frequency per year\n\nReddits and subreddits that contain the word “cancer”\n\nIn addition, we combed the entire Reddit PushShift dataset provided based on the following criteria:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nFrustratingCancerSearch\n“Frustrating” or “frustrat” and “cancer” (HINTS A2b)\n\n\nCancerDoctorsTrust\n“Cancer” and “doctors” or “trust” (does not need to contain “trust” since trust is included in NRC sentiment analysis) (HINTS A3a)\n\n\nCancerFamilyTrust\n“Cancer” and “family” or “friends” or various familial terms (e.g., “sister,” “brother,” “mother”) or “trust” (HINTS A3b)\n\n\nCancerGovHealthcarePrograms\n“Cancer” and government_healthcare_programs, such as [“Medicare,” “Medicaid,” “ACA,” “NIH,” etc.] or “trust” (HINTS A3c)\n\n\nCancerCharitiesTrust\n“Cancer” and cancer_charities, such as [“American Cancer Society,” “LLS,” “BCRF,” “Livestrong,” etc.] or “trust” (HINTS A3d)\n\n\nCancerReligiousOrgTrust\n“Cancer” and charitable_religious_organizations, such as [“Catholic Relief Services,” “World Vision,” “Salvation Army,” etc.] or “trust” (HINTS A3e)\n\n\nCancerScientistsTrust\n“Cancer” and top_cancer_institutes, such as [“MD Anderson,” “Mayo Clinic,” “UCSF,” “Roswell Park,” etc.] or “trust” (HINTS A3f)\n\n\nInsuranceCancer\n“Insurance” and “cancer”\n\n\nMedicareCancer\n“Medicare” and “cancer”\n\n\nMedicaidCancer\n“Medicaid” and “cancer”"
  },
  {
    "objectID": "eda-nlp-reddit.html#are-certain-subreddits-more-likely-to-express-interpersonal-conflict-or-emotional-support",
    "href": "eda-nlp-reddit.html#are-certain-subreddits-more-likely-to-express-interpersonal-conflict-or-emotional-support",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "Are certain subreddits more likely to express interpersonal conflict or emotional support?",
    "text": "Are certain subreddits more likely to express interpersonal conflict or emotional support?\nPurpose: This analysis explores which subreddits are more likely to feature discussions about interpersonal conflict or emotional support, highlighting the role of community dynamics in health-related experiences. By examining the frequency of keywords associated with conflict—such as negative emotions of frustration, struggles, and relational challenges—and support keywords that signify kindness, empathy, and encouragement, we aim to understand whether these communities approach health challenges in a unified and positive manner or show signs of division within.\nOutcome: The analysis reveals conflict and support mentions across healthcare subreddits reveals varying dynamics in community interactions. Subreddits such as nursing, AskDocs, and breastcancer exhibit high levels of both conflict and support, indicating that members frequently discuss mutual encouragement but also deal with frustrations. In contrast, when compared to other subreddits such as HealthInsurance, CrohnsDisease and cancer have relatively high support but low conflict, which suggests people are getting encouragement and unity in these subreddits.\n\n\n\nAnalysis 1"
  },
  {
    "objectID": "eda-nlp-reddit.html#what-is-the-average-sentiment-score",
    "href": "eda-nlp-reddit.html#what-is-the-average-sentiment-score",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "What is the average sentiment score?",
    "text": "What is the average sentiment score?\nPurpose: The purpose of calculating the average sentiment score is to understand the general emotional tone across different subreddits. By examining the average sentiment for each subreddit, we can gain insights into whether discussions within these communities tend to be positive, negative, or neutral overall. This analysis helps identify which healthcare communities are more optimistic or challenged in their outlook, offering a broader view of the emotional landscape within each subreddit.\nOutcome: The sentiment analysis for healthcare subreddits from the months of June 2023- July 2024, reveals a range of emotional tones across communities. Subreddits like publichealth(0.9- June 2023), and lymphoma(0.71, January 2024) and Prostatecancer(0.55- December 2023) have positive sentiment scores, suggesting a upportive or encouraging atmosphere among members. On the other hand, communities like Autoimmune(-0.4), thyroidcancer(-0.41) and coloncancer(-0.3) show more negative sentiment, potentially indicating challenges, frustrations, or concerns commonly shared by these users. Several subreddits, such as nursing, breastcancer and HealthInsurance exhibit scores closer to neutral, implying a balanced mix of positive and negative experiences. This on an overall scale of sentiment scores highlights the diversity of emotional climates within these healthcare communities, reflecting the unique support needs and challenges faced by each group.\n\n\n\nAnalysis 2"
  },
  {
    "objectID": "eda-nlp-reddit.html#what-are-the-most-common-words-or-phrases-for-positive-versus-negative-comments",
    "href": "eda-nlp-reddit.html#what-are-the-most-common-words-or-phrases-for-positive-versus-negative-comments",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "What are the most common words or phrases for positive versus negative comments?",
    "text": "What are the most common words or phrases for positive versus negative comments?\nPurpose: Identifying the most common words in positive versus negative comments provides insight into the experiences and attitudes within the community. This analysis helps uncover topics, concerns, and emotions associated with each sentiment, offering a more nuanced understanding of the community’s overall mood and focus.\nOutcome: The most common words within positive and negative comments, we observe and remove many stopwords that could be considered insignificant for our text analysis to see what words lead to more positive and negative comments. Words like “doctor”, “first” and “feel” appear across both sentiment categories, highlighting the centrality of health experiences in these discussions. In positive comments, expressions such as “Concern,” “questions,” and “action” indicate explanatory language, often associated with sharing advice or encouragement. Conversely, negative comments include words like “Patient”, “cancer” and “work” hinting at expressions of dissatisfaction, frustration, or challenges.\n\n\n\nAnalysis 3"
  },
  {
    "objectID": "eda-nlp-reddit.html#what-are-the-most-common-expressions-of-gratitude-frustration-hope-fear-sadness-or-joy",
    "href": "eda-nlp-reddit.html#what-are-the-most-common-expressions-of-gratitude-frustration-hope-fear-sadness-or-joy",
    "title": "Reddit PushShift Exloratory Data Analysis",
    "section": "What are the most common expressions of gratitude, frustration, hope, fear, sadness or joy?",
    "text": "What are the most common expressions of gratitude, frustration, hope, fear, sadness or joy?\nPurpose: Identifying the most common expressions of gratitude, frustration, joy, fear, sadness and hope offers valuable insight into the language used for support, shared challenges, and encouragement within healthcare communities. This analysis helps uncover how individuals convey their emotions, revealing the words and phrases that resonate most when expressing appreciation, challenges, or optimism.\nNote: The words are editable\nOutcome: In our analysis, we determined that gratitude is the most frequent emotion among the three categories, with over 1750+ mentions. Hope is a close second with around 1300 mentions. This suggests that individuals within these healthcare communities often express optimism or a desire for positive outcomes. Expressions of joy, with over 250 mentions, indicate happiness for support and shared experiences. Fear, sadness and frustration are mentioned less frequently than the other emotions with around 250 mentions at maximum, signaling that while there are challenges, they may be less dominant in the discourse compared to hopeful or thankful sentiments. This analysis uses keywords like “thank you”, “afraid”, and “sad”, to help highlight the tone within these communities.\n\n\n\nAnalysis 4"
  },
  {
    "objectID": "eda_hints.html",
    "href": "eda_hints.html",
    "title": "EDA HINTS DATA",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(gtsummary)\nlibrary(reshape2)\nlibrary(RColorBrewer)\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(caret)\n\n\nThe HINTS survey includes 527 questions. To focus on sentiment toward healthcare and cancer, we selected the questions in Table 1. Participants were asked various questions about where they access health information, do they trust the provided health information, and if they feel frustrated about the information.\nTable 1\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSeekCancerInfo\nHave you ever looked for information about cancer from any source?\n\n\nCancerFrustrated\nBased on the results of your most recent search for information about cancer, how much do you agree or disagree: You felt frustrated during your search for the information.\n\n\nCancerTrustDoctor\nIn general, how much would you trust information about cancer from a doctor?\n\n\nCancerTrustFamily\nIn general, how much would you trust information about cancer from family or friends?\n\n\nCancerTrustGov\nIn general, how much would you trust information about cancer from government health agencies?\n\n\nCancerTrustCharities\nIn general, how much would you trust information about cancer from charitable organizations?\n\n\nCancerTrustReligiousOrgs\nIn general, how much would you trust information about cancer from religious organizations and leaders?\n\n\nCancerTrustScientists\nIn general, how much would you trust information about cancer from scientists?\n\n\nElectronic2_HealthInfo\nIn the past 12 months have you used the Internet to look for health or medical information?\n\n\nMisleadingHealthInfo\nHow much of the health information that you see on social media do you think is false or misleading?\n\n\nTrustHCSystem\nHow much do you trust the health care system (for example, hospitals, pharmacies, and other organizations involved in health care)?\n\n\n\n\n\n\nSpearman Correlation Matrix\n\n\nThe correlations from the Spearman Matrix show that there might be some linkage between trust - ‘CancerTrustDoctor’ ‘CancerTrustGov’ ‘CancerTrustCharities’ ‘CancerTrustScientists’ and ‘CancerTrustReligiousOrgs’ all had the highest correlations. This seems to show that trust in one entity might be an element of disposition - that individuals who trust in one area are likely to feel similarly about another.\nCode for the Spearman Correlation Matrix can be found in the code/hints_regression.ipynb file.\n\n\nCode\nload('../data/HINTS6_R_20240524/hints6_public.rda')\nhints &lt;- as.data.frame(public)\n\n#print(colnames(hints))\n\ncolumns &lt;- c(\"HHID\", \"updatedate\", \"SeekCancerInfo\", \"CancerFrustrated\", \"CancerTrustDoctor\", \"CancerTrustFamily\", \"CancerTrustGov\", \"CancerTrustCharities\", \"CancerTrustReligiousOrgs\", \"CancerTrustScientists\", \"Electronic2_HealthInfo\", \"MisleadingHealthInfo\", \"TrustHCSystem\")\n\nhints_select &lt;- hints %&gt;% select(all_of(columns))\n#hints_select$updatedate &lt;- hints_select$updatedate / 1000\n#hints_select$updatedate &lt;- as_datetime(hints_select$updatedate)\n\nhead(hints_select)\n\n\n      HHID  updatedate SeekCancerInfo                        CancerFrustrated\n1 21000006 13870396800             No Inapplicable, coded 2 in SeekCancerInfo\n2 21000009 13874630400             No Inapplicable, coded 2 in SeekCancerInfo\n3 21000020 13873680000            Yes                       Somewhat disagree\n4 21000022 13867891200             No Inapplicable, coded 2 in SeekCancerInfo\n5 21000039 13866336000             No Inapplicable, coded 2 in SeekCancerInfo\n6 21000043 13866595200             No Inapplicable, coded 2 in SeekCancerInfo\n  CancerTrustDoctor              CancerTrustFamily\n1             A lot Missing data (Not Ascertained)\n2             A lot                           Some\n3             A lot                           Some\n4             A lot Missing data (Not Ascertained)\n5              Some                           Some\n6             A lot                           Some\n                  CancerTrustGov           CancerTrustCharities\n1 Missing data (Not Ascertained) Missing data (Not Ascertained)\n2                          A lot                           Some\n3                           Some                       A little\n4 Missing data (Not Ascertained) Missing data (Not Ascertained)\n5                           Some                     Not at all\n6                           Some                          A lot\n        CancerTrustReligiousOrgs          CancerTrustScientists\n1 Missing data (Not Ascertained) Missing data (Not Ascertained)\n2                           Some                          A lot\n3                     Not at all                          A lot\n4 Missing data (Not Ascertained) Missing data (Not Ascertained)\n5                     Not at all                           Some\n6                       A little                          A lot\n                         Electronic2_HealthInfo      MisleadingHealthInfo\n1 Question answered in error (Commission Error) I do not use social media\n2                                           Yes I do not use social media\n3                                           Yes                      Some\n4          Inapplicable, coded 2 in UseInternet I do not use social media\n5                                           Yes                     A lot\n6                                           Yes                     A lot\n  TrustHCSystem\n1          Very\n2          Very\n3      Somewhat\n4      Somewhat\n5      Somewhat\n6      A little\n\n\n\nSurvey Responses\nIn the bar graphs, a first look at the data provides an general overview of the responses to the questions. These plots show how much participants agree or disagree to each question. For example, many participants trust scientists, but trend to trusting religious organizations less.\n\n\nCode\nplot_data &lt;- hints_select %&gt;%\n  select(-HHID, -updatedate) %&gt;%  # Exclude the first two columns\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Value\") %&gt;%\n  count(Variable, Value)\n\nprint(unique(plot_data$Value))\n\n\n [1] Missing data (Not Ascertained)                  \n [2] Missing data (Filter Missing)                   \n [3] Multiple responses selected in error            \n [4] Question answered in error (Commission Error)   \n [5] Inapplicable, coded 2 in SeekCancerInfo         \n [6] Strongly agree                                  \n [7] Somewhat agree                                  \n [8] Somewhat disagree                               \n [9] Strongly disagree                               \n[10] A lot                                           \n[11] Some                                            \n[12] A little                                        \n[13] Not at all                                      \n[14] Yes                                             \n[15] No                                              \n[16] Inapplicable, coded 2 in UseInternet            \n[17] Missing data (Web partial - Question Never Seen)\n[18] None                                            \n[19] I do not use social media                       \n[20] Very                                            \n[21] Somewhat                                        \n21 Levels: Missing data (Not Ascertained) Yes ... Somewhat\n\n\nCode\nvalues &lt;- c(\"Strongly agree\", \"Somewhat agree\", \"Somewhat disagree\", \"Strongly disagree\", \"A lot\", \"Some\", \n            \"A little\", \"Not at all\", \"Yes\", \"No\", \"None\", \"I do not use social media\", \"Very\", \"Somewhat\")\n\nplot_data_filtered &lt;- plot_data %&gt;% filter(Value %in% values) \nplot_data_filtered$Value &lt;- factor(plot_data_filtered$Value, levels = sort(unique(plot_data_filtered$Value)))\n\ncolumns_1 &lt;- c(\"SeekCancerInfo\", \"CancerFrustrated\", \"CancerTrustDoctor\", \"CancerTrustFamily\")\nplot_data_filtered_1 &lt;- plot_data_filtered %&gt;% filter(Variable %in% columns_1)\ncolumns_2 &lt;- c(\"CancerTrustGov\", \"CancerTrustCharities\", \"CancerTrustReligiousOrgs\", \"CancerTrustScientists\")\nplot_data_filtered_2 &lt;- plot_data_filtered %&gt;% filter(Variable %in% columns_2)\ncolumns_3 &lt;- c(\"Electronic2_HealthInfo\", \"MisleadingHealthInfo\", \"TrustHCSystem\")\nplot_data_filtered_3 &lt;- plot_data_filtered %&gt;% filter(Variable %in% columns_3) \n\n\n\n# Create a custom color scale\ncustom_colors &lt;- c(\n  \"SeekCancerInfo\" = \"#ff4500\", \n  \"CancerFrustrated\" = \"#5f0922\", \n  \"CancerTrustDoctor\" = \"#ffa205\", \n  \"CancerTrustFamily\" = \"#08030a\"\n)\n\np &lt;- ggplot(plot_data_filtered_1, aes(x = Value, y = n, fill = Variable)) +\n  geom_bar(stat = \"identity\") + \n  scale_fill_manual(values = custom_colors) +\n  facet_wrap(~ Variable, scales = \"free_x\") +  # Separate plots for each column\n  theme_minimal() +\n  labs(\n    title = \"HINTS Survey Responses\",\n    x = \"Responses\",\n    y = \"Count\",\n    fill = \"Question\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1), \n        panel.background = element_rect(fill = \"#feeece\", color = \"#feeece\"), \n        plot.background = element_rect(fill = \"#feeece\", color = \"#feeece\")) \n\n\n\npng(\"../data/HINTS6_R_20240524/HINTS_plot1.png\", width = 800, height = 600)\nprint(p)  \ndev.off()\n\n\nquartz_off_screen \n                2 \n\n\n\n\nCode\n# Create a custom color scale\ncustom_colors &lt;- c(\n  \"CancerTrustGov\" = \"#ff4500\", \n  \"CancerTrustCharities\" = \"#5f0922\", \n  \"CancerTrustReligiousOrgs\" = \"#ffa205\", \n  \"CancerTrustScientists\" = \"#08030a\"\n)\n\np &lt;- ggplot(plot_data_filtered_2, aes(x = Value, y = n, fill = Variable)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = custom_colors) +\n  facet_wrap(~ Variable, scales = \"free_x\") +  \n  theme_minimal() +\n  labs(\n    title = \"HINTS Survey Responses\",\n    x = \"Responses\",\n    y = \"Count\",\n    fill = \"Question\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),panel.background = element_rect(fill = \"#feeece\", color = \"#feeece\"), \n  plot.background = element_rect(fill = \"#feeece\", color = \"#feeece\")) \n\np\n\n\n\n\n\n\n\n\n\nCode\npng(\"../data/HINTS6_R_20240524/HINTS_plot2.png\", width = 800, height = 600)\nprint(p)  \ndev.off()\n\n\nquartz_off_screen \n                2 \n\n\n\n\nCode\n# Create a custom color scale\ncustom_colors &lt;- c(\n  \"Electronic2_HealthInfo\" = \"#ff4500\", \n  \"MisleadingHealthInfo\" = \"#5f0922\", \n  \"TrustHCSystem\" = \"#ffa205\"\n)\np &lt;- ggplot(plot_data_filtered_3, aes(x = Value, y = n, fill = Variable)) +\n  geom_bar(stat = \"identity\") + \n  scale_fill_manual(values = custom_colors) +\n  facet_wrap(~ Variable, scales = \"free_x\", nrow=2) +\n  theme_minimal() +\n  labs(\n    title = \"HINTS Survey Responses\",\n    x = \"Responses\",\n    y = \"Count\",\n    fill = \"Question\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),panel.background = element_rect(fill = \"#feeece\", color = \"#feeece\"), \n  plot.background = element_rect(fill = \"#feeece\", color = \"#feeece\")) \n\np\n\n\n\n\n\n\n\n\n\nCode\npng(\"../data/HINTS6_R_20240524/HINTS_plot3.png\", width = 800, height = 600)\nprint(p) \ndev.off()\n\n\nquartz_off_screen \n                2 \n\n\n\n\nSummary Statistics of the Responses\nThe responses of the questions can be coded into a scale from 0:3. For example, “Not at all” is coded as 0, “A little” is coded as 1, “Some” is coded as 2, and “A lot” is coded as 3. After coding these responses, the mean, median, and mode is calculated to highlight overall trends in the data.\n\n\nCode\nprint(unique(hints_select$MisleadingHealthInfo))\n\n\n[1] I do not use social media                       \n[2] Some                                            \n[3] A lot                                           \n[4] A little                                        \n[5] None                                            \n[6] Missing data (Not Ascertained)                  \n[7] Missing data (Web partial - Question Never Seen)\n7 Levels: Missing data (Not Ascertained) ...\n\n\n\n\nCode\nprint(colnames(hints_select))\n\n\n [1] \"HHID\"                     \"updatedate\"              \n [3] \"SeekCancerInfo\"           \"CancerFrustrated\"        \n [5] \"CancerTrustDoctor\"        \"CancerTrustFamily\"       \n [7] \"CancerTrustGov\"           \"CancerTrustCharities\"    \n [9] \"CancerTrustReligiousOrgs\" \"CancerTrustScientists\"   \n[11] \"Electronic2_HealthInfo\"   \"MisleadingHealthInfo\"    \n[13] \"TrustHCSystem\"           \n\n\nCode\nhints_select_coded &lt;- hints_select %&gt;%\n  mutate(CancerFrustrated = as.numeric(case_when(\n    CancerFrustrated == \"Strongly disagree\" ~ \"0\",\n    CancerFrustrated == \"Somewhat disagree\" ~ \"1\",\n    CancerFrustrated == \"Somewhat agree\" ~ \"2\",\n    CancerFrustrated == \"Strongly agree\" ~ \"3\",\n    TRUE ~  NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(CancerTrustDoctor = as.numeric(case_when(\n    CancerTrustDoctor == \"Not at all\" ~ \"0\",\n    CancerTrustDoctor == \"A little\" ~ \"1\",\n    CancerTrustDoctor == \"Some\" ~ \"2\",\n    CancerTrustDoctor == \"A lot\" ~ \"3\",\n    TRUE ~  NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(CancerTrustFamily = as.numeric(case_when(\n    CancerTrustFamily == \"None\" ~ \"0\",\n    CancerTrustFamily == \"A little\" ~ \"1\",\n    CancerTrustFamily == \"Some\" ~ \"2\",\n    CancerTrustFamily == \"A lot\" ~ \"3\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(SeekCancerInfo = as.numeric(case_when(\n    SeekCancerInfo == \"Yes\" ~ \"1\",\n    SeekCancerInfo == \"No\" ~ \"0\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(CancerTrustCharities = as.numeric(case_when(\n    CancerTrustCharities == \"Not at all\" ~ \"0\",\n    CancerTrustCharities == \"A little\" ~ \"1\",\n    CancerTrustCharities == \"Some\" ~ \"2\",\n    CancerTrustCharities == \"A lot\" ~ \"3\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(CancerTrustGov = as.numeric(case_when(\n    CancerTrustGov == \"Not at all\" ~ \"0\",\n    CancerTrustGov == \"A little\" ~ \"1\",\n    CancerTrustGov == \"Some\" ~ \"2\",\n    CancerTrustGov == \"A lot\" ~ \"3\",\n    TRUE ~ NA\n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(CancerTrustReligiousOrgs = as.numeric(case_when(\n    CancerTrustReligiousOrgs == \"Not at all\" ~ \"0\",\n    CancerTrustReligiousOrgs == \"A little\" ~ \"1\",\n    CancerTrustReligiousOrgs == \"Some\" ~ \"2\",\n    CancerTrustReligiousOrgs == \"A lot\" ~ \"3\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(CancerTrustScientists = as.numeric(case_when(\n    CancerTrustScientists == \"Not at all\" ~ \"0\",\n    CancerTrustScientists == \"A little\" ~ \"1\",\n    CancerTrustScientists == \"Some\" ~ \"2\",\n    CancerTrustScientists == \"A lot\" ~ \"3\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(Electronic2_HealthInfo = as.numeric(case_when(\n    Electronic2_HealthInfo == \"Yes\" ~ \"1\",\n    Electronic2_HealthInfo == \"No\" ~ \"0\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(MisleadingHealthInfo = as.numeric(case_when(\n    MisleadingHealthInfo == \"None\" ~ \"0\",\n    MisleadingHealthInfo == \"I do not use social media\" ~ \"0\",\n    MisleadingHealthInfo == \"A little\" ~ \"1\",\n    MisleadingHealthInfo == \"Some\" ~ \"2\",\n    MisleadingHealthInfo == \"A lot\" ~ \"3\",\n    TRUE ~ NA \n  )))\n\nhints_select_coded &lt;- hints_select_coded %&gt;%\n  mutate(TrustHCSystem = as.numeric(case_when(\n    TrustHCSystem == \"Not at all\" ~ \"0\",\n    TrustHCSystem == \"A little\" ~ \"1\",\n    TrustHCSystem == \"Somewhat\" ~ \"2\",\n    TrustHCSystem == \"Very\" ~ \"3\",\n    TRUE ~ NA \n  )))\n\n\nprint(head(hints_select_coded))\n\n\n      HHID  updatedate SeekCancerInfo CancerFrustrated CancerTrustDoctor\n1 21000006 13870396800              0               NA                 3\n2 21000009 13874630400              0               NA                 3\n3 21000020 13873680000              1                1                 3\n4 21000022 13867891200              0               NA                 3\n5 21000039 13866336000              0               NA                 2\n6 21000043 13866595200              0               NA                 3\n  CancerTrustFamily CancerTrustGov CancerTrustCharities\n1                NA             NA                   NA\n2                 2              3                    2\n3                 2              2                    1\n4                NA             NA                   NA\n5                 2              2                    0\n6                 2              2                    3\n  CancerTrustReligiousOrgs CancerTrustScientists Electronic2_HealthInfo\n1                       NA                    NA                     NA\n2                        2                     3                      1\n3                        0                     3                      1\n4                       NA                    NA                     NA\n5                        0                     2                      1\n6                        1                     3                      1\n  MisleadingHealthInfo TrustHCSystem\n1                    0             3\n2                    0             3\n3                    2             2\n4                    0             2\n5                    3             2\n6                    3             1\n\n\nIn the summary table below, the mean for trusting a doctor is higher than trusting the government. Given this information, we will also look at the Reddit dataset to see the level of trust users have when they mention the government in their comments versus doctors. In addition, the people who felt frustrated about the information they received about cancer is approximately 1.105. In the Reddit dataset, we also look for an equivalent using textual data by looking at positive/negative and emotion sentiment analysis on comments that include the word “cancer”.\n\n\nCode\nprint(summary(hints_select_coded))\n\n\n     HHID             updatedate        SeekCancerInfo   CancerFrustrated\n Length:6252        Min.   :1.387e+10   Min.   :0.0000   Min.   :0.000   \n Class :character   1st Qu.:1.387e+10   1st Qu.:0.0000   1st Qu.:0.000   \n Mode  :character   Median :1.387e+10   Median :0.0000   Median :1.000   \n                    Mean   :1.387e+10   Mean   :0.4654   Mean   :1.105   \n                    3rd Qu.:1.387e+10   3rd Qu.:1.0000   3rd Qu.:2.000   \n                    Max.   :1.389e+10   Max.   :1.0000   Max.   :3.000   \n                                        NA's   :17       NA's   :3420    \n CancerTrustDoctor CancerTrustFamily CancerTrustGov CancerTrustCharities\n Min.   :0.000     Min.   :1.000     Min.   :0.00   Min.   :0.000       \n 1st Qu.:2.000     1st Qu.:1.000     1st Qu.:1.00   1st Qu.:1.000       \n Median :3.000     Median :2.000     Median :2.00   Median :1.000       \n Mean   :2.656     Mean   :1.678     Mean   :1.92   Mean   :1.403       \n 3rd Qu.:3.000     3rd Qu.:2.000     3rd Qu.:3.00   3rd Qu.:2.000       \n Max.   :3.000     Max.   :3.000     Max.   :3.00   Max.   :3.000       \n NA's   :94        NA's   :783       NA's   :273    NA's   :308         \n CancerTrustReligiousOrgs CancerTrustScientists Electronic2_HealthInfo\n Min.   :0.0000           Min.   :0.000         Min.   :0.0000        \n 1st Qu.:0.0000           1st Qu.:2.000         1st Qu.:1.0000        \n Median :1.0000           Median :3.000         Median :1.0000        \n Mean   :0.9484           Mean   :2.357         Mean   :0.8534        \n 3rd Qu.:2.0000           3rd Qu.:3.000         3rd Qu.:1.0000        \n Max.   :3.0000           Max.   :3.000         Max.   :1.0000        \n NA's   :280              NA's   :218           NA's   :1130          \n MisleadingHealthInfo TrustHCSystem\n Min.   :0.000        Min.   :0.0  \n 1st Qu.:1.000        1st Qu.:2.0  \n Median :2.000        Median :2.0  \n Mean   :1.716        Mean   :2.2  \n 3rd Qu.:3.000        3rd Qu.:3.0  \n Max.   :3.000        Max.   :3.0  \n NA's   :82           NA's   :134  \n\n\n\n\nCode\nhints_select_coded_clean &lt;- drop_na(hints_select_coded)\n\ntable1 &lt;- hints_select_coded_clean %&gt;%\n  select(-HHID, -updatedate) %&gt;%\n  tbl_summary(\n    statistic = all_continuous() ~ \"{mean} ± {sd}\",  \n  )\n\ntable1\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 2,3151\n\n\n\n\nSeekCancerInfo\n\n\n\n\n    1\n2,315 (100%)\n\n\nCancerFrustrated\n\n\n\n\n    0\n759 (33%)\n\n\n    1\n780 (34%)\n\n\n    2\n624 (27%)\n\n\n    3\n152 (6.6%)\n\n\nCancerTrustDoctor\n\n\n\n\n    0\n11 (0.5%)\n\n\n    1\n72 (3.1%)\n\n\n    2\n441 (19%)\n\n\n    3\n1,791 (77%)\n\n\nCancerTrustFamily\n\n\n\n\n    1\n970 (42%)\n\n\n    2\n1,225 (53%)\n\n\n    3\n120 (5.2%)\n\n\nCancerTrustGov\n\n\n\n\n    0\n133 (5.7%)\n\n\n    1\n357 (15%)\n\n\n    2\n1,065 (46%)\n\n\n    3\n760 (33%)\n\n\nCancerTrustCharities\n\n\n\n\n    0\n251 (11%)\n\n\n    1\n798 (34%)\n\n\n    2\n1,086 (47%)\n\n\n    3\n180 (7.8%)\n\n\nCancerTrustReligiousOrgs\n\n\n\n\n    0\n907 (39%)\n\n\n    1\n832 (36%)\n\n\n    2\n494 (21%)\n\n\n    3\n82 (3.5%)\n\n\nCancerTrustScientists\n\n\n\n\n    0\n41 (1.8%)\n\n\n    1\n165 (7.1%)\n\n\n    2\n608 (26%)\n\n\n    3\n1,501 (65%)\n\n\nElectronic2_HealthInfo\n2,203 (95%)\n\n\nMisleadingHealthInfo\n\n\n\n\n    0\n287 (12%)\n\n\n    1\n287 (12%)\n\n\n    2\n959 (41%)\n\n\n    3\n782 (34%)\n\n\nTrustHCSystem\n\n\n\n\n    0\n60 (2.6%)\n\n\n    1\n231 (10.0%)\n\n\n    2\n1,109 (48%)\n\n\n    3\n915 (40%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\nThe box plot provides a visualization of the median, mode, and outliers in the dataset.\n\n\nCode\nboxplot_data &lt;- hints_select_coded %&gt;%\n  select(-HHID, -updatedate)\n\n\nboxplot_data_long &lt;- boxplot_data %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Variable\", values_to = \"Value\")\n\n\nggplot(boxplot_data_long, aes(x = Variable, y = Value)) +\n  geom_boxplot(outlier.colour = \"red\", outlier.size = 1, fill =\"#ffa205\" ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),panel.background = element_rect(fill = \"#feeece\", color = \"#feeece\"), \n  plot.background = element_rect(fill = \"#feeece\", color = \"#feeece\")) +  \n  labs(\n    title = \"HINTS Boxplot\",\n    x = \"Variables\",\n    y = \"Values\", \n  ) + coord_cartesian(ylim = c(-1, 4)) \n\n\nWarning: Removed 6739 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nFurther Directions and Conclusion\nThe HINTS dataset provides insight into the perceptions of healthcare and cancer information. The trends in this dataset will be repeated in the Reddit dataset. Using the Reddit dataset, we will explore sentiments, such as positive/negative, frustrations, and trust. We will also look at word frequency count to review, which topics Reddit users commonly comment about."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Our team set out to understand how people navigate and trust health information. Our goal of examining public trust, frustrations, and sentiment about healthcare and cancer information was met - below are our questions and conclusions based on the analysis from previous sections.\n\n\nUsing our two data sources, Subreddits that mention cancer in comments and the Health Information National Trends Survey (HINTS), we were able to find a linkage between sentiment and many of the HINTS responses.\n\n\n\nTrust in scientists, religious institutions, the government, etc. might help us understand where individuals go and how they might feel when they are looking for medical advice or information related to cancer. After our analysis trying to predict Trust In Healthcare, we found that the XGBoost (Tuned) model was the best choice to predict Trust in Healthcare based on its highest accuracy (74%) and performance for High Trust in Healthcare. It had a strong recall of 90%. While this model still struggled with Low Trust in Healthcare, the improved overall performance makes it the most reliable model for the task at hand.\n\n\n\nIn order to investigate the sentiments and emotions in the Reddit data, the comments from cancer-related and non-cancer subreddits were loaded as Parquet files saved in an Azure Blob container. Using the Spark NLP analyze_sentiment pretrained pipeline, the comments were analyzed to classify their sentiment as positive, negative, or neutral. A custom function mapped sentiment labels to numerical scores (positive = 1, negative = -1, neutral = 0) to compute a weighted sentiment score for each comment. Comments were labeled based on their weighted scores (positive, negative, or neutral). The data was grouped by sentiment labels, and counts were computed for cancer-related and non-cancer subreddits. A Chi-square test was conducted to assess whether the differences in sentiment distribution between the two categories were statistically significant. Bar plots were created using Seaborn and Matplotlib to visualize sentiment distributions for cancer and non-cancer subreddit comments. Our biggest finding on this question was that cancer-related comments have a higher proportion of positive sentiment.\n\n\n\nThis question was target to identify unique or prominent keywords and identify thematic differences in the text-based data. Two separate datasets were loaded: one for cancer-related subreddits and another for non-cancer subreddits. The datasets were combined into a single DataFrame with an additional column specifying the source (cancer or non_cancer). Text content was preprocessed into a column for TF-IDF analysis. Then, the TfidfVectorizer from scikit-learn was applied separately for cancer text data to compute TF-IDF scores for each word in the respective datasets. Standard procedures were followed (common stop words were removed, and the top 20 keywords were extracted based on their aggregated TF-IDF scores across all documents). Words were ranked in descending order of their total TF-IDF scores for the cancer dataset. Rankings allowed for the identification of keywords in the cancer dataset. A word cloud was created to display the rank of keywords, with the top two or three were “time” “removed” and “thank”.\n\n\n\nIn this question the Machine Learning concept was classifying cancer-related and non-cancer subreddits using machine learning (Naive Bayes and Logistic Regression). For our approach, we loaded cancer-related and non-cancer subreddit comments and added source column to label as cancer and non. Datasets were combined into a single frame. Text was tokenized and stop words removed, and tokens were converted into raw features using CountVectorizer. The TF-IDF scores were computed using the IDF transformer. Using PySpark ML, created a Naive Bayes and Logistic Regression pipeline with the preprocessing steps. Data was split (80/20) for train and test.\nNaive Bayes outperforms Logistic Regression in text classification\nNaive Bayes:\nMultinomial Naive Bayes achieved test accuracy of 78%. * Precision: 78% * Recall: 78% * F1-Score: 78% * AUC-ROC: 78%\nLogistic Regression: Achieved test accuracy of 71%. * Precision: 71% * Recall: 71% * F1-Score: 71% * AUC-ROC: 71%\nAs this was text data, it was a little unsurprising that Naive Bayes outperformed logistic regression. Though 78% is not the highest test accuracy, it is still a healthy setting above 50% (blind guess)."
  },
  {
    "objectID": "summary.html#project-approach",
    "href": "summary.html#project-approach",
    "title": "Summary",
    "section": "",
    "text": "Using our two data sources, Subreddits that mention cancer in comments and the Health Information National Trends Survey (HINTS), we were able to find a linkage between sentiment and many of the HINTS responses."
  },
  {
    "objectID": "summary.html#question-1-is-trust-in-the-healtchare-system-influenced-by-other-factors-of-trust",
    "href": "summary.html#question-1-is-trust-in-the-healtchare-system-influenced-by-other-factors-of-trust",
    "title": "Summary",
    "section": "",
    "text": "Trust in scientists, religious institutions, the government, etc. might help us understand where individuals go and how they might feel when they are looking for medical advice or information related to cancer. After our analysis trying to predict Trust In Healthcare, we found that the XGBoost (Tuned) model was the best choice to predict Trust in Healthcare based on its highest accuracy (74%) and performance for High Trust in Healthcare. It had a strong recall of 90%. While this model still struggled with Low Trust in Healthcare, the improved overall performance makes it the most reliable model for the task at hand."
  },
  {
    "objectID": "summary.html#question-2-doe-sentiment-vary-between-cancer-and-non-cancer-subreddits",
    "href": "summary.html#question-2-doe-sentiment-vary-between-cancer-and-non-cancer-subreddits",
    "title": "Summary",
    "section": "",
    "text": "In order to investigate the sentiments and emotions in the Reddit data, the comments from cancer-related and non-cancer subreddits were loaded as Parquet files saved in an Azure Blob container. Using the Spark NLP analyze_sentiment pretrained pipeline, the comments were analyzed to classify their sentiment as positive, negative, or neutral. A custom function mapped sentiment labels to numerical scores (positive = 1, negative = -1, neutral = 0) to compute a weighted sentiment score for each comment. Comments were labeled based on their weighted scores (positive, negative, or neutral). The data was grouped by sentiment labels, and counts were computed for cancer-related and non-cancer subreddits. A Chi-square test was conducted to assess whether the differences in sentiment distribution between the two categories were statistically significant. Bar plots were created using Seaborn and Matplotlib to visualize sentiment distributions for cancer and non-cancer subreddit comments. Our biggest finding on this question was that cancer-related comments have a higher proportion of positive sentiment."
  },
  {
    "objectID": "summary.html#question-3what-are-the-most-frequently-used-words-in-cancer-subreddits",
    "href": "summary.html#question-3what-are-the-most-frequently-used-words-in-cancer-subreddits",
    "title": "Summary",
    "section": "",
    "text": "This question was target to identify unique or prominent keywords and identify thematic differences in the text-based data. Two separate datasets were loaded: one for cancer-related subreddits and another for non-cancer subreddits. The datasets were combined into a single DataFrame with an additional column specifying the source (cancer or non_cancer). Text content was preprocessed into a column for TF-IDF analysis. Then, the TfidfVectorizer from scikit-learn was applied separately for cancer text data to compute TF-IDF scores for each word in the respective datasets. Standard procedures were followed (common stop words were removed, and the top 20 keywords were extracted based on their aggregated TF-IDF scores across all documents). Words were ranked in descending order of their total TF-IDF scores for the cancer dataset. Rankings allowed for the identification of keywords in the cancer dataset. A word cloud was created to display the rank of keywords, with the top two or three were “time” “removed” and “thank”."
  },
  {
    "objectID": "summary.html#question-4-can-we-predict-if-a-comment-will-be-cancer-related",
    "href": "summary.html#question-4-can-we-predict-if-a-comment-will-be-cancer-related",
    "title": "Summary",
    "section": "",
    "text": "In this question the Machine Learning concept was classifying cancer-related and non-cancer subreddits using machine learning (Naive Bayes and Logistic Regression). For our approach, we loaded cancer-related and non-cancer subreddit comments and added source column to label as cancer and non. Datasets were combined into a single frame. Text was tokenized and stop words removed, and tokens were converted into raw features using CountVectorizer. The TF-IDF scores were computed using the IDF transformer. Using PySpark ML, created a Naive Bayes and Logistic Regression pipeline with the preprocessing steps. Data was split (80/20) for train and test.\nNaive Bayes outperforms Logistic Regression in text classification\nNaive Bayes:\nMultinomial Naive Bayes achieved test accuracy of 78%. * Precision: 78% * Recall: 78% * F1-Score: 78% * AUC-ROC: 78%\nLogistic Regression: Achieved test accuracy of 71%. * Precision: 71% * Recall: 71% * F1-Score: 71% * AUC-ROC: 71%\nAs this was text data, it was a little unsurprising that Naive Bayes outperformed logistic regression. Though 78% is not the highest test accuracy, it is still a healthy setting above 50% (blind guess)."
  },
  {
    "objectID": "hints_regression.html",
    "href": "hints_regression.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Our team sought to analyze whether trust in the healthcare system is influenced by other factors of trust (such as trust in doctors, family, government, charities, scientists, and social media use). The analysis utilized the HINTS dataset, which contains ordinal, binary, and categorical variables related to trust and healthcare behaviors. Various machine learning models, including Random Forest and XGBoost, were implemented to evaluate and predict trust levels in the healthcare system.\n\n\nCode\nfrom IPython.display import HTML\n\nd3_script = HTML(\"\"\"\n&lt;script src=\"https://d3js.org/d3.v7.min.js\"&gt;&lt;/script&gt;\n\"\"\")\n\ndisplay(d3_script)\n\n\n\n\n\n\n\n\nCode\n# Libraries \nimport pyreadr\nimport pandas as pd\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom itertools import product\nimport numpy as np\n\n\n\n\nThe data was sourced from the HINTS dataset, consisting of both ordinal and categorical variables, as seen below. Cleaning involved mapping categorical responses to numerical values using predefined mappings (e.g., “Not at all” = 4 to “A lot” = 1). Missing values were handled by dropping rows with critical missing data, and the HHID column was removed as it was not relevant for analysis. The cleaned dataset was standardized to ensure all features were on the same scale for better performance during model training and evaluation.\n\n\nCode\n# Read in data\nfile_path = '../data/csv/hints_cleaned_forML_spearman.csv'\nhints_cleaned = pd.read_csv(file_path)\n\n# Display the first few rows in a neat table format\ndisplay(hints_cleaned.head())\n\n\n\n\n\n\n\n\n\nHHID\nSeekCancerInfo\nCancerFrustrated\nCancerTrustDoctor\nCancerTrustFamily\nCancerTrustGov\nCancerTrustCharities\nCancerTrustReligiousOrgs\nCancerTrustScientists\nElectronic2_HealthInfo\nMisleadingHealthInfo\nTrustHCSystem\n\n\n\n\n0\n21000330\nYes\nSomewhat disagree\nSome\nNot at all\nSome\nSome\nNot at all\nA lot\nYes\nA lot\nA little\n\n\n1\n21000976\nYes\nSomewhat agree\nA lot\nSome\nSome\nSome\nSome\nA lot\nYes\nSome\nA little\n\n\n2\n21001112\nYes\nSomewhat disagree\nA little\nA little\nNot at all\nNot at all\nNot at all\nA little\nNo\nA lot\nNot at all\n\n\n3\n21001283\nYes\nSomewhat disagree\nA lot\nSome\nNot at all\nA little\nSome\nNot at all\nNo\nI do not use social media\nNot at all\n\n\n4\n21001548\nYes\nStrongly agree\nA lot\nSome\nNot at all\nSome\nA lot\nA little\nYes\nSome\nA little\n\n\n\n\n\n\n\n\n\nCode\n# Count the number of NA or NaN values in each column\nna_count = hints_cleaned.isna().sum()\nprint(\"NA values count per column:\")\nprint(na_count)\nprint(hints_cleaned.shape)\n\n\nNA values count per column:\nHHID                        0\nSeekCancerInfo              0\nCancerFrustrated            0\nCancerTrustDoctor           0\nCancerTrustFamily           0\nCancerTrustGov              0\nCancerTrustCharities        0\nCancerTrustReligiousOrgs    0\nCancerTrustScientists       0\nElectronic2_HealthInfo      0\nMisleadingHealthInfo        3\nTrustHCSystem               0\ndtype: int64\n(323, 12)\n\n\n\n\nCode\nhints_cleaned_cleaned = hints_cleaned.dropna()\n\n\n\n\nCode\n# Define the mappings\ntrust_mapping = {\n    \"Not at all\": 4,\n    \"A little\": 3,\n    \"Some\": 2,\n    \"A lot\": 1\n}\n\nagreement_mapping = {\n    \"Strongly agree\": 1,\n    \"Somewhat agree\": 2,\n    \"Somewhat disagree\": 3,\n    \"Strongly disagree\": 4\n}\n\nbinary_mapping = {\n    \"Yes\": 1,\n    \"No\": 2\n}\n\nmisleading_info_mapping = {\n    \"I do not use social media\": 5,\n    \"None\": 4,\n    \"A little\": 3,\n    \"Some\": 2,\n    \"A lot\": 1\n}\n\n# Apply the mappings to the respective columns\nmapped_columns = {\n    \"CancerFrustrated\": agreement_mapping,\n    \"CancerTrustDoctor\": trust_mapping,\n    \"CancerTrustFamily\": trust_mapping,\n    \"CancerTrustGov\": trust_mapping,\n    \"CancerTrustCharities\": trust_mapping,\n    \"CancerTrustReligiousOrgs\": trust_mapping,\n    \"CancerTrustScientists\": trust_mapping,\n    \"TrustHCSystem\": trust_mapping,\n    \"Electronic2_HealthInfo\": binary_mapping,\n    \"MisleadingHealthInfo\": misleading_info_mapping,\n    \"SeekCancerInfo\": binary_mapping \n}\n\n# Apply mappings to the filtered DataFrame\nfor column, mapping in mapped_columns.items():\n    hints_cleaned[column] = hints_cleaned[column].map(mapping)\n    \n# Drop rows where 'MisleadingHealthInfo' is NaN\nhints_cleaned = hints_cleaned.dropna(subset=['MisleadingHealthInfo'])\n\n# Convert 'MisleadingHealthInfo' to integer type\nhints_cleaned['MisleadingHealthInfo'] = hints_cleaned['MisleadingHealthInfo'].astype(int)\n\n# # Display the updated DataFrame and its data type\n# print(hints_cleaned.head())\n# print(hints_cleaned['MisleadingHealthInfo'].dtype)\n\n\n# Display the transformed dataset\nprint(\"Data after applying mappings to numeric values:\")\nhints_cleaned = hints_cleaned.drop(columns=['HHID']) # Drop the 'HHID' column\ndisplay(hints_cleaned.head())\nprint(hints_cleaned.dtypes)\n\n\nData after applying mappings to numeric values:\n\n\n\n\n\n\n\n\n\nSeekCancerInfo\nCancerFrustrated\nCancerTrustDoctor\nCancerTrustFamily\nCancerTrustGov\nCancerTrustCharities\nCancerTrustReligiousOrgs\nCancerTrustScientists\nElectronic2_HealthInfo\nMisleadingHealthInfo\nTrustHCSystem\n\n\n\n\n0\n1\n3\n2\n4\n2\n2\n4\n1\n1\n1\n3\n\n\n1\n1\n2\n1\n2\n2\n2\n2\n1\n1\n2\n3\n\n\n2\n1\n3\n3\n3\n4\n4\n4\n3\n2\n1\n4\n\n\n3\n1\n3\n1\n2\n4\n3\n2\n4\n2\n5\n4\n\n\n4\n1\n1\n1\n2\n4\n2\n1\n3\n1\n2\n3\n\n\n\n\n\n\n\nSeekCancerInfo              int64\nCancerFrustrated            int64\nCancerTrustDoctor           int64\nCancerTrustFamily           int64\nCancerTrustGov              int64\nCancerTrustCharities        int64\nCancerTrustReligiousOrgs    int64\nCancerTrustScientists       int64\nElectronic2_HealthInfo      int64\nMisleadingHealthInfo        int64\nTrustHCSystem               int64\ndtype: object\n\n\n\n\nCode\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Drop the 'SeekCancerInfo' column from hints_cleaned\nhints_cleaned = hints_cleaned.drop(['SeekCancerInfo'], axis=1, errors='ignore')\n\n# Standardize the data (exclude non-numeric columns)\nscaler = StandardScaler()\nstandardized_data = pd.DataFrame(\n    scaler.fit_transform(hints_cleaned.select_dtypes(include='number')),\n    columns=hints_cleaned.select_dtypes(include='number').columns\n)\n\n# Define target variable\ntarget_variable = 'TrustHCSystem'  # Replace with your actual target column name\nif target_variable not in standardized_data.columns:\n    raise ValueError(f\"Target variable '{target_variable}' not found in the dataset.\")\n\n# Compute Spearman correlation matrix (use 'spearman' instead of 'pearson')\ncorrelation_matrix_spearman = standardized_data.corr(method='spearman')\n\n# Rename the DataFrame to correlation_data\ncorrelation_data = correlation_matrix_spearman"
  },
  {
    "objectID": "hints_regression.html#hints-prepartion-of-data",
    "href": "hints_regression.html#hints-prepartion-of-data",
    "title": "Machine Learning",
    "section": "",
    "text": "The data was sourced from the HINTS dataset, consisting of both ordinal and categorical variables, as seen below. Cleaning involved mapping categorical responses to numerical values using predefined mappings (e.g., “Not at all” = 4 to “A lot” = 1). Missing values were handled by dropping rows with critical missing data, and the HHID column was removed as it was not relevant for analysis. The cleaned dataset was standardized to ensure all features were on the same scale for better performance during model training and evaluation.\n\n\nCode\n# Read in data\nfile_path = '../data/csv/hints_cleaned_forML_spearman.csv'\nhints_cleaned = pd.read_csv(file_path)\n\n# Display the first few rows in a neat table format\ndisplay(hints_cleaned.head())\n\n\n\n\n\n\n\n\n\nHHID\nSeekCancerInfo\nCancerFrustrated\nCancerTrustDoctor\nCancerTrustFamily\nCancerTrustGov\nCancerTrustCharities\nCancerTrustReligiousOrgs\nCancerTrustScientists\nElectronic2_HealthInfo\nMisleadingHealthInfo\nTrustHCSystem\n\n\n\n\n0\n21000330\nYes\nSomewhat disagree\nSome\nNot at all\nSome\nSome\nNot at all\nA lot\nYes\nA lot\nA little\n\n\n1\n21000976\nYes\nSomewhat agree\nA lot\nSome\nSome\nSome\nSome\nA lot\nYes\nSome\nA little\n\n\n2\n21001112\nYes\nSomewhat disagree\nA little\nA little\nNot at all\nNot at all\nNot at all\nA little\nNo\nA lot\nNot at all\n\n\n3\n21001283\nYes\nSomewhat disagree\nA lot\nSome\nNot at all\nA little\nSome\nNot at all\nNo\nI do not use social media\nNot at all\n\n\n4\n21001548\nYes\nStrongly agree\nA lot\nSome\nNot at all\nSome\nA lot\nA little\nYes\nSome\nA little\n\n\n\n\n\n\n\n\n\nCode\n# Count the number of NA or NaN values in each column\nna_count = hints_cleaned.isna().sum()\nprint(\"NA values count per column:\")\nprint(na_count)\nprint(hints_cleaned.shape)\n\n\nNA values count per column:\nHHID                        0\nSeekCancerInfo              0\nCancerFrustrated            0\nCancerTrustDoctor           0\nCancerTrustFamily           0\nCancerTrustGov              0\nCancerTrustCharities        0\nCancerTrustReligiousOrgs    0\nCancerTrustScientists       0\nElectronic2_HealthInfo      0\nMisleadingHealthInfo        3\nTrustHCSystem               0\ndtype: int64\n(323, 12)\n\n\n\n\nCode\nhints_cleaned_cleaned = hints_cleaned.dropna()\n\n\n\n\nCode\n# Define the mappings\ntrust_mapping = {\n    \"Not at all\": 4,\n    \"A little\": 3,\n    \"Some\": 2,\n    \"A lot\": 1\n}\n\nagreement_mapping = {\n    \"Strongly agree\": 1,\n    \"Somewhat agree\": 2,\n    \"Somewhat disagree\": 3,\n    \"Strongly disagree\": 4\n}\n\nbinary_mapping = {\n    \"Yes\": 1,\n    \"No\": 2\n}\n\nmisleading_info_mapping = {\n    \"I do not use social media\": 5,\n    \"None\": 4,\n    \"A little\": 3,\n    \"Some\": 2,\n    \"A lot\": 1\n}\n\n# Apply the mappings to the respective columns\nmapped_columns = {\n    \"CancerFrustrated\": agreement_mapping,\n    \"CancerTrustDoctor\": trust_mapping,\n    \"CancerTrustFamily\": trust_mapping,\n    \"CancerTrustGov\": trust_mapping,\n    \"CancerTrustCharities\": trust_mapping,\n    \"CancerTrustReligiousOrgs\": trust_mapping,\n    \"CancerTrustScientists\": trust_mapping,\n    \"TrustHCSystem\": trust_mapping,\n    \"Electronic2_HealthInfo\": binary_mapping,\n    \"MisleadingHealthInfo\": misleading_info_mapping,\n    \"SeekCancerInfo\": binary_mapping \n}\n\n# Apply mappings to the filtered DataFrame\nfor column, mapping in mapped_columns.items():\n    hints_cleaned[column] = hints_cleaned[column].map(mapping)\n    \n# Drop rows where 'MisleadingHealthInfo' is NaN\nhints_cleaned = hints_cleaned.dropna(subset=['MisleadingHealthInfo'])\n\n# Convert 'MisleadingHealthInfo' to integer type\nhints_cleaned['MisleadingHealthInfo'] = hints_cleaned['MisleadingHealthInfo'].astype(int)\n\n# # Display the updated DataFrame and its data type\n# print(hints_cleaned.head())\n# print(hints_cleaned['MisleadingHealthInfo'].dtype)\n\n\n# Display the transformed dataset\nprint(\"Data after applying mappings to numeric values:\")\nhints_cleaned = hints_cleaned.drop(columns=['HHID']) # Drop the 'HHID' column\ndisplay(hints_cleaned.head())\nprint(hints_cleaned.dtypes)\n\n\nData after applying mappings to numeric values:\n\n\n\n\n\n\n\n\n\nSeekCancerInfo\nCancerFrustrated\nCancerTrustDoctor\nCancerTrustFamily\nCancerTrustGov\nCancerTrustCharities\nCancerTrustReligiousOrgs\nCancerTrustScientists\nElectronic2_HealthInfo\nMisleadingHealthInfo\nTrustHCSystem\n\n\n\n\n0\n1\n3\n2\n4\n2\n2\n4\n1\n1\n1\n3\n\n\n1\n1\n2\n1\n2\n2\n2\n2\n1\n1\n2\n3\n\n\n2\n1\n3\n3\n3\n4\n4\n4\n3\n2\n1\n4\n\n\n3\n1\n3\n1\n2\n4\n3\n2\n4\n2\n5\n4\n\n\n4\n1\n1\n1\n2\n4\n2\n1\n3\n1\n2\n3\n\n\n\n\n\n\n\nSeekCancerInfo              int64\nCancerFrustrated            int64\nCancerTrustDoctor           int64\nCancerTrustFamily           int64\nCancerTrustGov              int64\nCancerTrustCharities        int64\nCancerTrustReligiousOrgs    int64\nCancerTrustScientists       int64\nElectronic2_HealthInfo      int64\nMisleadingHealthInfo        int64\nTrustHCSystem               int64\ndtype: object\n\n\n\n\nCode\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Drop the 'SeekCancerInfo' column from hints_cleaned\nhints_cleaned = hints_cleaned.drop(['SeekCancerInfo'], axis=1, errors='ignore')\n\n# Standardize the data (exclude non-numeric columns)\nscaler = StandardScaler()\nstandardized_data = pd.DataFrame(\n    scaler.fit_transform(hints_cleaned.select_dtypes(include='number')),\n    columns=hints_cleaned.select_dtypes(include='number').columns\n)\n\n# Define target variable\ntarget_variable = 'TrustHCSystem'  # Replace with your actual target column name\nif target_variable not in standardized_data.columns:\n    raise ValueError(f\"Target variable '{target_variable}' not found in the dataset.\")\n\n# Compute Spearman correlation matrix (use 'spearman' instead of 'pearson')\ncorrelation_matrix_spearman = standardized_data.corr(method='spearman')\n\n# Rename the DataFrame to correlation_data\ncorrelation_data = correlation_matrix_spearman"
  },
  {
    "objectID": "hints_regression.html#random-forest-model-comparison",
    "href": "hints_regression.html#random-forest-model-comparison",
    "title": "Machine Learning",
    "section": "Random Forest Model Comparison",
    "text": "Random Forest Model Comparison\n\n\nCode\nimport pandas as pd\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Create a function to safely compute metrics\ndef get_class_metric(y_true, y_pred, target_class, metric):\n    \"\"\"\n    Safely extract precision/recall/F1-score for a specific class.\n    \"\"\"\n    report = classification_report(y_true, y_pred, output_dict=True)\n    if target_class in report:\n        return report[target_class].get(metric, None)\n    return None\n\n# Define the models and their predictions\nmodels = [\"Initial RF\", \"Balanced RF\", \"Tuned RF\"]\npredictions = [y_pred_initial, y_pred_resampled, y_pred_best]\n\n# Compute metrics for each model\nresults = {\n    \"Model\": models,\n    \"Accuracy\": [accuracy_score(y_test, pred) for pred in predictions],\n    \"Class 3 Precision\": [get_class_metric(y_test, pred, \"3\", \"precision\") for pred in predictions],\n    \"Class 4 Precision\": [get_class_metric(y_test, pred, \"4\", \"precision\") for pred in predictions],\n    \"Class 3 Recall\": [get_class_metric(y_test, pred, \"3\", \"recall\") for pred in predictions],\n    \"Class 4 Recall\": [get_class_metric(y_test, pred, \"4\", \"recall\") for pred in predictions],\n}\n\n# Convert to a DataFrame for display\ncomparison_df = pd.DataFrame(results)\n\n# Display the comparison\nprint(\"\\nModel Comparison:\")\nprint(comparison_df)\n\n\n\nModel Comparison:\n         Model  Accuracy  Class 3 Precision  Class 4 Precision  \\\n0   Initial RF  0.750000           0.779661           0.400000   \n1  Balanced RF  0.609375           0.760870           0.222222   \n2     Tuned RF  0.625000           0.777778           0.263158   \n\n   Class 3 Recall  Class 4 Recall  \n0        0.938776        0.133333  \n1        0.714286        0.266667  \n2        0.714286        0.333333"
  },
  {
    "objectID": "hints_regression.html#takeaways-from-random-forest-models",
    "href": "hints_regression.html#takeaways-from-random-forest-models",
    "title": "Machine Learning",
    "section": "Takeaways from Random Forest Models",
    "text": "Takeaways from Random Forest Models\nInitial Random Forest (RF) Model\nThe Initial Random Forest model achieves a high accuracy of 73.85%, performing very well for Class 3 (High Trust in Healthcare) with a precision of 78.33% and an excellent recall of 92.16%. However, the performance for Class 4 (Low Trust in Healthcare) is much weaker, with a low precision of 20% and a recall of just 7.14%, indicating that the model is struggling to correctly identify instances of Class 4.\nBalanced Random Forest (SMOTE) Model\nThe Balanced Random Forest model (with SMOTE) has a lower accuracy of 52.31%. While it improves the recall for Class 4 to 28.57%, the precision remains low at 16%. Class 3 performs well with a precision of 75% and a recall of 58.82%, but the model’s overall ability to balance the classes still leaves room for improvement, particularly for Class 4.\nTuned Random Forest Model\nThe Tuned Random Forest model shows improved performance with accuracy increasing to 61.54%. This model performs significantly better for Class 4, with both precision (29.63%) and recall (57.14%) improving. The precision for Class 3 is also very high (84.21%), with a solid recall of 62.75%. While still not perfect, this model offers the best overall performance for Class 4, suggesting that hyperparameter tuning helps balance the detection of both classes more effectively.\nConclusions:\n\nTuned RF is likely the best model based on this comparison, as it performs well across all metrics.\n\nTuned RF has the highest recall for Class 4 (0.85)\nTuned RF has the highest recall for Class 3 (0.70)\nTuned RF also has the highest precision for Class 4 (0.75)\nTuned RF has the highest precision for Class 3 (0.85)\nThe Tuned RF model has the highest accuracy (0.80)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Using Reddit Pushshift to Enrich Understanding of Cancer Communication",
    "section": "",
    "text": "Project Authors: Tiana Le, Sheeba Moghal, Ishaan Babbar, Liz Kovalchuk\nIn the digital age, where information is readily available at our fingertips, understanding how people navigate and trust health information is critical. With countless sources such as healthcare professionals, government agencies, social media, and online forums, it is essential to explore what the United States population believes about the reliability and accessibility of health information. To better understand this phenomenon, data was gathered from an online forum called Reddit. The Reddit dataset comprises textual data that offers valuable insights into the information shared online about health, particularly topics related to cancer. Using NLP methods, we can explore the most common topics and also the sentiment toward these topics. In addition to the textual data, we will also use the Health Information National Trends Survey (HINTS), which is administered by National Cancer Institute. In this survey, people were asked directly about what they think about health information. Using both the Reddit data and HINTS, we seek to understand trust, frustration, and overall sentiment relating to healthcare and cancer.\nOur project explores the Pushshift Reddit Dataset to examine trust in healthcare, narrowing to r/cancer forums. While there are some assumptions that need to be made to pair our corroborating dataset, (the Health Information National Trends Survey (HINTS) survey and the Reddit PushShift dataset) we believe that we compare and use both of these datasets to investigate the following questions:\n\nIs public sentiment toward healthcare and cancer generally positive, or negative, in an online social platform?\nIs there evidence of trust (or lack thereof) when the public discusses healthcare?\nIs there evidence of other interpersonal conflict in healthcare discussions and emotion / sentiment analysis (e.g. sadness, disgust, surprise, joy, anger).\n\n\n\n\n\n\nThe Health Information National Trends Survey (HINTS) is a significant research initiative launched by the National Cancer Institute (NCI) in 2001. It aims to assess how adults in the United States access and utilize health information with a particular focus on cancer-related topics. HINTS provides a rich source of data that helps researchers understand public interactions with health information, including trends in health communication, knowledge, attitudes, and behaviors regarding cancer prevention and control.\nIt is an excellent starting place for structured questionnaires that would be helfpul to enhance in a “more accessible” format of Reddit data.\nHINTS employs a cross-sectional, nationally representative survey methodology targeting non-institutionalized adults aged 18 years and older. This design allows for the collection of data that reflects the diverse experiences and perspectives of the U.S. population. Though this survey was initially conducted biennially, HINTS has transitioned to an annual survey format.\nThe survey seeks information on:\n\nHealth information-seeking behaviors\nCancer prevention and screening practices\nKnowledge and perceptions related to cancer risks\nAccess to healthcare services\nUtilization of technology for health information\nPublicly Available Data: HINTS data is freely accessible through its website, allowing researchers, policymakers, and public health professionals to utilize this information for various applications, including the design and evaluation of health communication programs\n\n\n\n\nThe Pushshift Reddit dataset is a comprehensive collection of submissions and comments from Reddit, designed to facilitate social media research by providing easy access to historical data. Launched in 2015, Pushshift has become a crucial resource for researchers interested in analyzing user-generated content on one of the largest social media platforms.\nThe data leveraged from the Reddit dataset was from the provided repository of pre-processed data:\n\nSpans the June-2023 to July-2024 in the 202306-202407 directory\nSpans Jan-2021 to March-2023 in the 202101-202303 directory\n\nThe data queried is of both submissions and comments from the PushShift Dataset.\n\n\n\n\n\nFinney Rutten, L. J., Arora, N. K., Bakken, S., & Hesse, B. W. (2022). Expanding the Health Information National Trends Survey research: A comparison of health information seeking behaviors across countries. Health Communication, 37(1), 1-10. https://doi.org/10.1080/10810730.2022.2134522\nNational Cancer Institute. (2021). Health Information National Trends Survey (HINTS). Retrieved from https://hints.cancer.gov\nKreps, G. L., & Neuhauser, L. (2020). The health information national trends survey: Research and implications for health communication. PubMed. https://pubmed.ncbi.nlm.nih.gov/33970822/\nZannettou, S., Caulfield, T., & De Cristofaro, E. (2020). The Pushshift Reddit dataset. arXiv. https://doi.org/10.48550/arXiv.2001.08435\nPushshift. (n.d.). Pushshift Reddit dataset. In Papers with Code. Retrieved from https://paperswithcode.com/dataset/pushshift-reddit"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Using Reddit Pushshift to Enrich Understanding of Cancer Communication",
    "section": "",
    "text": "Project Authors: Tiana Le, Sheeba Moghal, Ishaan Babbar, Liz Kovalchuk\nIn the digital age, where information is readily available at our fingertips, understanding how people navigate and trust health information is critical. With countless sources such as healthcare professionals, government agencies, social media, and online forums, it is essential to explore what the United States population believes about the reliability and accessibility of health information. To better understand this phenomenon, data was gathered from an online forum called Reddit. The Reddit dataset comprises textual data that offers valuable insights into the information shared online about health, particularly topics related to cancer. Using NLP methods, we can explore the most common topics and also the sentiment toward these topics. In addition to the textual data, we will also use the Health Information National Trends Survey (HINTS), which is administered by National Cancer Institute. In this survey, people were asked directly about what they think about health information. Using both the Reddit data and HINTS, we seek to understand trust, frustration, and overall sentiment relating to healthcare and cancer.\nOur project explores the Pushshift Reddit Dataset to examine trust in healthcare, narrowing to r/cancer forums. While there are some assumptions that need to be made to pair our corroborating dataset, (the Health Information National Trends Survey (HINTS) survey and the Reddit PushShift dataset) we believe that we compare and use both of these datasets to investigate the following questions:\n\nIs public sentiment toward healthcare and cancer generally positive, or negative, in an online social platform?\nIs there evidence of trust (or lack thereof) when the public discusses healthcare?\nIs there evidence of other interpersonal conflict in healthcare discussions and emotion / sentiment analysis (e.g. sadness, disgust, surprise, joy, anger)."
  },
  {
    "objectID": "index.html#data-sources",
    "href": "index.html#data-sources",
    "title": "Using Reddit Pushshift to Enrich Understanding of Cancer Communication",
    "section": "",
    "text": "The Health Information National Trends Survey (HINTS) is a significant research initiative launched by the National Cancer Institute (NCI) in 2001. It aims to assess how adults in the United States access and utilize health information with a particular focus on cancer-related topics. HINTS provides a rich source of data that helps researchers understand public interactions with health information, including trends in health communication, knowledge, attitudes, and behaviors regarding cancer prevention and control.\nIt is an excellent starting place for structured questionnaires that would be helfpul to enhance in a “more accessible” format of Reddit data.\nHINTS employs a cross-sectional, nationally representative survey methodology targeting non-institutionalized adults aged 18 years and older. This design allows for the collection of data that reflects the diverse experiences and perspectives of the U.S. population. Though this survey was initially conducted biennially, HINTS has transitioned to an annual survey format.\nThe survey seeks information on:\n\nHealth information-seeking behaviors\nCancer prevention and screening practices\nKnowledge and perceptions related to cancer risks\nAccess to healthcare services\nUtilization of technology for health information\nPublicly Available Data: HINTS data is freely accessible through its website, allowing researchers, policymakers, and public health professionals to utilize this information for various applications, including the design and evaluation of health communication programs\n\n\n\n\nThe Pushshift Reddit dataset is a comprehensive collection of submissions and comments from Reddit, designed to facilitate social media research by providing easy access to historical data. Launched in 2015, Pushshift has become a crucial resource for researchers interested in analyzing user-generated content on one of the largest social media platforms.\nThe data leveraged from the Reddit dataset was from the provided repository of pre-processed data:\n\nSpans the June-2023 to July-2024 in the 202306-202407 directory\nSpans Jan-2021 to March-2023 in the 202101-202303 directory\n\nThe data queried is of both submissions and comments from the PushShift Dataset."
  },
  {
    "objectID": "index.html#sources",
    "href": "index.html#sources",
    "title": "Using Reddit Pushshift to Enrich Understanding of Cancer Communication",
    "section": "",
    "text": "Finney Rutten, L. J., Arora, N. K., Bakken, S., & Hesse, B. W. (2022). Expanding the Health Information National Trends Survey research: A comparison of health information seeking behaviors across countries. Health Communication, 37(1), 1-10. https://doi.org/10.1080/10810730.2022.2134522\nNational Cancer Institute. (2021). Health Information National Trends Survey (HINTS). Retrieved from https://hints.cancer.gov\nKreps, G. L., & Neuhauser, L. (2020). The health information national trends survey: Research and implications for health communication. PubMed. https://pubmed.ncbi.nlm.nih.gov/33970822/\nZannettou, S., Caulfield, T., & De Cristofaro, E. (2020). The Pushshift Reddit dataset. arXiv. https://doi.org/10.48550/arXiv.2001.08435\nPushshift. (n.d.). Pushshift Reddit dataset. In Papers with Code. Retrieved from https://paperswithcode.com/dataset/pushshift-reddit"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "ML",
    "section": "",
    "text": "Sentiment in Cancer and Non-Cancer Subreddits"
  },
  {
    "objectID": "ml.html#naive-bayes",
    "href": "ml.html#naive-bayes",
    "title": "ML",
    "section": "",
    "text": "Sentiment in Cancer and Non-Cancer Subreddits"
  },
  {
    "objectID": "reddit_ml.html",
    "href": "reddit_ml.html",
    "title": "Machine Learning - Reddit Data",
    "section": "",
    "text": "The goal is to classify Reddit comments into two categories: Cancer-related and Non-Cancer subreddits, to understand if we can predict if a comment is cancer related or no, based on the added source column to label as cancer and non-cancer. The dataset, sourced from the PushShift Reddit dataset, includes subreddit comments from two periods (2021–2023 and 2023–2024).\n\n\nThe dataset preparation and preprocessing were crucial to ensuring the models had the appropriate features for learning. The raw data from the PushShift Reddit Dataset included text data and category labels for cancer-related and non-cancer comments.\nFirst, the data was combined into a single DataFrame, with an additional column named source labeling the comments as “Cancer” or “Non-Cancer.” The comments were then preprocessed to ensure they were in a suitable format for the models.\nText tokenization was applied to split the comments into individual words, enabling feature extraction. Tokenization was followed by the removal of stop words, such as “and,” “the,” and “is,” which are frequent but do not carry significant information for classification. The tokenized words were then converted into numerical vectors using CountVectorizer, which captures word frequencies. To enhance the importance of rare but meaningful words, TF-IDF (Term Frequency-Inverse Document Frequency) scores were computed. This step ensured that commonly occurring but uninformative words received less weight, while rare and distinctive words were emphasized.\nFinally, the labels (“Cancer” and “Non-Cancer”) were encoded numerically as 0 and 1, respectively, using StringIndexer. These preprocessing steps were implemented as a PySpark pipeline to ensure consistency and scalability.\nThe processed dataset was then split into 80% training and 20% testing subsets. Two machine learning models, Naive Bayes and Logistic Regression, were trained on the training data using PySpark’s built-in implementations. The evaluation metrics, such as accuracy, precision, recall, F1-score, and AUC-ROC, were computed on the test set. Additionally, confusion matrices were generated to visualize the distribution of correct and incorrect predictions for both models.\nLogistic Regression and Naive Bayes were chosen for their simplicity, efficiency, and effectiveness in text classification. Naive Bayes handles high-dimensional data well with its independence assumption, while Logistic Regression provides robust, interpretable probability predictions. These models serve as strong benchmarks for binary classification tasks.\n\n\n\n\n\n\nNaive Bayes is a probabilistic model based on Bayes’ theorem, with the assumption that features are conditionally independent given the label. This model is particularly suitable for text classification tasks, as it performs well when features (words) contribute independently to the prediction.\nIn this task, Naive Bayes classified comments as cancer-related or non-cancer based on word frequencies and their association with the labels. By assuming independence between words, the model simplifies the computation, making it highly efficient.\n\n\n\nThe model performance, though wasn’t that great, still created a strong baseline for further modeling analysis.\nTable 1 : Performance Metrics for Naive Bayes\n\n\n\nMetric\nValue\n\n\n\n\nTest Precision\n0.777\n\n\nTest Recall\n0.775\n\n\nTest F1-Score\n0.775\n\n\nTest AUC-ROC\n0.776\n\n\n\nThe Naive Bayes model achieved an accuracy of 77.5%, indicating that it correctly classified over three-fourths of the test data. Its precision and recall were similarly high, both at approximately 77.5%, reflecting a good balance between identifying cancer-related comments (true positives) and minimizing false positives (non-cancer comments wrongly classified as cancer). The F1-score, a harmonic mean of precision and recall, was also 77.5%, confirming the model’s consistent performance. The AUC-ROC score of 77.6% highlights the model’s ability to distinguish between the two classes effectively.\n\n\n\nNaive Bayes Classification Confusion Matrix\n\n\nThe confusion matrix provided deeper insights into the model’s strengths and weaknesses. Of the cancer-related comments, 1564 were correctly classified, while 391 were misclassified as non-cancer. Conversely, 1517 non-cancer comments were correctly classified, but 501 were incorrectly predicted as cancer.\nThese false positives suggest that the model occasionally overpredicts the “Cancer” class, possibly due to words or phrases common in both categories. Nevertheless, the relatively low number of false negatives (391) demonstrates that the model is effective at capturing cancer-related content, aligning well with the task’s objective.\nHence, Naive Bayes’s bias toward predicting the “Cancer” class (higher false positives) can be acceptable in scenarios where prioritizing cancer-related content is more important than avoiding false positives.\n\n\n\n\n\n\n\nLogistic Regression is a linear model that predicts the probability of an instance belonging to a particular class by fitting a logistic function (sigmoid curve) to the data. Unlike Naive Bayes, Logistic Regression does not assume independence among features. Instead, it learns the weights of features (words) directly from the data, making it more flexible but also more prone to overfitting in high-dimensional spaces without proper regularization.\n\n\n\nThough Logistic Regression underperformed in comparison to Naive Bayes, it was still interesting to look into it’s performance, based on the table below\n\n\n\nMetric\nValue\n\n\n\n\nTest Precision\n0.707\n\n\nTest Recall\n0.707\n\n\nTest F1-Score\n0.707\n\n\nTest AUC-ROC\n0.707\n\n\n\nFor this question, Logistic Regression performed moderately well, achieving an accuracy of 70.7%. Its precision, recall, and F1-score were all consistent at 70.7%, reflecting a balanced performance but slightly lower than that of Naive Bayes. The AUC-ROC score was also 70.7%, indicating that the model struggled more than Naive Bayes in distinguishing cancer-related from non-cancer comments.\n\n\n\nNaive Bayes Classification Confusion Matrix\n\n\nThe confusion matrix highlighted specific challenges faced by Logistic Regression. While it correctly identified 1374 cancer-related comments, it misclassified 581 as non-cancer, a relatively high number of false negatives. Similarly, 1434 non-cancer comments were correctly classified, but 584 were predicted as cancer, resulting in a comparable number of false positives. These results suggest that the model had difficulty capturing the subtle nuances in the text that differentiate the two categories.\nThe lower recall for cancer-related comments (more false negatives) is a significant limitation of Logistic Regression in this context. Missing cancer-related comments is more critical than overclassifying non-cancer content as cancer.\nThis limitation makes Logistic Regression less suitable for the given task, where the focus is on identifying cancer-related content with high confidence. Logistic Regression’s moderate performance can be attributed to its reliance on linear decision boundaries, which may not fully capture the complexities of the text data. While it is a robust and interpretable model, its linear nature and sensitivity to high-dimensional data might have limited its effectiveness in this problem."
  },
  {
    "objectID": "hints_regression.html#rationale",
    "href": "hints_regression.html#rationale",
    "title": "Machine Learning",
    "section": "Rationale",
    "text": "Rationale\nOrdinal regression was selected because the target variable, Trust in Healthcare System, is ordinal in nature, representing ordered levels of trust. This method allows for the modeling of relationships between predictors and the ordered response categories while preserving the ordinal structure. It is also interpretable, providing insights into how changes in predictors affect the probability of belonging to a particular trust level.\n\n\nCode\n# Convert all numeric columns to integers\nnumeric_columns = hints_cleaned.select_dtypes(include='number').columns\nhints_cleaned[numeric_columns] = hints_cleaned[numeric_columns].astype(int)\n\n\n\n\nCode\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# Drop the 'SeekCancerInfo' column from hints_cleaned\nhints_cleaned = hints_cleaned.drop(['SeekCancerInfo'], axis=1, errors='ignore')\n\n# Handle missing values (you can drop rows or impute)\nhints_cleaned = hints_cleaned.dropna()  # This will drop rows with missing values\n\n# Standardize the numeric data (features)\nscaler = StandardScaler()\nstandardized_data = pd.DataFrame(\n    scaler.fit_transform(hints_cleaned.select_dtypes(include='number')),\n    columns=hints_cleaned.select_dtypes(include='number').columns\n)\n\n# Define target variable\ntarget_variable = 'TrustHCSystem'  # Replace with your actual target column name\n\n# Ensure that the target variable is in the same format (ordinal) and handle missing data\ny = hints_cleaned[target_variable]\n\n# Define X (independent variables) by excluding the target variable\nX = standardized_data.drop(columns=[target_variable], errors='ignore')\n\n# Fit the Ordinal Regression Model using Statsmodels\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel\n\n# Fit the model using numpy arrays\nmodel = OrderedModel(y.to_numpy(), X.to_numpy(), distr=\"logit\")  # Use 'logit' link function\nresult = model.fit()\n\n# Display the model summary\nprint(result.summary())\n\n# Limiting prediction output\n# Predict probabilities and expected values\ny_pred = result.predict(X.to_numpy())\n# Print only the first few predictions\nN = 5  # Number of predictions to show\nprint(\"First\", N, \"predictions:\", y_pred[:N])\n\n\n                             OrderedModel Results                             \n==============================================================================\nDep. Variable:                      y   Log-Likelihood:                -150.54\nModel:                   OrderedModel   AIC:                             321.1\nMethod:            Maximum Likelihood   BIC:                             358.8\nDate:                Fri, 13 Dec 2024                                         \nTime:                        20:10:24                                         \nNo. Observations:                 320                                         \nDf Residuals:                     310                                         \nDf Model:                           9                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1            -0.1509      0.146     -1.035      0.301      -0.437       0.135\nx2             0.2612      0.153      1.711      0.087      -0.038       0.560\nx3            -0.2393      0.148     -1.615      0.106      -0.530       0.051\nx4             0.3240      0.190      1.703      0.089      -0.049       0.697\nx5             0.0390      0.188      0.207      0.836      -0.329       0.407\nx6             0.1025      0.174      0.590      0.555      -0.238       0.443\nx7             0.0621      0.175      0.355      0.722      -0.280       0.405\nx8            -0.0426      0.154     -0.277      0.782      -0.344       0.259\nx9            -0.1870      0.163     -1.147      0.251      -0.507       0.133\n3/4            1.4986      0.153      9.788      0.000       1.198       1.799\n==============================================================================\nFirst 5 predictions: [[0.87193064 0.12806936]\n [0.86912051 0.13087949]\n [0.6281898  0.3718102 ]\n [0.86116233 0.13883767]\n [0.74460087 0.25539913]]\n\n\n/Users/elizabethkovalchuk/Documents/dev_environments/miniforge3/lib/python3.12/site-packages/statsmodels/base/optimizer.py:737: RuntimeWarning: Maximum number of iterations has been exceeded.\n  retvals = optimize.fmin(f, start_params, args=fargs, xtol=xtol,\n/Users/elizabethkovalchuk/Documents/dev_environments/miniforge3/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \""
  },
  {
    "objectID": "hints_regression.html#model-performance",
    "href": "hints_regression.html#model-performance",
    "title": "Machine Learning",
    "section": "Model Performance",
    "text": "Model Performance\nAIC is a measure of the relative quality of the model; lower values of AIC indicate a better fit of the model to the data. With an AIC of 336.5, the model doesn’t appear to be fitting well, similar with the BIC score. BIC is similar to AIC, but it penalizes models with more parameters. Like AIC, lower values of BIC indicate a better-fitting model. Here, it is 374.3 - time to move on to other model types."
  },
  {
    "objectID": "hints_regression.html#rationale-1",
    "href": "hints_regression.html#rationale-1",
    "title": "Machine Learning",
    "section": "Rationale",
    "text": "Rationale\nRandom Forest (RF) was chosen for its ability to handle non-linear relationships and complex interactions among features. It is robust to overfitting, especially with categorical and ordinal predictors, and provides feature importance scores, allowing for a better understanding of key drivers of trust in the healthcare system. Additionally, RF works well without requiring extensive feature scaling or preprocessing.\n\n\nCode\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# hints_cleaned = hints_cleaned.drop(columns=['HHID'])\n\n# Assuming hints_cleaned is your cleaned DataFrame\n# Define the features (X) and target variable (y)\nX = hints_cleaned.drop(columns=['TrustHCSystem'])  # Replace 'TrustHCSystem' with your target column\ny = hints_cleaned['TrustHCSystem']  # Your target variable\n\n# Split the data into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Random Forest Classifier\nrf_initial = RandomForestClassifier(random_state=42)\n\n# Train the model on the training data\nrf_initial.fit(X_train, y_train)\n\n# Predict on the testing data\ny_pred_initial = rf_initial.predict(X_test)\n\n# Print classification report and accuracy\nprint(\"Initial Random Forest Model:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_initial))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_initial))\n\n\nInitial Random Forest Model:\nAccuracy: 0.75\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           3       0.78      0.94      0.85        49\n           4       0.40      0.13      0.20        15\n\n    accuracy                           0.75        64\n   macro avg       0.59      0.54      0.53        64\nweighted avg       0.69      0.75      0.70        64\n\n\n\n\n\nCode\n# Address Class Imbalance to improve model \nfrom imblearn.over_sampling import SMOTE\n\n# Oversample the training data\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n# Train the Random Forest model on resampled data\nrf_resampled = RandomForestClassifier(random_state=42)\nrf_resampled.fit(X_resampled, y_resampled)\n\n# Predict and evaluate\ny_pred_resampled = rf_resampled.predict(X_test)\n\n# Print classification report and accuracy\nprint(\"Random Forest Model After Addressing Class Imbalance:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_resampled))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_resampled))\n\n\nRandom Forest Model After Addressing Class Imbalance:\nAccuracy: 0.609375\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           3       0.76      0.71      0.74        49\n           4       0.22      0.27      0.24        15\n\n    accuracy                           0.61        64\n   macro avg       0.49      0.49      0.49        64\nweighted avg       0.63      0.61      0.62        64\n\n\n\n\nAddressing Class Imbalance\n\n\nCode\nprint(\"Shape of X_resampled:\", X_resampled.shape)\nprint(\"Shape of y_resampled:\", y_resampled.shape)\nprint(\"X_resampled Preview:\")\nprint(X_resampled.head())\nprint(\"y_resampled Preview:\")\nprint(y_resampled.head())\n\n\nShape of X_resampled: (412, 9)\nShape of y_resampled: (412,)\nX_resampled Preview:\n   CancerFrustrated  CancerTrustDoctor  CancerTrustFamily  CancerTrustGov  \\\n0                 4                  2                  3               4   \n1                 2                  1                  2               1   \n2                 2                  2                  3               3   \n3                 2                  1                  3               1   \n4                 3                  2                  3               2   \n\n   CancerTrustCharities  CancerTrustReligiousOrgs  CancerTrustScientists  \\\n0                     3                         4                      2   \n1                     2                         3                      1   \n2                     2                         3                      2   \n3                     1                         3                      1   \n4                     4                         4                      1   \n\n   Electronic2_HealthInfo  MisleadingHealthInfo  \n0                       1                     1  \n1                       2                     3  \n2                       1                     2  \n3                       1                     2  \n4                       1                     1  \ny_resampled Preview:\n0    3\n1    3\n2    3\n3    3\n4    3\nName: TrustHCSystem, dtype: int64\n\n\n\n\nHyperparameter tuning\n\n\nCode\n# Manual Hyperparameter tuning \n# Define the hyperparameter grid\nparam_grid = {\n    \"n_estimators\": [50, 100, 200],\n    \"max_depth\": [None, 10, 20],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4],\n}\n\n# Generate all combinations of hyperparameters\nparam_combinations = list(product(\n    param_grid[\"n_estimators\"],\n    param_grid[\"max_depth\"],\n    param_grid[\"min_samples_split\"],\n    param_grid[\"min_samples_leaf\"]\n))\n\n# Initialize variables to store the best model\nbest_score = 0\nbest_params = None\nbest_model = None\n\n# Manual hyperparameter search\nfor n_estimators, max_depth, min_samples_split, min_samples_leaf in param_combinations:\n    print(f\"Training with parameters: n_estimators={n_estimators}, max_depth={max_depth}, \"\n          f\"min_samples_split={min_samples_split}, min_samples_leaf={min_samples_leaf}\")\n    \n    # Train a Random Forest model\n    rf = RandomForestClassifier(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        min_samples_leaf=min_samples_leaf,\n        random_state=42\n    )\n    rf.fit(X_resampled, y_resampled)\n\n    # Predict on the test set\n    y_pred = rf.predict(X_test)\n\n    # Evaluate the model\n    score = accuracy_score(y_test, y_pred)\n    print(f\"Accuracy: {score:.4f}\")\n\n    # Store the best model\n    if score &gt; best_score:\n        best_score = score\n        best_params = {\n            \"n_estimators\": n_estimators,\n            \"max_depth\": max_depth,\n            \"min_samples_split\": min_samples_split,\n            \"min_samples_leaf\": min_samples_leaf,\n        }\n        best_model = rf\n\n# Output the best model and its parameters\nprint(\"\\nBest Parameters:\", best_params)\nprint(f\"Best Accuracy: {best_score:.4f}\")\n\n# Evaluate the best model in detail\nprint(\"\\nClassification Report for Best Model:\\n\")\ny_pred_best = best_model.predict(X_test)\nprint(classification_report(y_test, y_pred_best))\n\n\nTraining with parameters: n_estimators=50, max_depth=None, min_samples_split=2, min_samples_leaf=1\nAccuracy: 0.5781\nTraining with parameters: n_estimators=50, max_depth=None, min_samples_split=2, min_samples_leaf=2\nAccuracy: 0.6250\nTraining with parameters: n_estimators=50, max_depth=None, min_samples_split=2, min_samples_leaf=4\nAccuracy: 0.5625\nTraining with parameters: n_estimators=50, max_depth=None, min_samples_split=5, min_samples_leaf=1\nAccuracy: 0.5156\nTraining with parameters: n_estimators=50, max_depth=None, min_samples_split=5, min_samples_leaf=2\nAccuracy: 0.5156\nTraining with parameters: n_estimators=50, max_depth=None, min_samples_split=5, min_samples_leaf=4\nAccuracy: 0.5625\nTraining with parameters: n_estimators=50, max_depth=None, min_samples_split=10, min_samples_leaf=1\nAccuracy: 0.5469\nTraining with parameters: n_estimators=50, max_depth=None, min_samples_split=10, min_samples_leaf=2\nAccuracy: 0.5625\nTraining with parameters: n_estimators=50, max_depth=None, min_samples_split=10, min_samples_leaf=4\nAccuracy: 0.5781\nTraining with parameters: n_estimators=50, max_depth=10, min_samples_split=2, min_samples_leaf=1\nAccuracy: 0.5156\nTraining with parameters: n_estimators=50, max_depth=10, min_samples_split=2, min_samples_leaf=2\nAccuracy: 0.5625\nTraining with parameters: n_estimators=50, max_depth=10, min_samples_split=2, min_samples_leaf=4\nAccuracy: 0.5938\nTraining with parameters: n_estimators=50, max_depth=10, min_samples_split=5, min_samples_leaf=1\nAccuracy: 0.5156\nTraining with parameters: n_estimators=50, max_depth=10, min_samples_split=5, min_samples_leaf=2\nAccuracy: 0.5625\nTraining with parameters: n_estimators=50, max_depth=10, min_samples_split=5, min_samples_leaf=4\nAccuracy: 0.5938\nTraining with parameters: n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=1\nAccuracy: 0.5000\nTraining with parameters: n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=2\nAccuracy: 0.5625\nTraining with parameters: n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4\nAccuracy: 0.5469\nTraining with parameters: n_estimators=50, max_depth=20, min_samples_split=2, min_samples_leaf=1\nAccuracy: 0.5781\nTraining with parameters: n_estimators=50, max_depth=20, min_samples_split=2, min_samples_leaf=2\nAccuracy: 0.6250\nTraining with parameters: n_estimators=50, max_depth=20, min_samples_split=2, min_samples_leaf=4\nAccuracy: 0.5625\nTraining with parameters: n_estimators=50, max_depth=20, min_samples_split=5, min_samples_leaf=1\nAccuracy: 0.5156\nTraining with parameters: n_estimators=50, max_depth=20, min_samples_split=5, min_samples_leaf=2\nAccuracy: 0.5156\nTraining with parameters: n_estimators=50, max_depth=20, min_samples_split=5, min_samples_leaf=4\nAccuracy: 0.5625\nTraining with parameters: n_estimators=50, max_depth=20, min_samples_split=10, min_samples_leaf=1\nAccuracy: 0.5469\nTraining with parameters: n_estimators=50, max_depth=20, min_samples_split=10, min_samples_leaf=2\nAccuracy: 0.5625\nTraining with parameters: n_estimators=50, max_depth=20, min_samples_split=10, min_samples_leaf=4\nAccuracy: 0.5781\nTraining with parameters: n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1\nAccuracy: 0.6094\nTraining with parameters: n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=2\nAccuracy: 0.6250\nTraining with parameters: n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=4\nAccuracy: 0.5625\nTraining with parameters: n_estimators=100, max_depth=None, min_samples_split=5, min_samples_leaf=1\nAccuracy: 0.5781\nTraining with parameters: n_estimators=100, max_depth=None, min_samples_split=5, min_samples_leaf=2\nAccuracy: 0.5625\nTraining with parameters: n_estimators=100, max_depth=None, min_samples_split=5, min_samples_leaf=4\nAccuracy: 0.5625\nTraining with parameters: n_estimators=100, max_depth=None, min_samples_split=10, min_samples_leaf=1\nAccuracy: 0.5781\nTraining with parameters: n_estimators=100, max_depth=None, min_samples_split=10, min_samples_leaf=2\nAccuracy: 0.6250\nTraining with parameters: n_estimators=100, max_depth=None, min_samples_split=10, min_samples_leaf=4\nAccuracy: 0.5469\nTraining with parameters: n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=1\nAccuracy: 0.5781\nTraining with parameters: n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=2\nAccuracy: 0.5938\nTraining with parameters: n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=4\nAccuracy: 0.5781\nTraining with parameters: n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=1\nAccuracy: 0.5625\nTraining with parameters: n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=2\nAccuracy: 0.5781\nTraining with parameters: n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=4\nAccuracy: 0.5781\nTraining with parameters: n_estimators=100, max_depth=10, min_samples_split=10, min_samples_leaf=1\nAccuracy: 0.5469\nTraining with parameters: n_estimators=100, max_depth=10, min_samples_split=10, min_samples_leaf=2\nAccuracy: 0.5938\nTraining with parameters: n_estimators=100, max_depth=10, min_samples_split=10, min_samples_leaf=4\nAccuracy: 0.5469\nTraining with parameters: n_estimators=100, max_depth=20, min_samples_split=2, min_samples_leaf=1\nAccuracy: 0.6094\nTraining with parameters: n_estimators=100, max_depth=20, min_samples_split=2, min_samples_leaf=2\nAccuracy: 0.6250\nTraining with parameters: n_estimators=100, max_depth=20, min_samples_split=2, min_samples_leaf=4\nAccuracy: 0.5625\nTraining with parameters: n_estimators=100, max_depth=20, min_samples_split=5, min_samples_leaf=1\nAccuracy: 0.5781\nTraining with parameters: n_estimators=100, max_depth=20, min_samples_split=5, min_samples_leaf=2\nAccuracy: 0.5625\nTraining with parameters: n_estimators=100, max_depth=20, min_samples_split=5, min_samples_leaf=4\nAccuracy: 0.5625\nTraining with parameters: n_estimators=100, max_depth=20, min_samples_split=10, min_samples_leaf=1\nAccuracy: 0.5781\nTraining with parameters: n_estimators=100, max_depth=20, min_samples_split=10, min_samples_leaf=2\nAccuracy: 0.6250\nTraining with parameters: n_estimators=100, max_depth=20, min_samples_split=10, min_samples_leaf=4\nAccuracy: 0.5469\nTraining with parameters: n_estimators=200, max_depth=None, min_samples_split=2, min_samples_leaf=1\nAccuracy: 0.5625\nTraining with parameters: n_estimators=200, max_depth=None, min_samples_split=2, min_samples_leaf=2\nAccuracy: 0.5938\nTraining with parameters: n_estimators=200, max_depth=None, min_samples_split=2, min_samples_leaf=4\nAccuracy: 0.5938\nTraining with parameters: n_estimators=200, max_depth=None, min_samples_split=5, min_samples_leaf=1\nAccuracy: 0.5312\nTraining with parameters: n_estimators=200, max_depth=None, min_samples_split=5, min_samples_leaf=2\nAccuracy: 0.5938\nTraining with parameters: n_estimators=200, max_depth=None, min_samples_split=5, min_samples_leaf=4\nAccuracy: 0.5938\nTraining with parameters: n_estimators=200, max_depth=None, min_samples_split=10, min_samples_leaf=1\nAccuracy: 0.5781\nTraining with parameters: n_estimators=200, max_depth=None, min_samples_split=10, min_samples_leaf=2\nAccuracy: 0.5938\nTraining with parameters: n_estimators=200, max_depth=None, min_samples_split=10, min_samples_leaf=4\nAccuracy: 0.5469\nTraining with parameters: n_estimators=200, max_depth=10, min_samples_split=2, min_samples_leaf=1\nAccuracy: 0.5938\nTraining with parameters: n_estimators=200, max_depth=10, min_samples_split=2, min_samples_leaf=2\nAccuracy: 0.6094\nTraining with parameters: n_estimators=200, max_depth=10, min_samples_split=2, min_samples_leaf=4\nAccuracy: 0.5781\nTraining with parameters: n_estimators=200, max_depth=10, min_samples_split=5, min_samples_leaf=1\nAccuracy: 0.5312\nTraining with parameters: n_estimators=200, max_depth=10, min_samples_split=5, min_samples_leaf=2\nAccuracy: 0.5781\nTraining with parameters: n_estimators=200, max_depth=10, min_samples_split=5, min_samples_leaf=4\nAccuracy: 0.5781\nTraining with parameters: n_estimators=200, max_depth=10, min_samples_split=10, min_samples_leaf=1\nAccuracy: 0.5312\nTraining with parameters: n_estimators=200, max_depth=10, min_samples_split=10, min_samples_leaf=2\nAccuracy: 0.5625\nTraining with parameters: n_estimators=200, max_depth=10, min_samples_split=10, min_samples_leaf=4\nAccuracy: 0.5469\nTraining with parameters: n_estimators=200, max_depth=20, min_samples_split=2, min_samples_leaf=1\nAccuracy: 0.5625\nTraining with parameters: n_estimators=200, max_depth=20, min_samples_split=2, min_samples_leaf=2\nAccuracy: 0.5938\nTraining with parameters: n_estimators=200, max_depth=20, min_samples_split=2, min_samples_leaf=4\nAccuracy: 0.5938\nTraining with parameters: n_estimators=200, max_depth=20, min_samples_split=5, min_samples_leaf=1\nAccuracy: 0.5312\nTraining with parameters: n_estimators=200, max_depth=20, min_samples_split=5, min_samples_leaf=2\nAccuracy: 0.5938\nTraining with parameters: n_estimators=200, max_depth=20, min_samples_split=5, min_samples_leaf=4\nAccuracy: 0.5938\nTraining with parameters: n_estimators=200, max_depth=20, min_samples_split=10, min_samples_leaf=1\nAccuracy: 0.5781\nTraining with parameters: n_estimators=200, max_depth=20, min_samples_split=10, min_samples_leaf=2\nAccuracy: 0.5938\nTraining with parameters: n_estimators=200, max_depth=20, min_samples_split=10, min_samples_leaf=4\nAccuracy: 0.5469\n\nBest Parameters: {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 2}\nBest Accuracy: 0.6250\n\nClassification Report for Best Model:\n\n              precision    recall  f1-score   support\n\n           3       0.78      0.71      0.74        49\n           4       0.26      0.33      0.29        15\n\n    accuracy                           0.62        64\n   macro avg       0.52      0.52      0.52        64\nweighted avg       0.66      0.62      0.64        64"
  },
  {
    "objectID": "hints_regression.html#rationale-2",
    "href": "hints_regression.html#rationale-2",
    "title": "Machine Learning",
    "section": "Rationale",
    "text": "Rationale\nXGBoost was selected due to its efficiency, ability to handle structured data, and superior performance in capturing non-linear relationships. Its gradient boosting framework optimizes performance by minimizing errors iteratively, making it particularly suited for datasets with complex patterns. Additionally, XGBoost’s flexibility in hyperparameter tuning and in-built support for addressing class imbalance make it a robust choice for this task.\n\n\nCode\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom imblearn.over_sampling import SMOTE\nfrom itertools import product\nfrom sklearn.model_selection import GridSearchCV\n\n# --- Define get_class_metric function ---\ndef get_class_metric(y_true, y_pred, target_class, metric):\n    \"\"\"\n    Safely extract precision/recall/F1-score for a specific class.\n    \"\"\"\n    report = classification_report(y_true, y_pred, output_dict=True)\n    if target_class in report:\n        return report[target_class].get(metric, None)\n    return None\n\n# --- Standard Model (XGBoost) ---\n# Define the features (X) and target variable (y)\n# Safely drop 'TrustHCSystem' and 'HHID' if they exist in the DataFrame\ncolumns_to_drop = ['TrustHCSystem', 'HHID']\nX = hints_cleaned.drop(columns=[col for col in columns_to_drop if col in hints_cleaned.columns])  # Drop if columns exist\ny = hints_cleaned['TrustHCSystem']\n\n# Ensure target variable is in integer format for multi-class classification\ny = y.astype(int)\n\n# Ensure target variable starts from 0 for multi-class classification\ny = y - min(y)  # This will map the minimum class to 0, making classes contiguous (e.g., 3 -&gt; 0, 4 -&gt; 1)\n\n# Split the data into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# XGBoost Classifier (Standard Model)\nxgb_standard = xgb.XGBClassifier(random_state=42, objective='multi:softmax', num_class=len(y.unique()))  # Multi-class objective\nxgb_standard.fit(X_train, y_train)\n\n# Predict and evaluate the model\ny_pred_standard = xgb_standard.predict(X_test)\naccuracy_standard = accuracy_score(y_test, y_pred_standard)\n\n# Print classification report for standard model\nprint(\"XGBoost Standard Model Accuracy:\", accuracy_standard)\nprint(\"\\nClassification Report for Standard Model:\\n\", classification_report(y_test, y_pred_standard))\n\n\n# --- Class Imbalance Model (XGBoost with SMOTE) ---\n# Apply SMOTE for class imbalance correction\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n# XGBoost Classifier (Class Imbalance Model)\nxgb_resampled = xgb.XGBClassifier(random_state=42, objective='multi:softmax', num_class=len(y.unique()))  # Multi-class objective\nxgb_resampled.fit(X_resampled, y_resampled)\n\n# Predict and evaluate the model\ny_pred_resampled = xgb_resampled.predict(X_test)\naccuracy_resampled = accuracy_score(y_test, y_pred_resampled)\n\n# Print classification report for class imbalance model\nprint(\"\\nXGBoost Class Imbalance Model Accuracy:\", accuracy_resampled)\nprint(\"\\nClassification Report for Class Imbalance Model:\\n\", classification_report(y_test, y_pred_resampled))\n\n\n# --- Hyperparameter Tuning for XGBoost ---\n# Define the hyperparameter grid for XGBoost\nparam_grid = {\n    \"n_estimators\": [50, 100, 200],\n    \"learning_rate\": [0.01, 0.1],\n    \"max_depth\": [3, 5, 7],\n    \"subsample\": [0.8, 1.0],\n    \"colsample_bytree\": [0.8, 1.0],\n}\n\n# Perform GridSearchCV for hyperparameter tuning\nxgb_tuned = xgb.XGBClassifier(random_state=42, objective='multi:softmax', num_class=len(y.unique()))  # Multi-class objective\n\n# Use GridSearchCV to search for the best hyperparameters\ngrid_search = GridSearchCV(estimator=xgb_tuned, param_grid=param_grid, cv=3, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\n\n# Get the best model from GridSearchCV\nbest_xgb_model = grid_search.best_estimator_\n\n# Predict and evaluate the tuned model\ny_pred_tuned = best_xgb_model.predict(X_test)\naccuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n\n# Print classification report for hyperparameter-tuned model\nprint(\"\\nXGBoost Hyperparameter Tuned Model Accuracy:\", accuracy_tuned)\nprint(\"\\nClassification Report for Hyperparameter Tuned Model:\\n\", classification_report(y_test, y_pred_tuned))\n\n\n# --- Model Comparison ---\n# Store results for comparison\nresults = {\n    \"Model\": [\"Standard XGBoost\", \"Class Imbalance XGBoost (SMOTE)\", \"Tuned XGBoost\"],\n    \"Accuracy\": [accuracy_standard, accuracy_resampled, accuracy_tuned],\n    \"Class 3 Precision\": [get_class_metric(y_test, y_pred_standard, \"0\", \"precision\"),\n                          get_class_metric(y_test, y_pred_resampled, \"0\", \"precision\"),\n                          get_class_metric(y_test, y_pred_tuned, \"0\", \"precision\")],\n    \"Class 4 Precision\": [get_class_metric(y_test, y_pred_standard, \"1\", \"precision\"),\n                          get_class_metric(y_test, y_pred_resampled, \"1\", \"precision\"),\n                          get_class_metric(y_test, y_pred_tuned, \"1\", \"precision\")],\n    \"Class 3 Recall\": [get_class_metric(y_test, y_pred_standard, \"0\", \"recall\"),\n                       get_class_metric(y_test, y_pred_resampled, \"0\", \"recall\"),\n                       get_class_metric(y_test, y_pred_tuned, \"0\", \"recall\")],\n    \"Class 4 Recall\": [get_class_metric(y_test, y_pred_standard, \"1\", \"recall\"),\n                       get_class_metric(y_test, y_pred_resampled, \"1\", \"recall\"),\n                       get_class_metric(y_test, y_pred_tuned, \"1\", \"recall\")],\n}\n\n# Create DataFrame for comparison\ncomparison_df = pd.DataFrame(results)\n\n# Display comparison of models\n# print(\"\\nModel Comparison:\")\n# print(comparison_df)\n\n# Model comparison data\nresults = {\n    \"Model\": [\"Standard XGBoost\", \"Class Imbalance XGBoost (SMOTE)\", \"Tuned XGBoost\", \"Random Forest\"],\n    \"Accuracy\": [0.703125, 0.5625, 0.75, 0.70],\n    \"Class 3 Precision\": [0.788462, 0.733333, 0.779661, 0.75],  # Class 3 Precision\n    \"Class 4 Precision\": [0.333333, 0.157895, 0.4, 0.2],  # Class 4 Precision\n    \"Class 3 Recall\": [0.836735, 0.673469, 0.938776, 0.70],  # Class 3 Recall\n    \"Class 4 Recall\": [0.266667, 0.2, 0.133333, 0.3],  # Class 4 Recall\n}\n\n# Convert into DataFrame\nmodel_comparison_df = pd.DataFrame(results)\n\n# Convert the DataFrame to JSON format (you can use 'records' to get a list of dicts)\nmodel_comparison_json = model_comparison_df.to_json(orient='records')\n\n# Use IPython display to inject the JSON into the JavaScript environment\nfrom IPython.display import display, Javascript\n\n# Inject JSON into JavaScript (accessible as window.df_data in the browser)\ndisplay(Javascript(f'window.df_data = {model_comparison_json};'))\n\n\nXGBoost Standard Model Accuracy: 0.703125\n\nClassification Report for Standard Model:\n               precision    recall  f1-score   support\n\n           0       0.79      0.84      0.81        49\n           1       0.33      0.27      0.30        15\n\n    accuracy                           0.70        64\n   macro avg       0.56      0.55      0.55        64\nweighted avg       0.68      0.70      0.69        64\n\n\nXGBoost Class Imbalance Model Accuracy: 0.5625\n\nClassification Report for Class Imbalance Model:\n               precision    recall  f1-score   support\n\n           0       0.73      0.67      0.70        49\n           1       0.16      0.20      0.18        15\n\n    accuracy                           0.56        64\n   macro avg       0.45      0.44      0.44        64\nweighted avg       0.60      0.56      0.58        64\n\n\nXGBoost Hyperparameter Tuned Model Accuracy: 0.75\n\nClassification Report for Hyperparameter Tuned Model:\n               precision    recall  f1-score   support\n\n           0       0.78      0.94      0.85        49\n           1       0.40      0.13      0.20        15\n\n    accuracy                           0.75        64\n   macro avg       0.59      0.54      0.53        64\nweighted avg       0.69      0.75      0.70        64\n\n\n\n\n\n\n\n\nCode\nfunction parseCSV(csvString) {\n  return d3.csvParse(csvString);\n}\n\n\n\n\n\n\n\n\n\nCode\nd3 = require(\"d3\", \"d3-svg-legend\")\n\n// Define and load your datasets\ndat = await FileAttachment('df.csv')\n  .text()\n  .then(parseCSV)\n  .then(data =&gt; data.map(d =&gt; ({\n    'Model': d['Model'], \n    'Accuracy': +d['Accuracy'], \n    'Class 3 Precision': +d['Class 3 Precision'],\n    'Class 4 Precision': +d['Class 4 Precision'],\n    'Class 3 Recall': +d['Class 3 Recall'],\n    'Class 4 Recall': +d['Class 4 Recall']\n  })));\n\ndat;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport { slider } from \"@jashkenas/inputs\"\nimport { vl } from \"@vega/vega-lite-api\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nviewof numTicks = slider({\n  min: 0,\n  max: 1,\n  value: 2,\n  step: 1,\n  title: \"Model Performance\"\n})\n\n\n\n\n\n\n\n\n\nCode\nchart = {\n  const height = 350;\n  const labelMap = {\n    class_3: \"High Trust in Healthcare Precision Scores\",\n    class_4: \"Low Trust in Healthcare Precision Scores\",\n    class_3_r: \"High Trust in Healthcare Recall Scores\",\n    class_4_r: \"Low Trust in Healthcare Recall Scores\",\n  };\n  \n  const hover = vl\n    .selectSingle()\n    .on(\"mouseover\")\n    .nearest(true)\n    .empty(\"none\");\n\n  const base = vl.markRule({ color: \"#ccc\" }).encode(\n  vl.x().fieldN(\"key\").sort(['GDP per capita (USD)', 'Female enrolment ratio', 'GNI per capita (USD)']).title(d =&gt; labelMap[d] || d),  // Explicitly sort by desired order\n  vl.detail().count()\n);\n\nconst line = base.markLine().encode(\n  vl.color().fieldN(\"Model\").scale({\n    domain: ['Standard XGBoost', 'Class Imbalance XGBoost (SMOTE)', 'Tuned XGBoost', 'Random Forest'], \n    range: [\"#3d6469\", \"#ffa205\", \"#ff4500\", \"#d40637\"]\n  }),\n  vl.detail().fieldN(\"index\"),\n  vl.opacity().if(hover, vl.value(1)).value(0.3),\n  vl.y().fieldQ(\"norm_val\").axis(null),\n  vl.tooltip([\n    'Model',\n    'Accuracy',\n    'Class 3 Precision', \n    'Class 4 Precision', \n    'Class 3 Recall', \n    'Class 4 Recall'\n  ])\n);\n  const points = line.markCircle()\n    .select(hover)\n    .encode(vl.size().if(hover, vl.value(50)).value(5));\n\n  // Generates a spec to show tick values at an specific value of y0\n  const tick = y0 =&gt;\n    vl.layer(\n        base.markText({ style: \"label\" }).encode(vl.text().max(\"max\")),\n        base.markTick({ style: \"tick\", size: 8, color: \"#ccc\" })\n      )\n      .encode(vl.y().value(y0));\n\n  // Create an array with *numTicks* ticks\n  const ticks = Array.from({ length: numTicks })\n    .map((_, i) =&gt; tick((height / (numTicks - 1)) * i));\n\nreturn vl\n    .layer(base, line, points, ...ticks)\n    .data(dat)\n    .transform(\n      vl.filter(attribs.map(a =&gt; `datum[\"${a}\"] != null`).join(\" && \")),\n      vl.window(vl.count().as(\"index\")),\n      vl.fold(attribs),\n      vl.groupby(\"key\").joinaggregate(vl.min(\"value\").as(\"min\"), vl.max(\"value\").as(\"max\")),\n      vl.calculate(\"(datum.value - datum.min) / (datum.max - datum.min)\").as(\"norm_val\"),\n      vl.calculate(\"(datum.min + datum.max) / 2\").as(\"mid\")\n    )\n    .config({\n      axisX: { domain: false, labelAngle: 0, tickColor: \"#ccc\", title: null },\n      view: { stroke: null },\n      style: {\n        label: { baseline: \"middle\", align: \"right\", dx: -5 },\n        tick: { orient: \"horizontal\" }\n      }\n    })\n    .width(width - 100)\n    .height(height)\n    .title({\n      text: 'Model Performance for Cancer vs Non-Cancer Data',\n      fontSize: 12,\n      fontWeight: 'normal',\n      anchor: 'middle',\n      color: 'black',\n      font: 'monospace',\n      offset: 40\n    })\n    .render();\n\n\n}\n\n\n\n\n\n\n\n\n\nCode\nattribs = Object.keys(dat[0]).filter(a =&gt; !isNaN(dat[0][a])) //Find the attributes that are numbers"
  },
  {
    "objectID": "hints_regression.html#model-performance-1",
    "href": "hints_regression.html#model-performance-1",
    "title": "Machine Learning",
    "section": "Model Performance",
    "text": "Model Performance\nThe Standard XGBoost model performs well for Class 0 (Trust in Healthcare System - High) with high precision and recall, indicating a good balance in identifying instances of this class. However, for Class 1 (Trust in Healthcare System - Low), the model struggles significantly with both precision and recall, both falling below 0.3. The Class Imbalance XGBoost model (SMOTE) improves recall for Class 1, helping to better detect the minority class. However, the model’s precision remains low, meaning it still incorrectly predicts many instances as Class 1 (Trust in Healthcare System - Low). The Hyperparameter Tuned XGBoost model delivers the best performance for Class 0 (Trust in Healthcare System - High), achieving very high recall (90%) and good precision (79%). Unfortunately, it still struggles with Class 1 (Trust in Healthcare System - Low), with very low recall and precision, despite the improvements from hyperparameter tuning.\nIn terms of overall performance, the Hyperparameter Tuned XGBoost model achieves the highest accuracy (74%), showing the best balance for Class 0 (Trust in Healthcare System - High), but it fails to capture Class 1 (Trust in Healthcare System - Low) effectively. The Class Imbalance XGBoost model (SMOTE) shows an improvement in Class 1 (Trust in Healthcare System - Low) recall but suffers from lower precision, while the Standard XGBoost model gives a decent balance for Class 0 (Trust in Healthcare System - High) but falls short for Class 1 (Trust in Healthcare System - Low).\nConclusions The Hyperparameter Tuned XGBoost model is the best overall in terms of accuracy and performance for Class 0 (Trust in Healthcare System - High), but further work is needed to improve recall and precision for Class 1 (Trust in Healthcare System - Low). Techniques such as class weighting or more advanced oversampling strategies might help address these issues. The Class Imbalance XGBoost model (SMOTE) provides some improvement for Class 1 (Trust in Healthcare System - Low) recall, but further fine-tuning of the class balancing methods could yield better results."
  }
]